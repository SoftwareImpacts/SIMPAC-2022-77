import matplotlib.pyplot as plt
import numpy as np
import shap, ast

from collections import defaultdict
from sklearn.metrics import f1_score, roc_curve, roc_auc_score, auc, precision_recall_curve, accuracy_score, average_precision_score
from utils import *

"""Load feature list by each category"""
time_features = ast.literal_eval(open("../data/features/time_features.txt").read())
aaai_features = ast.literal_eval(open("../data/features/aaai_features.txt").read())
graph_features = ast.literal_eval(open("../data/features/graph_features.txt").read())
statistical_features = ast.literal_eval(open("../data/features/statistical_features.txt").read())
context_features = ast.literal_eval(open("../data/features/context_features.txt").read())
our_features = ast.literal_eval(open("selected_features.txt", "r").read())


def measure(gpu=False):
    filename = "../data/us_2020_election_data.csv"
    feature_set = [aaai_features, statistical_features, context_features, time_features, graph_features, our_features]
    AUCEs = [None] * len(feature_set)
    F1Es = [None] * len(feature_set)
    precision = []
    recall = []
    FP = []
    TP = []
    ROC = []
    model_params = parse_model_params("xgboost")

    iterations = 10
    data_res = defaultdict(lambda: defaultdict(lambda: []))
    for it in range(iterations):
        df, target, X_test, Y_test = read_data(filename, known_features=False)

        X_train, X_val, Y_train, Y_val = train_test_split(df, target, test_size=0.30, stratify=target)

        ####################################################
        # Oversample train and validation portion separately#
        ####################################################

        # oversampling clean on training set
        X_train, Y_train = oversample(X_train, Y_train)

        for run in range(len(feature_set)):
            ########################################################################
            # Train model based on train portion of data with selected features only#
            ########################################################################
            selected_model = get_xgboost_model(objective=model_params["objective"],
                              learning_rate=model_params["learning_rate"],
                              n_estimators=model_params["n_estimators"],
                              max_depth=model_params["max_depth"],
                              colsample_bytree=model_params["colsample_bytree"],
                              eval_metric=model_params["eval_metric"],
                              num_class=2, gpu=gpu)


            XGB_fitted_opt = selected_model.fit(X_train[feature_set[run]], Y_train)
            XGB_test_probs = XGB_fitted_opt.predict_proba(X_test[feature_set[run]])

            data_res[str(run)]["lr_probs"].append(XGB_test_probs)
            XGB_test_probs = XGB_test_probs[:, 1]

            data_res[str(run)]["RF_fitted_opt"].append(selected_model)
            data_res[str(run)]["X_test"].append(X_test[feature_set[run]])
            data_res[str(run)]["y_test"].append(Y_test)

            XGB_precision, XGB_recall, _ = precision_recall_curve(Y_test, XGB_test_probs)

            # AUC
            XGB_auc = auc(XGB_recall, XGB_precision)

            # F1
            XGB_f1 = f1_score(Y_test, selected_model.predict(X_test[feature_set[run]]))

            fpr, tpr, threshold = roc_curve(Y_test, XGB_test_probs)

            roc_auc = auc(fpr, tpr)

            ###########################
            # October data measurment #
            ###########################
            october_df = pd.read_csv("../data/us_2020_election_data_october.csv", header=0)

            october_target = october_df["target"]


            october_f1 = f1_score(october_target, selected_model.predict(october_df[feature_set[run]]))
            october_probs = XGB_fitted_opt.predict_proba(october_df[feature_set[run]])[:, 1]
            october_fpr, october_tpr, october_threshold = roc_curve(october_target, october_probs)

            october_roc_auc = auc(october_fpr, october_tpr)

            data_res[str(run)]["PREC"].append(XGB_precision)
            data_res[str(run)]["REC"].append(XGB_recall)

            data_res[str(run)]["FP"].append(fpr)
            data_res[str(run)]["TP"].append(tpr)
            data_res[str(run)]["ROC"].append(roc_auc)
            data_res[str(run)]["AUC"].append(XGB_auc)
            data_res[str(run)]["F1"].append(XGB_f1)
            data_res[str(run)]["F1_october"].append(october_f1)
            data_res[str(run)]["ROC_october"].append(october_roc_auc)


            FP.append(fpr)
            TP.append(tpr)
            ROC.append(roc_auc)
            # AUCEs[run] = XGB_auc
            # F1Es[run] = XGB_f1
            precision.append(XGB_precision)
            recall.append(XGB_recall)

    return data_res


def roc_curves(data_res):
    plt.rcParams.update({'font.size': 13})
    fig = plt.figure(figsize=(6, 6), dpi=600)
    model_labels = ["Statistical General only", "Statistical", "Context ", "Time", "Graph", "Our Model"]

    for c_type in range(0, 6):
        tprs = []
        aucs = []

        mean_fpr = np.linspace(0, 1, 100)

        PR_prs = []
        PR_aucs = []
        PR_mean_recall = np.linspace(0, 1, 100)

        y_real = []
        precision_array = []
        y_proba = []
        precision_array_SMOTE = []
        y_proba_SMOTE = []

        for MonteCarlo_i in range(0, 10):
            # ------------------ Load parameters needed for plotting purposes ----------
            fpr = data_res[str(c_type)]["FP"][MonteCarlo_i]
            tpr = data_res[str(c_type)]["TP"][MonteCarlo_i]

            roc_auc = auc(fpr, tpr)
            # predict probabilities
            lr_probs = data_res[str(c_type)]["lr_probs"][MonteCarlo_i]

            # keep probabilities for the positive outcome only
            lr_probs = lr_probs[:, 1]

            # -------------- Plot Precision-Recall curves ------------------------------
            RF_fitted_opt = data_res[str(c_type)]["RF_fitted_opt"][MonteCarlo_i]
            X_test = data_res[str(c_type)]["X_test"][MonteCarlo_i]
            y_test = data_res[str(c_type)]["y_test"][MonteCarlo_i]

            # predict class values
            yhat = RF_fitted_opt.predict(X_test)
            lr_precision, lr_recall, _ = precision_recall_curve(y_test.values, lr_probs)
            lr_f1, lr_auc = f1_score(y_test.values, yhat), auc(lr_recall, lr_precision)
            averagePrecision = average_precision_score(y_test.values, lr_probs)

            # ------------------------ average values ----------------
            tprs.append(np.interp(mean_fpr, fpr, tpr))
            roc_auc = auc(fpr, tpr)
            aucs.append(roc_auc)

            # Compute PR curve and area the curve------------------
            lr_precision, lr_recall, _ = precision_recall_curve(y_test.values, lr_probs)
            PR_prs.append(np.interp(PR_mean_recall, lr_precision, lr_recall))
            pr_auc = auc(lr_recall, lr_precision)
            PR_aucs.append(pr_auc)

            # --- used to compute below, the mean PR curves------------------
            y_real.append(y_test.values)

            y_proba.append(lr_probs)
            lr_precision, lr_recall = lr_precision[::-1], lr_recall[::-1]  # reverse order of results
            precision_array = np.interp(PR_mean_recall, lr_recall, lr_precision)

        # ----------------------- mean ROC curves --------------------------------------
        aucs1 = aucs

        std_auc1 = np.std(aucs1)

        tprs1 = tprs
        mean_tpr1 = np.mean(tprs1, axis=0)

        mean_fpr1 = mean_fpr
        mean_auc1 = auc(mean_fpr1, mean_tpr1)

        std_tpr1 = np.std(tprs1, axis=0)
        tprs_upper1 = np.minimum(mean_tpr1 + std_tpr1, 1)
        tprs_lower1 = np.maximum(mean_tpr1 - std_tpr1, 0)

        collors = [4, 0, 1, 5, 3, 2]
        col = collors[c_type]

        plt.plot(mean_fpr1, mean_tpr1, color='C{}'.format(col),
                 label=r'{} (AUC: {:.2f} $\pm$ {:.3f}'.format(model_labels[c_type], mean_auc1, std_auc1),
                 lw=2, alpha=.8)

        plt.fill_between(mean_fpr1, tprs_lower1, tprs_upper1, color='C{}'.format(col), alpha=.1)

    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive rate', fontsize=13)
    plt.ylabel('True Positive rate ', fontsize=13)
    plt.title('ROC curves ', fontsize=15)
    plt.legend(loc='best')

    plt.savefig("../plots/roc_mean.png", bbox_inches='tight', dpi=600, facecolor='w')

def pr_curves(data_res):
    plt.rcParams.update({'font.size': 13})
    fig = plt.figure(figsize=(6, 6), dpi=600)
    model_labels = ["Statistical General only", "Statistical", "Context ", "Time", "Graph", "Our Model"]

    for c_type in range(0, 6):
        tprs = []
        aucs = []
        f1 = []
        mean_fpr = np.linspace(0, 1, 100)

        PR_prs = []
        PR_aucs = []
        PR_mean_recall = np.linspace(0, 1, 100)

        y_real = []
        precision_array = []
        y_proba = []
        precision_array_SMOTE = []
        y_proba_SMOTE = []

        for MonteCarlo_i in range(0, 10):
            # ------------------ Load parameters needed for plotting purposes ----------
            fpr = data_res[str(c_type)]["FP"][MonteCarlo_i]
            tpr = data_res[str(c_type)]["TP"][MonteCarlo_i]
            f1.append(data_res[str(c_type)]["F1"][MonteCarlo_i])

            roc_auc = auc(fpr, tpr)
            # predict probabilities
            lr_probs = data_res[str(c_type)]["lr_probs"][MonteCarlo_i]

            # keep probabilities for the positive outcome only
            lr_probs = lr_probs[:, 1]

            # -------------- Plot Precision-Recall curves ------------------------------
            RF_fitted_opt = data_res[str(c_type)]["RF_fitted_opt"][MonteCarlo_i]
            X_test = data_res[str(c_type)]["X_test"][MonteCarlo_i]
            y_test = data_res[str(c_type)]["y_test"][MonteCarlo_i]


            # predict class values
            yhat = RF_fitted_opt.predict(X_test)
            lr_precision, lr_recall, _ = precision_recall_curve(y_test.values, lr_probs)
            lr_f1, lr_auc = f1_score(y_test.values, yhat), auc(lr_recall, lr_precision)
            averagePrecision = average_precision_score(y_test.values, lr_probs)

            # ------------------------ average values ----------------
            tprs.append(np.interp(mean_fpr, lr_precision, lr_recall))
            roc_auc = auc(fpr, tpr)
            aucs.append(roc_auc)

            # Compute PR curve and area the curve------------------
            lr_precision, lr_recall, _ = precision_recall_curve(y_test.values, lr_probs)
            PR_prs.append(np.interp(PR_mean_recall, lr_precision, lr_recall))
            pr_auc = auc(lr_recall, lr_precision)
            PR_aucs.append(pr_auc)

            # --- used to compute below, the mean PR curves------------------
            y_real.append(y_test.values)

            y_proba.append(lr_probs)
            lr_precision, lr_recall = lr_precision[::-1], lr_recall[::-1]  # reverse order of results
            precision_array = np.interp(PR_mean_recall, lr_recall, lr_precision)

        # ----------------------- mean ROC curves --------------------------------------
        aucs1 = aucs

        std_auc1 = np.std(aucs1)

        tprs1 = tprs
        mean_tpr1 = np.mean(tprs1, axis=0)

        mean_fpr1 = mean_fpr
        mean_auc1 = auc(mean_fpr1, mean_tpr1)

        std_tpr1 = np.std(tprs1, axis=0)
        tprs_upper1 = np.minimum(mean_tpr1 + std_tpr1, 1)
        tprs_lower1 = np.maximum(mean_tpr1 - std_tpr1, 0)

        collors = [4, 0, 1, 5, 3, 2]
        col = collors[c_type]

        plt.plot(mean_fpr1, mean_tpr1, color='C{}'.format(col),
                 label=r'{} (AUC: {:.2f} $\pm$ {:.3f}'.format(model_labels[c_type], mean_auc1, std_auc1),
                 lw=2, alpha=.8)

        plt.fill_between(mean_fpr1, tprs_lower1, tprs_upper1, color='C{}'.format(col), alpha=.1)


    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('Precision', fontsize=13)
    plt.ylabel('Recall', fontsize=13)
    plt.title('PR curves', fontsize=15)
    plt.legend(loc='best')

    plt.savefig("../plots/prec_recal_mean.png", 
            # bbox_inches='tight', 
            dpi=600, facecolor='w')

def shap_values(gpu=False):
    filename = "../data/us_2020_election_data.csv"
    model_params = parse_model_params("xgboost")

    df, target, X_test, Y_test = read_data(filename, known_features=True)

    X_train, X_val, Y_train, Y_val = train_test_split(df, target, test_size=0.30, stratify=target)

    ####################################################
    # Oversample train and validation portion separately#
    ####################################################

    # oversampling clean on training set
    X_train, Y_train = oversample(X_train, Y_train)

    # Flush both models, in order to use the from scratch
    XGB_model = get_xgboost_model(objective=model_params["objective"],
                              learning_rate=model_params["learning_rate"],
                              n_estimators=model_params["n_estimators"],
                              max_depth=model_params["max_depth"],
                              colsample_bytree=model_params["colsample_bytree"],
                              eval_metric=model_params["eval_metric"],
                              num_class=2, gpu=gpu)


    XGB_model.fit(X_train, Y_train.ravel())

    shap_values_xgb = shap.TreeExplainer(XGB_model).shap_values(X_test)

    fig = plt.figure()
    shap.summary_plot(shap_values_xgb[1], X_test)
    fig.savefig("../plots/shap.png", bbox_inches='tight', dpi=600, facecolor='w')

