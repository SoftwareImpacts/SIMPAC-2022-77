{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#libraries that need to be installed extra , if already do not exists\n",
    "#mglearn, pandas, numpy, sklearn\n",
    "#----------------------------------------\n",
    "#---------------------------------------------------------------------------\n",
    "#FLAG used in each code block mean \n",
    "#                1: that can be executed at any machine with final csv file\n",
    "#                0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#import libaries\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import mglearn, csv, random , matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "#data scaling \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "#accuracy score metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#cosine similatiry metric\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#covariance with ellipticEnvelope (anomaly detection)\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#oversampling \n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "#jupiter display dataset without limits \n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for duplicates :11836 and 11836. Should be equeal\n",
      "Dataset filtering. Users before:86927 users after:11836\n",
      "Final dataset shape:(11836, 328)\n"
     ]
    }
   ],
   "source": [
    "def filter_users_by_label(df):\n",
    "    u_before = df.shape[0]\n",
    "    user_ids = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    target = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    \n",
    "    filtered_bots = set([int(x.split(\" \")[0]) for x in open(\"../data/bot_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "    filtered_clear = set([int(x.split(\" \")[0]) for x in open(\"../data/clear_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "\n",
    "    keep_ids = []\n",
    "    for x in np.where(target == 2)[0]:\n",
    "        if user_ids[x]  in filtered_bots:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 1\n",
    "\n",
    "    for x in np.where(target == 0)[0]:\n",
    "        if user_ids[x] in filtered_clear:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 0\n",
    "    \n",
    "    df[\"target\"] = target \n",
    "    df[\"user_id\"] = user_ids\n",
    "    print(\"Check for duplicates :{} and {}. Should be equeal\".format(len(keep_ids), len(set(keep_ids))))\n",
    "    df = df.iloc[keep_ids]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    u_after = df.shape[0]\n",
    "    print(\"Dataset filtering. Users before:{} users after:{}\".format(u_before, u_after))\n",
    "    return df\n",
    "    \n",
    "\n",
    "def graph_features(df):\n",
    "    graph = [(np.int64(x.split(\"\\\"\")[1]), np.int64(x.split(\"\\\"\")[3])) for x in open(\"../data/ret_graph.dot\", \"r\").read().split(\"\\n\") if \"->\" in x ]\n",
    "    #identify self loops in graphs \n",
    "    self_loop = set([a for a,b in graph if a == b]) \n",
    "    \n",
    "    #Get node degree data with weighted and un-weighted\n",
    "    degree_data = { np.int64(x.split(\",\")[0]) : x.split(\",\")[3:] for x in open(\"../data/ret_data_degree.csv\",\"r\").read().split(\"\\n\")[1:] if x != \"\"}\n",
    "    \n",
    "    #Assemble all features in single DataFrame\n",
    "    df[\"rt_self\"]  = np.array([1 if x in self_loop else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"in_degree\"] = np.array([degree_data[x][0] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"out_degree\"] = np.array([degree_data[x][1] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_in_degree\"] = np.array([degree_data[x][3] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_out_degree\"] = np.array([degree_data[x][4] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_degree\"] = np.array([degree_data[x][5] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    return df\n",
    "                               \n",
    "def word_2_vec(dataFrame):\n",
    "    model = Word2Vec.load(\"../data/word2vec_1_new.model\")\n",
    "    \n",
    "    dataframes = [dataFrame]\n",
    "    \n",
    "    labels_to_remove = []\n",
    "    labels = [\"mst_fr_ment_tw_word_\", \"mst_fr_ment_rt_word_\", \n",
    "              \"mst_fr_hs_tw_word_\", \"mst_fr_hs_rt_word_\", \n",
    "              \"words_frq_tw_\", \"words_frq_rt_\"]\n",
    "    for label in labels:\n",
    "        for i in range(1,4):\n",
    "            labels_to_remove.append(label+str(i))\n",
    "            temp = []\n",
    "            for word in dataFrame[label+str(i)]:\n",
    "                if word == \"_\":\n",
    "                    score = [0]*10\n",
    "                elif label == \"words_frq_rt_\" and i == 2 and word != word:\n",
    "                    score = model.wv['nan']\n",
    "                else:\n",
    "                    score = model.wv[word]\n",
    "                temp.append(score)\n",
    "            col = [\"{}{}_{}\".format(label,i,k) for k in range(10)]\n",
    "            dataframes.append(pd.DataFrame(temp, columns=col))\n",
    "    \n",
    "    \n",
    "    #Merge all dataframes and drop the text based columns that was used in word2vec model\n",
    "    merged = pd.concat(dataframes, axis=\"columns\")\n",
    "    merged = merged.drop(labels_to_remove, axis=\"columns\")\n",
    "    return merged\n",
    "\n",
    "def combine_data():\n",
    "    df = pd.read_csv(\"../data/new_vectors_new.csv\",sep = '\\t', header=0)\n",
    "    \n",
    "    #read labels from botometer and keep only users that agree to been bot or removed according to botometer\n",
    "    df = filter_users_by_label(df)\n",
    "    \n",
    "    #Read graph features \n",
    "    df = graph_features(df)\n",
    "                               \n",
    "    #transform word features with word2vec to numerical vectors  \n",
    "    df = word_2_vec(df)\n",
    "    df = df.drop([\"followers.1\"], axis=\"columns\")\n",
    "    print(\"Final dataset shape:{}\".format(df.shape))\n",
    "    \n",
    "    #save csv dataframe in order to reduce computation since the dataset remain same within multiple executions\n",
    "    df.to_csv (r'../data/features_large_with_words.csv', index = False, header=True)\n",
    "    \n",
    "\n",
    "combine_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv file with features and merge it with graph features and labels from label file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of mix of bot labels from Botsentiel and Botometer\n",
    "### !!!! \n",
    "### Important combine_data are used only on Alex machine since i have all files that is combined to final csv file with all features no need to run \n",
    "### !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "def filter_users_by_label(df):\n",
    "    u_before = df.shape[0]\n",
    "    user_ids = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    target = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    \n",
    "    filtered_bots = set([int(x.split(\" \")[0]) for x in open(\"../data/bot_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "    filtered_clear = set([int(x.split(\" \")[0]) for x in open(\"../data/clear_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "\n",
    "    keep_ids = []\n",
    "    for x in np.where(target == 2)[0]:\n",
    "        if user_ids[x]  in filtered_bots:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 1\n",
    "\n",
    "    for x in np.where(target == 0)[0]:\n",
    "        if user_ids[x] in filtered_clear:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 0\n",
    "    \n",
    "    df[\"target\"] = target \n",
    "    df[\"user_id\"] = user_ids\n",
    "    print(\"Check for duplicates :{} and {}. Should be equeal\".format(len(keep_ids), len(set(keep_ids))))\n",
    "    df = df.iloc[keep_ids]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    u_after = df.shape[0]\n",
    "    print(\"Dataset filtering. Users before:{} users after:{}\".format(u_before, u_after))\n",
    "    return df\n",
    "    \n",
    "\n",
    "def graph_features(df):\n",
    "    graph = [(np.int64(x.split(\"\\\"\")[1]), np.int64(x.split(\"\\\"\")[3])) for x in open(\"../data/ret_graph.dot\", \"r\").read().split(\"\\n\") if \"->\" in x ]\n",
    "    #identify self loops in graphs \n",
    "    self_loop = set([a for a,b in graph if a == b]) \n",
    "    \n",
    "    #Get node degree data with weighted and un-weighted\n",
    "    degree_data = { np.int64(x.split(\",\")[0]) : x.split(\",\")[3:] for x in open(\"../data/ret_data_degree.csv\",\"r\").read().split(\"\\n\")[1:] if x != \"\"}\n",
    "    \n",
    "    #Assemble all features in single DataFrame\n",
    "    df[\"rt_self\"]  = np.array([1 if x in self_loop else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"in_degree\"] = np.array([degree_data[x][0] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"out_degree\"] = np.array([degree_data[x][1] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_in_degree\"] = np.array([degree_data[x][3] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_out_degree\"] = np.array([degree_data[x][4] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_degree\"] = np.array([degree_data[x][5] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    return df\n",
    "                               \n",
    "def word_2_vec(dataFrame):\n",
    "    model = Word2Vec.load(\"../data/word2vec_1_new.model\")\n",
    "    \n",
    "    dataframes = [dataFrame]\n",
    "    \n",
    "    labels_to_remove = []\n",
    "    labels = [\"mst_fr_ment_tw_word_\", \"mst_fr_ment_rt_word_\", \n",
    "              \"mst_fr_hs_tw_word_\", \"mst_fr_hs_rt_word_\", \n",
    "              \"words_frq_tw_\", \"words_frq_rt_\"]\n",
    "    for label in labels:\n",
    "        for i in range(1,4):\n",
    "            labels_to_remove.append(label+str(i))\n",
    "            temp = []\n",
    "            for word in dataFrame[label+str(i)]:\n",
    "                if word == \"_\":\n",
    "                    score = [0]*10\n",
    "                elif label == \"words_frq_rt_\" and i == 2 and word != word:\n",
    "                    score = model.wv['nan']\n",
    "                else:\n",
    "                    score = model.wv[word]\n",
    "                temp.append(score)\n",
    "            col = [\"{}{}_{}\".format(label,i,k) for k in range(10)]\n",
    "            dataframes.append(pd.DataFrame(temp, columns=col))\n",
    "    \n",
    "    \n",
    "    #Merge all dataframes and drop the text based columns that was used in word2vec model\n",
    "    merged = pd.concat(dataframes, axis=\"columns\")\n",
    "    merged = merged.drop(labels_to_remove, axis=\"columns\")\n",
    "    return merged\n",
    "\n",
    "def combine_data():\n",
    "    df = pd.read_csv(\"../data/new_vectors_new.csv\",sep = '\\t', header=0)\n",
    "    \n",
    "    #read labels from botometer and keep only users that agree to been bot or removed according to botometer\n",
    "    df = filter_users_by_label(df)\n",
    "    \n",
    "    #Read graph features \n",
    "    df = graph_features(df)\n",
    "                               \n",
    "    #transform word features with word2vec to numerical vectors  \n",
    "    df = word_2_vec(df)\n",
    "    df = df.drop([\"followers.1\"], axis=\"columns\")\n",
    "    print(\"Final dataset shape:{}\".format(df.shape))\n",
    "    \n",
    "    #save csv dataframe in order to reduce computation since the dataset remain same within multiple executions\n",
    "    df.to_csv (r'../data/features_large_with_words.csv', index = False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#scale  data of each portion of dataset (train, evaluation and test)\n",
    "def scale_data(train, evaluation, test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n",
    "    # scale train and test\n",
    "    X_eval = pd.DataFrame(scaler.transform(evaluation), columns = evaluation.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(test), columns = test.columns)\n",
    "    return X_train, X_eval, X_test\n",
    "\n",
    "#oversample dataset portion\n",
    "#used in separated form for train and evalution datasets\n",
    "#do not use in test data at all\n",
    "def oversample(data, target):\n",
    "    smk = SMOTETomek()\n",
    "    data, target = smk.fit_sample(data, target)\n",
    "    return data, target\n",
    "\n",
    "#load twitter features\n",
    "def read_data(filename, verbose=False):\n",
    "    #read the csv file that was combined with word2vec and graph features\n",
    "    df = pd.read_csv(filename, header=0)\n",
    "    \n",
    "    \n",
    "    #random shuffle the dataframe\n",
    "    df = shuffle(df)\n",
    "    \n",
    "    #extract user id from dataframe\n",
    "    user_ids = df[\"user_id\"]\n",
    "    \n",
    "    #extract target from datafrmae\n",
    "    target = df[\"target\"]\n",
    "    \n",
    "    df = df.drop([\"user_id\",\"target\"], axis=\"columns\")\n",
    "    \n",
    "    #make stratified train and test split 80/20 \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, \n",
    "                                                        target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=target)\n",
    "    \n",
    "    #scale features values with MinMaxScaler\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(X_train)\n",
    "    #scale train and test\n",
    "    #X_train = scaler.transform(X_train)\n",
    "    #X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #after scaling cast from np.array to dataframe again\n",
    "    X_test = pd.DataFrame(X_test, columns = df.columns)\n",
    "    X_train = pd.DataFrame(X_train, columns = df.columns)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "def kbest_features(data, labels,n_features):\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaled_data = scaler.fit(data).transform(data)\n",
    "\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=n_features)\n",
    "    fit = bestfeatures.fit(data, labels)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(data.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "    #print(featureScores.nlargest(n_features,'Score'))  #print 10 best features\n",
    "    res = featureScores.nlargest(n_features,'Score')\n",
    "    #print(dir(res))\n",
    "    res = [x[0] for x in res.values]\n",
    "    \n",
    "    embeded_rf_support = [True if x in res else False for x in df.columns.tolist()]\n",
    "    #res = [x[0] for x in res.values]\n",
    "    #res = res.get_support()\n",
    "    #embeded_rf_feature = data.loc[:,res].columns.tolist()\n",
    "    return data[res]\n",
    "\n",
    "def rforest_features(data, labels, n_features):\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaled_data = scaler.fit(data).transform(data)\n",
    "\n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=n_features)\n",
    "    embeded_rf_selector.fit(data, labels)\n",
    "\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = data.loc[:,embeded_rf_support].columns.tolist()\n",
    "    #print(\"R-Forest selected features: {} number of features: {}\".format(embeded_rf_feature, len(embeded_rf_feature)))\n",
    "    return data[embeded_rf_feature], embeded_rf_feature\n",
    "\n",
    "\n",
    "def lasso_features(data, labels,n_features):\n",
    "    #scaler = StandardScaler()\n",
    "    #scaled_data= scaler.fit(data).transform(data)\n",
    "    #embeded_lr_selector = SelectFromModel(LogisticRegression(solver = \"sag\", penalty=\"l2\", max_iter=10000), \n",
    "    #                                      max_features=n_features)\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(solver = \"saga\", penalty=\"l2\", max_iter=10000), \n",
    "                                          max_features=n_features)\n",
    "    \n",
    "    embeded_lr_selector.fit(data, labels)\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    \n",
    "    #print(embeded_lr_support)\n",
    "    embeded_lr_feature = data.loc[:,embeded_lr_support].columns.tolist()\n",
    "    #print(\"Lasso selected features: {} number of features:{}\".format(embeded_lr_feature,len(embeded_lr_feature)))\n",
    "    #return data[:,embeded_lr_support], embeded_lr_feature\n",
    "    return data[embeded_lr_feature], embeded_lr_feature\n",
    "\n",
    "def lasso(data, labels, alpha = 0.02):\n",
    "    res = Lasso(alpha=alpha,selection='cyclic',max_iter=100000)\n",
    "    res.fit(data,labels)\n",
    "    mst = np.where(res.coef_ != 0.0)[0]\n",
    "    #print(mst)\n",
    "    #print(\"Features:{}\".format(list(data.columns[mst])))\n",
    "    #print(\"Number:{}\".format(len(list(data.columns[mst]))))\n",
    "    return data[list(data.columns[mst])], list(data.columns[mst])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality reduction - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "colors = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', \n",
    "              '#f58231', '#911eb4', '#46f0f0', '#f032e6', \n",
    "              '#bcf60c', '#fabebe', '#008080', '#e6beff', \n",
    "              '#9a6324', '#fffac8', '#800000', '#aaffc3', \n",
    "              '#808000', '#ffd8b1', '#000075', '#808080', \n",
    "              '#ffffff', \"#fe4a49\", \"#BD3430\", \"#fed766\"]\n",
    "\n",
    "#------------\n",
    "#---T-SNE----\n",
    "#------------\n",
    "def make_tsne(data_scaled, label, title, learning_rate=100, perplexity=20):\n",
    "    #tsne = TSNE(random_state=1)\n",
    "    tsne = TSNE(learning_rate=learning_rate,perplexity=perplexity, init='pca')\n",
    "    \n",
    "    #use fit_transform instead of fit, as TSNE has no transform method\n",
    "    digits_tsne = tsne.fit_transform(data_scaled)\n",
    "    #create color palette\n",
    "    palette = sns.color_palette(None, max(list(set(label))) + 1)\n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "    #print(len(palette))\n",
    "    c_color = [palette[x] if x != -1 else (0,0,0) for x in label]\n",
    "    \n",
    "    plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "    plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "    plt.scatter(digits_tsne[:, 0],digits_tsne[:, 1],c=c_color)\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('t-SNE projection {}'.format(title), fontsize=24)\n",
    "    \"\"\"\n",
    "    for i in range(len(digits_tsne[:, 0])):\n",
    "        # actually plot the digits as text instead of using scatter\n",
    "        plt.text(digits_tsne[i, 0], digits_tsne[i, 1],str(target[i]),\n",
    "                     color = colors[target[i]],\n",
    "                     fontdict={'weight': 'bold', 'size': 9})\n",
    "    \"\"\"\n",
    "    plt.xlabel(\"t-SNE feature 0\")\n",
    "    plt.xlabel(\"t-SNE feature 1\")\n",
    "    plt.show()\n",
    "    return digits_tsne\n",
    "\n",
    "#------------\n",
    "#----UMAP----\n",
    "#------------\n",
    "def perofrm_umap_clustering(data, labels,  title, n_neighbors=25, n_components=2,):\n",
    "    #reducer = umap.UMAP(n_neighbors=25,metric=\"manhattan\")\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components= n_components, metric = \"euclidean\")#metric=\"euclidean\")\n",
    "    embedding = reducer.fit_transform(data)\n",
    "    embedding.shape\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        c=[sns.color_palette()[x] for x in labels])\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('UMAP projection {}'.format(title), fontsize=24)\n",
    "    plt.show()\n",
    "    return embedding\n",
    "\n",
    "#------------\n",
    "#---DBSCAN---\n",
    "#------------\n",
    "def dbscan_clustering(positions, eps, comp_filter, target, scale=False):\n",
    "    #dbscan = DBSCAN(eps=0.0058)\n",
    "    #dbscan = DBSCAN(eps=0.0050)\n",
    "    #dbscan = DBSCAN(eps=0.0048)\n",
    "    #dbscan = DBSCAN(eps=0.0049)\n",
    "    dbscan = DBSCAN(eps=eps)\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(positions)\n",
    "        p_scaled = scaler.transform(positions)\n",
    "    else:\n",
    "        p_scaled = positions\n",
    "    clusters = dbscan.fit_predict(p_scaled)\n",
    "    \n",
    "    #print(\"Cluster memberships:\\n{}\".format(clusters))\n",
    "    #print(\"Unique slucter ids:{}\\n\\n\".format(set(clusters)))\n",
    "    #cmap = plt.cm.rainbow\n",
    "    #norm = matplotlib.colors.Normalize(vmin=1.5, vmax=4.5)\n",
    "    #print([(x/256.0, list(clusters).count(x)) for x in set(clusters)])\n",
    "    #print(len(sns.color_palette()))\n",
    "    #[print(sns.color_palette()[x]) for x in clusters]\n",
    "    \n",
    "    palette = sns.color_palette(None, len(set(clusters)))\n",
    "    \n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "    c_color = [palette[x] if x != -1 else (0,0,0) for x in clusters]\n",
    "    plt.scatter(p_scaled[:, 0], p_scaled[:, 1], color=c_color, cmap=mglearn.cm3, s=20)\n",
    "    #plt.scatter(p_scaled[clusters == 0, 0], p_scaled[clusters == 0, 1], color=colors[0], cmap=mglearn.cm3, s=20)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    plt.show()\n",
    "    \n",
    "    components = defaultdict(lambda: set())\n",
    "    bot_comp = defaultdict(lambda:set())\n",
    "    clear_comp = defaultdict(lambda:set())\n",
    "    comp_size = []\n",
    "    \"\"\"\n",
    "    for i in range(len(clusters)):\n",
    "        #if target[i] == 1:\n",
    "        #    #print(clusters[i])\n",
    "        comp_size.append(clusters[i])\n",
    "    comp_size = set(comp_size)\n",
    "    \"\"\"\n",
    "    #max_comp = max(set(comp_size), key = comp_size.count)\n",
    "    #for i in range(len(clusters)):\n",
    "    #    if clusters[i] in comp_size :\n",
    "    #        #print(user_ids[i])\n",
    "    #        c_o[clusters[i]].add(i)\n",
    "    #        if target[i] == 1:\n",
    "    #            b[clusters[i]].add(i)\n",
    "    return clusters\n",
    "    for i in range(len(clusters)):\n",
    "        if clusters[i] == -1:\n",
    "            continue\n",
    "        components[clusters[i]].add(i)\n",
    "        if target[i] == 1:\n",
    "            bot_comp[clusters[i]].add(i)\n",
    "        if target[i] == 0:\n",
    "            clear_comp[clusters[i]].add(i)\n",
    "    \n",
    "    max_tp_bot = []\n",
    "    for comp in bot_comp:\n",
    "        if len(components[comp]) > comp_filter and len(bot_comp[comp])/len(components[comp]) > 0.5:\n",
    "            max_tp_bot.append((len(bot_comp[comp])/len(components[comp])) *100.0)\n",
    "            #print(\"TP :{} FP :{} Bperc: {} Comp size:{}\".format((len(b[comp])/len(c_o[comp])) *100.0, ((len(c_o[comp]) - len(b[comp]))/len(c_o[comp])) * 100.0 , len(b[comp])/len(target[target == 1]), len(c_o[comp])))\n",
    "    print(\"Bot accuracy Max:{} AVG:{} Min:{}\".format(max(max_tp_bot), sum(max_tp_bot)/len(max_tp_bot) , min(max_tp_bot)))\n",
    "    max_tp_clear = []\n",
    "    for comp in clear_comp:\n",
    "        if len(components[comp]) > comp_filter and len(clear_comp[comp])/len(components[comp]) > 0.5:\n",
    "            max_tp_clear.append((len(clear_comp[comp])/len(components[comp])) *100.0)\n",
    "    print(\"Clear accuracy Max:{} AVG:{} Min:{}\".format(max(max_tp_clear), sum(max_tp_clear)/len(max_tp_clear) , min(max_tp_clear)))\n",
    "\n",
    "#------------\n",
    "#---DBSCAN--- !!!!!!with clustering sampling !!!!!!!\n",
    "#------------\n",
    "def dbscan_clustering_with_smaple(positions, eps, comp_filter, bot_indx, clear_indx, target=None):\n",
    "    #dbscan = DBSCAN(eps=0.0058)\n",
    "    #dbscan = DBSCAN(eps=0.0050)\n",
    "    #dbscan = DBSCAN(eps=0.0048)\n",
    "    #dbscan = DBSCAN(eps=0.0049)\n",
    "    dbscan = DBSCAN(eps=eps)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(positions)\n",
    "    p_scaled = scaler.transform(positions)\n",
    "    clusters = dbscan.fit_predict(p_scaled)\n",
    "    #print(\"Cluster memberships:\\n{}\".format(clusters))\n",
    "    #print(\"Unique slucter ids:{}\\n\\n\".format(set(clusters)))\n",
    "    #cmap = plt.cm.rainbow\n",
    "    #norm = matplotlib.colors.Normalize(vmin=1.5, vmax=4.5)\n",
    "    #print([(x/256.0, list(clusters).count(x)) for x in set(clusters)])\n",
    "    #print(len(sns.color_palette()))\n",
    "    #[print(sns.color_palette()[x]) for x in clusters]\n",
    "   \n",
    "    palette = sns.color_palette(None, len(set(clusters)))\n",
    "    \n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "    c_color = [palette[x] if x != -1 else (0,0,0) for x in clusters]\n",
    "    plt.scatter(p_scaled[:, 0], p_scaled[:, 1], color=c_color, cmap=mglearn.cm3, s=20)\n",
    "    #plt.scatter(p_scaled[clusters == 0, 0], p_scaled[clusters == 0, 1], color=colors[0], cmap=mglearn.cm3, s=20)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    plt.show()\n",
    "    \n",
    "    components = defaultdict(lambda: set())\n",
    "    bot_comp = defaultdict(lambda:set())\n",
    "    clear_comp = defaultdict(lambda:set())\n",
    "    comp_size = []\n",
    "    \"\"\"\n",
    "    for i in range(len(clusters)):\n",
    "        #if target[i] == 1:\n",
    "        #    #print(clusters[i])\n",
    "        comp_size.append(clusters[i])\n",
    "    comp_size = set(comp_size)\n",
    "    \"\"\"\n",
    "    #max_comp = max(set(comp_size), key = comp_size.count)\n",
    "    #for i in range(len(clusters)):\n",
    "    #    if clusters[i] in comp_size :\n",
    "    #        #print(user_ids[i])\n",
    "    #        c_o[clusters[i]].add(i)\n",
    "    #        if target[i] == 1:\n",
    "    #            b[clusters[i]].add(i)\n",
    "    comp_all = defaultdict(lambda: set())\n",
    "    \n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "        if clusters[i] == -1:\n",
    "            continue\n",
    "        comp_all[clusters[i]].add(i)\n",
    "        if i in bot_indx or i in clear_indx:\n",
    "            continue\n",
    "        components[clusters[i]].add(i)\n",
    "        if target[i] == 1:\n",
    "            bot_comp[clusters[i]].add(i)\n",
    "        if target[i] == 0:\n",
    "            clear_comp[clusters[i]].add(i)\n",
    "    \n",
    "    max_tp = []\n",
    "    max_fp = []\n",
    "    bot_indx = set(bot_indx)\n",
    "    clear_indx = set(clear_indx)\n",
    "    \n",
    "        \n",
    "    for comp in bot_comp:\n",
    "        #check comp size and purity \n",
    "        if len(components[comp]) > comp_filter and len(bot_comp[comp])/len(components[comp]) > 0.6:\n",
    "            #at this point componnent is labeled as bot component\n",
    "            \n",
    "            sample_bot_comp = len(comp_all[comp].intersection(bot_indx))\n",
    "            sample_clear_comp = len(comp_all[comp].intersection(clear_indx))\n",
    "            if sample_bot_comp != 0 :\n",
    "                max_tp.append( (sample_bot_comp / (sample_bot_comp+sample_clear_comp)) *100.0)\n",
    "                print(comp)\n",
    "                \n",
    "            if sample_clear_comp != 0 :\n",
    "                max_fp.append( (sample_clear_comp / (sample_clear_comp+sample_bot_comp)) *100.0 )\n",
    "            #print(\"TP :{} FP :{} Bperc: {} Comp size:{}\".format((len(b[comp])/len(c_o[comp])) *100.0, ((len(c_o[comp]) - len(b[comp]))/len(c_o[comp])) * 100.0 , len(b[comp])/len(target[target == 1]), len(c_o[comp])))\n",
    "    plt.figure()\n",
    "    for i in comp_all[9]:\n",
    "        if i in bot_indx:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], color = \"#a11515\")\n",
    "        elif i in clear_indx:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], color = \"#a11515\")\n",
    "        elif i in clear_comp[1]:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], c = \"#167a19\")\n",
    "        elif i in bot_comp[1]:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], c = \"#a11515\")\n",
    "    plt.show()\n",
    "    if len(max_tp) != 0:\n",
    "        print(\"Bot TP accuracy Max:{} AVG:{} Min:{} len:{}\".format(max(max_tp), sum(max_tp)/len(max_tp) , min(max_tp), len(max_tp)))\n",
    "    else:\n",
    "        print(\"Bot TP is Zero\")\n",
    "    if len(max_fp) != 0:\n",
    "        print(\"Bot FP Max:{} AVG:{} Min:{} len:{}\".format(max(max_fp), sum(max_fp)/len(max_fp) , min(max_fp), len(max_fp)))\n",
    "    else:\n",
    "        print(\"Bot FP is Zero\")\n",
    "    #max_tp_clear = []\n",
    "    #for comp in clear_comp:\n",
    "    #    if len(components[comp]) > comp_filter and len(clear_comp[comp])/len(components[comp]) > 0.9:\n",
    "    #        max_tp_clear.append((len(clear_comp[comp])/len(components[comp])) *100.0)\n",
    "    #print(\"Clear accuracy Max:{} AVG:{} Min:{}\".format(max(max_tp_clear), sum(max_tp_clear)/len(max_tp_clear) , min(max_tp_clear)))\n",
    "\n",
    "    \n",
    "#------------\n",
    "#-Mean-Shift- \n",
    "#------------\n",
    "def mean_shift(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    p_scaled = scaler.transform(data)\n",
    "    bandwidth = estimate_bandwidth(p_scaled, quantile=0.05, n_samples=500)\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    meanshift.fit(p_scaled)\n",
    "    labels = meanshift.labels_\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    P = meanshift.predict(p_scaled)\n",
    "    palette = sns.color_palette(None, len(P))\n",
    "    col = [palette[x] if x != -1 else (0,0,0) for x in P]\n",
    "    plt.scatter(p_scaled[:,0], p_scaled[:,1], c=col, marker=\"o\", picker=True)\n",
    "    plt.title(f'Estimated number of clusters = {n_clusters_}')\n",
    "    plt.xlabel('Temperature yesterday')\n",
    "    plt.ylabel('Temperature today')\n",
    "    plt.show()\n",
    "    \n",
    "    c_o = defaultdict(lambda: set())\n",
    "    b = defaultdict(lambda:set())\n",
    "    comp_size = []\n",
    "    for i in range(len(P)):\n",
    "        if target[i] == 1:\n",
    "            #print(clusters[i])\n",
    "            comp_size.append(P[i])\n",
    "    comp_size = set(comp_size)\n",
    "    \n",
    "    #max_comp = max(set(comp_size), key = comp_size.count)\n",
    "    for i in range(len(P)):\n",
    "        if P[i] in comp_size :\n",
    "            #print(user_ids[i])\n",
    "            c_o[P[i]].add(i)\n",
    "            if target[i] == 1:\n",
    "                b[P[i]].add(i)\n",
    "    for comp in c_o:\n",
    "        if len(b[comp]) > 30 and len(c_o[comp]) > 50:\n",
    "            print(\"TP :{} FP :{} Bperc: {}\".format((len(b[comp])/len(c_o[comp])) *100.0, ((len(c_o[comp]) - len(b[comp]))/len(c_o[comp])) * 100.0 , len(b[comp])/len(target[target == 1]) ))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features (Database, Graph and new labels ) and store them into csv file \n",
    "### !!!!\n",
    "### No need to run , execute next cell where data is readed from CSV file by filename\n",
    "### !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for duplicates :11836 and 11836. Should be equeal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_rt_tw_0</th>\n",
       "      <th>daily_rt_tw_1</th>\n",
       "      <th>daily_rt_tw_2</th>\n",
       "      <th>daily_rt_tw_3</th>\n",
       "      <th>daily_rt_tw_4</th>\n",
       "      <th>daily_rt_tw_5</th>\n",
       "      <th>daily_rt_tw_6</th>\n",
       "      <th>daily_rt_0</th>\n",
       "      <th>daily_rt_1</th>\n",
       "      <th>daily_rt_2</th>\n",
       "      <th>daily_rt_3</th>\n",
       "      <th>daily_rt_4</th>\n",
       "      <th>daily_rt_5</th>\n",
       "      <th>daily_rt_6</th>\n",
       "      <th>daily_tw_0</th>\n",
       "      <th>daily_tw_1</th>\n",
       "      <th>daily_tw_2</th>\n",
       "      <th>daily_tw_3</th>\n",
       "      <th>daily_tw_4</th>\n",
       "      <th>daily_tw_5</th>\n",
       "      <th>daily_tw_6</th>\n",
       "      <th>hour_rt_tw_0</th>\n",
       "      <th>hour_rt_tw_1</th>\n",
       "      <th>hour_rt_tw_2</th>\n",
       "      <th>hour_rt_tw_3</th>\n",
       "      <th>hour_rt_tw_4</th>\n",
       "      <th>hour_rt_tw_5</th>\n",
       "      <th>hour_rt_tw_6</th>\n",
       "      <th>hour_rt_tw_7</th>\n",
       "      <th>hour_rt_tw_8</th>\n",
       "      <th>hour_rt_tw_9</th>\n",
       "      <th>hour_rt_tw_10</th>\n",
       "      <th>hour_rt_tw_11</th>\n",
       "      <th>hour_rt_tw_12</th>\n",
       "      <th>hour_rt_tw_13</th>\n",
       "      <th>hour_rt_tw_14</th>\n",
       "      <th>hour_rt_tw_15</th>\n",
       "      <th>hour_rt_tw_16</th>\n",
       "      <th>hour_rt_tw_17</th>\n",
       "      <th>hour_rt_tw_18</th>\n",
       "      <th>hour_rt_tw_19</th>\n",
       "      <th>hour_rt_tw_20</th>\n",
       "      <th>hour_rt_tw_21</th>\n",
       "      <th>hour_rt_tw_22</th>\n",
       "      <th>hour_rt_tw_23</th>\n",
       "      <th>hour_tw_0</th>\n",
       "      <th>hour_tw_1</th>\n",
       "      <th>hour_tw_2</th>\n",
       "      <th>hour_tw_3</th>\n",
       "      <th>hour_tw_4</th>\n",
       "      <th>hour_tw_5</th>\n",
       "      <th>hour_tw_6</th>\n",
       "      <th>hour_tw_7</th>\n",
       "      <th>hour_tw_8</th>\n",
       "      <th>hour_tw_9</th>\n",
       "      <th>hour_tw_10</th>\n",
       "      <th>hour_tw_11</th>\n",
       "      <th>hour_tw_12</th>\n",
       "      <th>hour_tw_13</th>\n",
       "      <th>hour_tw_14</th>\n",
       "      <th>hour_tw_15</th>\n",
       "      <th>hour_tw_16</th>\n",
       "      <th>hour_tw_17</th>\n",
       "      <th>hour_tw_18</th>\n",
       "      <th>hour_tw_19</th>\n",
       "      <th>hour_tw_20</th>\n",
       "      <th>hour_tw_21</th>\n",
       "      <th>hour_tw_22</th>\n",
       "      <th>hour_tw_23</th>\n",
       "      <th>hour_rt_0</th>\n",
       "      <th>hour_rt_1</th>\n",
       "      <th>hour_rt_2</th>\n",
       "      <th>hour_rt_3</th>\n",
       "      <th>hour_rt_4</th>\n",
       "      <th>hour_rt_5</th>\n",
       "      <th>hour_rt_6</th>\n",
       "      <th>hour_rt_7</th>\n",
       "      <th>hour_rt_8</th>\n",
       "      <th>hour_rt_9</th>\n",
       "      <th>hour_rt_10</th>\n",
       "      <th>hour_rt_11</th>\n",
       "      <th>hour_rt_12</th>\n",
       "      <th>hour_rt_13</th>\n",
       "      <th>hour_rt_14</th>\n",
       "      <th>hour_rt_15</th>\n",
       "      <th>hour_rt_16</th>\n",
       "      <th>hour_rt_17</th>\n",
       "      <th>hour_rt_18</th>\n",
       "      <th>hour_rt_19</th>\n",
       "      <th>hour_rt_20</th>\n",
       "      <th>hour_rt_21</th>\n",
       "      <th>hour_rt_22</th>\n",
       "      <th>hour_rt_23</th>\n",
       "      <th>mst_fr_ment_tw_1</th>\n",
       "      <th>mst_fr_ment_tw_2</th>\n",
       "      <th>mst_fr_ment_tw_3</th>\n",
       "      <th>mst_fr_ment_rt_1</th>\n",
       "      <th>mst_fr_ment_rt_2</th>\n",
       "      <th>mst_fr_ment_rt_3</th>\n",
       "      <th>mst_fr_hs_tw_1</th>\n",
       "      <th>mst_fr_hs_tw_2</th>\n",
       "      <th>mst_fr_hs_tw_3</th>\n",
       "      <th>mst_fr_hs_rt_1</th>\n",
       "      <th>mst_fr_hs_rt_2</th>\n",
       "      <th>mst_fr_hs_rt_3</th>\n",
       "      <th>tw_urls_avg</th>\n",
       "      <th>tw_urls_std</th>\n",
       "      <th>rt_urls_avg</th>\n",
       "      <th>rt_urls_std</th>\n",
       "      <th>tw_hash_avg</th>\n",
       "      <th>tw_hash_std</th>\n",
       "      <th>tw_ment_avg</th>\n",
       "      <th>tw_ment_std</th>\n",
       "      <th>rt_hash_avg</th>\n",
       "      <th>rt_hash_std</th>\n",
       "      <th>rt_ment_avg</th>\n",
       "      <th>rt_ment_std</th>\n",
       "      <th>rt_time_avg</th>\n",
       "      <th>rt_time_min</th>\n",
       "      <th>rt_time_max</th>\n",
       "      <th>rt_time_std</th>\n",
       "      <th>rt_avg</th>\n",
       "      <th>tw_avg</th>\n",
       "      <th>tw_rt_ration</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>favourites</th>\n",
       "      <th>listed</th>\n",
       "      <th>statuses</th>\n",
       "      <th>followers.1</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>name_len</th>\n",
       "      <th>name_screen_sim</th>\n",
       "      <th>geo</th>\n",
       "      <th>protected</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>description_len</th>\n",
       "      <th>bckg_img</th>\n",
       "      <th>default_prof</th>\n",
       "      <th>entities</th>\n",
       "      <th>rt_self</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>w_in_degree</th>\n",
       "      <th>w_out_degree</th>\n",
       "      <th>w_degree</th>\n",
       "      <th>target</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hs_tw_v_0</th>\n",
       "      <th>hs_tw_v_1</th>\n",
       "      <th>hs_tw_v_2</th>\n",
       "      <th>hs_tw_v_3</th>\n",
       "      <th>hs_tw_v_4</th>\n",
       "      <th>hs_tw_v_5</th>\n",
       "      <th>hs_tw_v_6</th>\n",
       "      <th>hs_tw_v_7</th>\n",
       "      <th>hs_tw_v_8</th>\n",
       "      <th>hs_tw_v_9</th>\n",
       "      <th>hs_rt_v_0</th>\n",
       "      <th>hs_rt_v_1</th>\n",
       "      <th>hs_rt_v_2</th>\n",
       "      <th>hs_rt_v_3</th>\n",
       "      <th>hs_rt_v_4</th>\n",
       "      <th>hs_rt_v_5</th>\n",
       "      <th>hs_rt_v_6</th>\n",
       "      <th>hs_rt_v_7</th>\n",
       "      <th>hs_rt_v_8</th>\n",
       "      <th>hs_rt_v_9</th>\n",
       "      <th>ment_tw_v_0</th>\n",
       "      <th>ment_tw_v_1</th>\n",
       "      <th>ment_tw_v_2</th>\n",
       "      <th>ment_tw_v_3</th>\n",
       "      <th>ment_tw_v_4</th>\n",
       "      <th>ment_tw_v_5</th>\n",
       "      <th>ment_tw_v_6</th>\n",
       "      <th>ment_tw_v_7</th>\n",
       "      <th>ment_tw_v_8</th>\n",
       "      <th>ment_tw_v_9</th>\n",
       "      <th>ment_rt_v_0</th>\n",
       "      <th>ment_rt_v_1</th>\n",
       "      <th>ment_rt_v_2</th>\n",
       "      <th>ment_rt_v_3</th>\n",
       "      <th>ment_rt_v_4</th>\n",
       "      <th>ment_rt_v_5</th>\n",
       "      <th>ment_rt_v_6</th>\n",
       "      <th>ment_rt_v_7</th>\n",
       "      <th>ment_rt_v_8</th>\n",
       "      <th>ment_rt_v_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178988</td>\n",
       "      <td>0.112840</td>\n",
       "      <td>0.128405</td>\n",
       "      <td>0.070039</td>\n",
       "      <td>0.182879</td>\n",
       "      <td>0.128405</td>\n",
       "      <td>0.198444</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.175141</td>\n",
       "      <td>0.124294</td>\n",
       "      <td>0.158192</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.180791</td>\n",
       "      <td>0.107345</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.042802</td>\n",
       "      <td>0.035019</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.038911</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.042802</td>\n",
       "      <td>0.073930</td>\n",
       "      <td>0.077821</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.116732</td>\n",
       "      <td>0.163424</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>0.039548</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.073446</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>0.146893</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>5.980586</td>\n",
       "      <td>6.887675</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>5.980586</td>\n",
       "      <td>9.177593</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>6.667747</td>\n",
       "      <td>3.043134</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>4.803376</td>\n",
       "      <td>6.667747</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.411693</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>5.016949</td>\n",
       "      <td>5.633836</td>\n",
       "      <td>1.988701</td>\n",
       "      <td>2.665960</td>\n",
       "      <td>3.962500</td>\n",
       "      <td>4.602988</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.915476</td>\n",
       "      <td>171.483958</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1292.233333</td>\n",
       "      <td>330.839721</td>\n",
       "      <td>2.758621</td>\n",
       "      <td>6.103448</td>\n",
       "      <td>2.212500</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>7349</td>\n",
       "      <td>0</td>\n",
       "      <td>3502</td>\n",
       "      <td>343</td>\n",
       "      <td>151</td>\n",
       "      <td>14</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>166</td>\n",
       "      <td>742</td>\n",
       "      <td>1</td>\n",
       "      <td>1258962548503740425</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.951101</td>\n",
       "      <td>2.923159</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>9.150005</td>\n",
       "      <td>9.552075</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>6.059891</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.061553</td>\n",
       "      <td>169.037500</td>\n",
       "      <td>7.183333</td>\n",
       "      <td>432.416667</td>\n",
       "      <td>241.493538</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>1463</td>\n",
       "      <td>0</td>\n",
       "      <td>1057</td>\n",
       "      <td>172</td>\n",
       "      <td>403</td>\n",
       "      <td>21</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1277260249687261186</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>-2.666591</td>\n",
       "      <td>3.297360</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>1.148872</td>\n",
       "      <td>-4.909317</td>\n",
       "      <td>4.969856</td>\n",
       "      <td>2.520902</td>\n",
       "      <td>0.120532</td>\n",
       "      <td>-3.473754</td>\n",
       "      <td>-1.942731</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>11.871754</td>\n",
       "      <td>6.725062</td>\n",
       "      <td>5.916363</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>2.488408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.049390</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>513.326667</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1020.416667</td>\n",
       "      <td>635.460221</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>36991</td>\n",
       "      <td>4</td>\n",
       "      <td>37368</td>\n",
       "      <td>281</td>\n",
       "      <td>462</td>\n",
       "      <td>15</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>746102335915556868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736998</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>-0.292788</td>\n",
       "      <td>-0.561506</td>\n",
       "      <td>-0.082709</td>\n",
       "      <td>0.042723</td>\n",
       "      <td>0.180475</td>\n",
       "      <td>-0.226081</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.524840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.896686</td>\n",
       "      <td>7.102824</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>5.161735</td>\n",
       "      <td>5.840240</td>\n",
       "      <td>6.162355</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>3.585302</td>\n",
       "      <td>12.348678</td>\n",
       "      <td>4.551958</td>\n",
       "      <td>8.989075</td>\n",
       "      <td>7.998400</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.987461</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>362.737500</td>\n",
       "      <td>12.116667</td>\n",
       "      <td>733.633333</td>\n",
       "      <td>475.360941</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>8697</td>\n",
       "      <td>0</td>\n",
       "      <td>4739</td>\n",
       "      <td>136</td>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1231691345271586824</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>-4.284786</td>\n",
       "      <td>7.481483</td>\n",
       "      <td>6.267010</td>\n",
       "      <td>4.508372</td>\n",
       "      <td>11.494592</td>\n",
       "      <td>-1.333691</td>\n",
       "      <td>-2.366664</td>\n",
       "      <td>0.944676</td>\n",
       "      <td>-3.689538</td>\n",
       "      <td>-7.882071</td>\n",
       "      <td>1.424672</td>\n",
       "      <td>0.269191</td>\n",
       "      <td>2.436769</td>\n",
       "      <td>3.364856</td>\n",
       "      <td>-2.865908</td>\n",
       "      <td>5.763546</td>\n",
       "      <td>3.289723</td>\n",
       "      <td>-7.134625</td>\n",
       "      <td>-1.938984</td>\n",
       "      <td>3.373724</td>\n",
       "      <td>-0.913174</td>\n",
       "      <td>4.238219</td>\n",
       "      <td>-0.241484</td>\n",
       "      <td>4.171506</td>\n",
       "      <td>-2.921207</td>\n",
       "      <td>6.198004</td>\n",
       "      <td>3.403417</td>\n",
       "      <td>-6.699732</td>\n",
       "      <td>-2.091649</td>\n",
       "      <td>-0.219933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.896686</td>\n",
       "      <td>10.566221</td>\n",
       "      <td>10.750414</td>\n",
       "      <td>6.268109</td>\n",
       "      <td>5.161735</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>6.516144</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.845154</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.195229</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>697.972222</td>\n",
       "      <td>521.350000</td>\n",
       "      <td>861.083333</td>\n",
       "      <td>711.683184</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "      <td>5757</td>\n",
       "      <td>0</td>\n",
       "      <td>9201</td>\n",
       "      <td>241</td>\n",
       "      <td>492</td>\n",
       "      <td>2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1518469190</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>1.424672</td>\n",
       "      <td>0.269191</td>\n",
       "      <td>2.436769</td>\n",
       "      <td>3.364856</td>\n",
       "      <td>-2.865908</td>\n",
       "      <td>5.763546</td>\n",
       "      <td>3.289723</td>\n",
       "      <td>-7.134625</td>\n",
       "      <td>-1.938984</td>\n",
       "      <td>3.373724</td>\n",
       "      <td>2.414909</td>\n",
       "      <td>-3.960131</td>\n",
       "      <td>-0.615269</td>\n",
       "      <td>2.886237</td>\n",
       "      <td>-8.917027</td>\n",
       "      <td>2.754604</td>\n",
       "      <td>-1.793620</td>\n",
       "      <td>-7.303990</td>\n",
       "      <td>-5.697265</td>\n",
       "      <td>4.512267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>7.810123</td>\n",
       "      <td>11.525478</td>\n",
       "      <td>8.102567</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>4.150451</td>\n",
       "      <td>3.265572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.522233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.343923</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>211.589394</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>1145.433333</td>\n",
       "      <td>378.904435</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>88977</td>\n",
       "      <td>0</td>\n",
       "      <td>15392</td>\n",
       "      <td>341</td>\n",
       "      <td>963</td>\n",
       "      <td>5</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1043726264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.481577</td>\n",
       "      <td>-4.951790</td>\n",
       "      <td>11.181436</td>\n",
       "      <td>8.863973</td>\n",
       "      <td>3.241972</td>\n",
       "      <td>-4.242408</td>\n",
       "      <td>-0.542843</td>\n",
       "      <td>3.072947</td>\n",
       "      <td>-7.411512</td>\n",
       "      <td>-3.821877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.205255</td>\n",
       "      <td>-13.955618</td>\n",
       "      <td>1.322916</td>\n",
       "      <td>3.720930</td>\n",
       "      <td>-6.821259</td>\n",
       "      <td>-13.958460</td>\n",
       "      <td>-3.371006</td>\n",
       "      <td>-5.193164</td>\n",
       "      <td>2.854197</td>\n",
       "      <td>0.453024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>4.176059</td>\n",
       "      <td>4.977433</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>2.571623</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.569047</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>337.120000</td>\n",
       "      <td>50.016667</td>\n",
       "      <td>962.716667</td>\n",
       "      <td>444.474572</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>266</td>\n",
       "      <td>4146</td>\n",
       "      <td>4</td>\n",
       "      <td>2577</td>\n",
       "      <td>266</td>\n",
       "      <td>564</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2542010298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>4.977433</td>\n",
       "      <td>5.120814</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>5.213256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.936492</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>19.533333</td>\n",
       "      <td>9.885420</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>22348</td>\n",
       "      <td>0</td>\n",
       "      <td>29246</td>\n",
       "      <td>96</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2520645596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.481577</td>\n",
       "      <td>-4.951790</td>\n",
       "      <td>11.181436</td>\n",
       "      <td>8.863973</td>\n",
       "      <td>3.241972</td>\n",
       "      <td>-4.242408</td>\n",
       "      <td>-0.542843</td>\n",
       "      <td>3.072947</td>\n",
       "      <td>-7.411512</td>\n",
       "      <td>-3.821877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.206959</td>\n",
       "      <td>-2.618856</td>\n",
       "      <td>1.212161</td>\n",
       "      <td>-3.037526</td>\n",
       "      <td>-1.402811</td>\n",
       "      <td>1.444823</td>\n",
       "      <td>12.596374</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>6.377641</td>\n",
       "      <td>-2.976061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.803406</td>\n",
       "      <td>2.923159</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.923159</td>\n",
       "      <td>4.669607</td>\n",
       "      <td>8.837133</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>7.183257</td>\n",
       "      <td>7.119354</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>2.488408</td>\n",
       "      <td>3.265572</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.972092</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>5.364492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.452966</td>\n",
       "      <td>317.183333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1312.000000</td>\n",
       "      <td>593.041953</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>2580</td>\n",
       "      <td>4078</td>\n",
       "      <td>97</td>\n",
       "      <td>122644</td>\n",
       "      <td>2580</td>\n",
       "      <td>961</td>\n",
       "      <td>11</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>97</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>14811111</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>-1.483979</td>\n",
       "      <td>1.254190</td>\n",
       "      <td>9.255416</td>\n",
       "      <td>5.533646</td>\n",
       "      <td>1.226344</td>\n",
       "      <td>4.875967</td>\n",
       "      <td>0.917352</td>\n",
       "      <td>-4.030603</td>\n",
       "      <td>-2.897294</td>\n",
       "      <td>-0.845497</td>\n",
       "      <td>-5.901112</td>\n",
       "      <td>3.408430</td>\n",
       "      <td>5.464797</td>\n",
       "      <td>5.483321</td>\n",
       "      <td>-3.814475</td>\n",
       "      <td>3.054348</td>\n",
       "      <td>1.276991</td>\n",
       "      <td>0.458579</td>\n",
       "      <td>-1.790708</td>\n",
       "      <td>-1.426023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1117713</td>\n",
       "      <td>8621</td>\n",
       "      <td>1244</td>\n",
       "      <td>24671</td>\n",
       "      <td>1117713</td>\n",
       "      <td>597</td>\n",
       "      <td>14</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "      <td>904</td>\n",
       "      <td>0</td>\n",
       "      <td>1064042478</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11836 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daily_rt_tw_0  daily_rt_tw_1  daily_rt_tw_2  daily_rt_tw_3  \\\n",
       "0           0.178988       0.112840       0.128405       0.070039   \n",
       "1           0.000000       0.400000       0.000000       0.400000   \n",
       "2           0.100000       0.200000       0.000000       0.200000   \n",
       "3           0.000000       0.250000       0.041667       0.041667   \n",
       "4           0.000000       0.400000       0.200000       0.100000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.090909       0.363636       0.090909       0.181818   \n",
       "11832       0.100000       0.100000       0.100000       0.200000   \n",
       "11833       0.250000       0.000000       0.250000       0.500000   \n",
       "11834       0.133333       0.000000       0.133333       0.266667   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       daily_rt_tw_4  daily_rt_tw_5  daily_rt_tw_6  daily_rt_0  daily_rt_1  \\\n",
       "0           0.182879       0.128405       0.198444    0.187500    0.087500   \n",
       "1           0.000000       0.000000       0.200000    0.000000    0.250000   \n",
       "2           0.100000       0.200000       0.200000    0.100000    0.200000   \n",
       "3           0.375000       0.125000       0.166667    0.000000    0.250000   \n",
       "4           0.100000       0.200000       0.000000    0.000000    0.000000   \n",
       "...              ...            ...            ...         ...         ...   \n",
       "11831       0.000000       0.272727       0.000000    0.090909    0.363636   \n",
       "11832       0.400000       0.000000       0.100000    0.100000    0.100000   \n",
       "11833       0.000000       0.000000       0.000000    0.250000    0.000000   \n",
       "11834       0.400000       0.000000       0.066667    0.222222    0.000000   \n",
       "11835       0.000000       1.000000       0.000000    0.000000    0.000000   \n",
       "\n",
       "       daily_rt_2  daily_rt_3  daily_rt_4  daily_rt_5  daily_rt_6  daily_tw_0  \\\n",
       "0        0.062500    0.000000    0.187500    0.175000    0.300000    0.175141   \n",
       "1        0.000000    0.500000    0.000000    0.000000    0.250000    0.000000   \n",
       "2        0.000000    0.200000    0.100000    0.200000    0.200000    0.000000   \n",
       "3        0.250000    0.250000    0.000000    0.250000    0.000000    0.000000   \n",
       "4        0.666667    0.000000    0.000000    0.333333    0.000000    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.090909    0.181818    0.000000    0.272727    0.000000    0.000000   \n",
       "11832    0.100000    0.200000    0.400000    0.000000    0.100000    0.000000   \n",
       "11833    0.250000    0.500000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834    0.111111    0.222222    0.333333    0.000000    0.111111    0.000000   \n",
       "11835    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       daily_tw_1  daily_tw_2  daily_tw_3  daily_tw_4  daily_tw_5  daily_tw_6  \\\n",
       "0        0.124294    0.158192    0.101695    0.180791    0.107345    0.152542   \n",
       "1        1.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2        0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "3        0.250000    0.000000    0.000000    0.450000    0.100000    0.200000   \n",
       "4        0.571429    0.000000    0.142857    0.142857    0.142857    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11832    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11833    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834    0.000000    0.166667    0.333333    0.500000    0.000000    0.000000   \n",
       "11835    0.000000    0.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "\n",
       "       hour_rt_tw_0  hour_rt_tw_1  hour_rt_tw_2  hour_rt_tw_3  hour_rt_tw_4  \\\n",
       "0               0.0      0.000000           0.0      0.019455      0.015564   \n",
       "1               0.0      0.000000           0.0      0.000000      0.000000   \n",
       "2               0.0      0.100000           0.0      0.100000      0.000000   \n",
       "3               0.0      0.000000           0.0      0.041667      0.083333   \n",
       "4               0.0      0.000000           0.1      0.100000      0.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "11831           0.0      0.272727           0.0      0.272727      0.000000   \n",
       "11832           0.1      0.000000           0.1      0.000000      0.000000   \n",
       "11833           0.0      0.500000           0.0      0.000000      0.000000   \n",
       "11834           0.0      0.000000           0.0      0.000000      0.000000   \n",
       "11835           1.0      0.000000           0.0      0.000000      0.000000   \n",
       "\n",
       "       hour_rt_tw_5  hour_rt_tw_6  hour_rt_tw_7  hour_rt_tw_8  hour_rt_tw_9  \\\n",
       "0          0.042802      0.035019      0.007782      0.038911      0.046693   \n",
       "1          0.000000      0.000000      0.000000      0.200000      0.000000   \n",
       "2          0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3          0.208333      0.000000      0.083333      0.000000      0.000000   \n",
       "4          0.000000      0.000000      0.100000      0.000000      0.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "11831      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "11832      0.100000      0.000000      0.100000      0.000000      0.000000   \n",
       "11833      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "11834      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "11835      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "       hour_rt_tw_10  hour_rt_tw_11  hour_rt_tw_12  hour_rt_tw_13  \\\n",
       "0           0.054475       0.011673       0.019455       0.042802   \n",
       "1           0.000000       0.000000       0.000000       0.000000   \n",
       "2           0.000000       0.100000       0.000000       0.200000   \n",
       "3           0.000000       0.000000       0.000000       0.000000   \n",
       "4           0.000000       0.500000       0.000000       0.200000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.000000       0.000000       0.000000       0.000000   \n",
       "11832       0.000000       0.000000       0.100000       0.200000   \n",
       "11833       0.000000       0.000000       0.000000       0.000000   \n",
       "11834       0.000000       0.066667       0.066667       0.133333   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       hour_rt_tw_14  hour_rt_tw_15  hour_rt_tw_16  hour_rt_tw_17  \\\n",
       "0           0.073930       0.077821       0.066148       0.105058   \n",
       "1           0.000000       0.000000       0.000000       0.000000   \n",
       "2           0.200000       0.000000       0.000000       0.000000   \n",
       "3           0.000000       0.041667       0.041667       0.000000   \n",
       "4           0.000000       0.000000       0.000000       0.000000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.000000       0.181818       0.000000       0.000000   \n",
       "11832       0.000000       0.100000       0.000000       0.000000   \n",
       "11833       0.000000       0.000000       0.000000       0.000000   \n",
       "11834       0.133333       0.000000       0.333333       0.133333   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       hour_rt_tw_18  hour_rt_tw_19  hour_rt_tw_20  hour_rt_tw_21  \\\n",
       "0           0.116732       0.163424       0.054475       0.007782   \n",
       "1           0.000000       0.000000       0.200000       0.200000   \n",
       "2           0.000000       0.000000       0.100000       0.000000   \n",
       "3           0.000000       0.000000       0.041667       0.041667   \n",
       "4           0.000000       0.000000       0.000000       0.000000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.000000       0.000000       0.000000       0.000000   \n",
       "11832       0.000000       0.200000       0.000000       0.000000   \n",
       "11833       0.000000       0.000000       0.000000       0.250000   \n",
       "11834       0.066667       0.000000       0.066667       0.000000   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       hour_rt_tw_22  hour_rt_tw_23  hour_tw_0  hour_tw_1  hour_tw_2  \\\n",
       "0           0.000000       0.000000        0.0        0.0   0.000000   \n",
       "1           0.000000       0.400000        0.0        0.0   0.000000   \n",
       "2           0.100000       0.100000        0.0        0.0   0.000000   \n",
       "3           0.375000       0.041667        0.0        0.0   0.000000   \n",
       "4           0.000000       0.000000        0.0        0.0   0.142857   \n",
       "...              ...            ...        ...        ...        ...   \n",
       "11831       0.181818       0.090909        0.0        0.0   0.000000   \n",
       "11832       0.000000       0.000000        0.0        0.0   0.000000   \n",
       "11833       0.250000       0.000000        0.0        0.0   0.000000   \n",
       "11834       0.000000       0.000000        0.0        0.0   0.000000   \n",
       "11835       0.000000       0.000000        1.0        0.0   0.000000   \n",
       "\n",
       "       hour_tw_3  hour_tw_4  hour_tw_5  hour_tw_6  hour_tw_7  hour_tw_8  \\\n",
       "0       0.028249    0.00565   0.050847   0.045198   0.000000   0.028249   \n",
       "1       0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "2       0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "3       0.050000    0.05000   0.250000   0.000000   0.000000   0.000000   \n",
       "4       0.142857    0.00000   0.000000   0.000000   0.142857   0.000000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11832   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11833   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11834   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11835   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "       hour_tw_9  hour_tw_10  hour_tw_11  hour_tw_12  hour_tw_13  hour_tw_14  \\\n",
       "0       0.039548    0.056497    0.011299    0.022599    0.056497    0.084746   \n",
       "1       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "3       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4       0.000000    0.000000    0.428571    0.000000    0.142857    0.000000   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "11831   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11832   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11833   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834   0.000000    0.000000    0.000000    0.166667    0.166667    0.166667   \n",
       "11835   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       hour_tw_15  hour_tw_16  hour_tw_17  hour_tw_18  hour_tw_19  hour_tw_20  \\\n",
       "0        0.073446    0.067797    0.101695    0.129944    0.146893    0.045198   \n",
       "1        0.000000    0.000000    0.000000    0.000000    0.000000    1.000000   \n",
       "2        0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "3        0.050000    0.000000    0.000000    0.000000    0.000000    0.050000   \n",
       "4        0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11832    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11833    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834    0.000000    0.166667    0.166667    0.000000    0.000000    0.166667   \n",
       "11835    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       hour_tw_21  hour_tw_22  hour_tw_23  hour_rt_0  hour_rt_1  hour_rt_2  \\\n",
       "0         0.00565        0.00        0.00        0.0   0.000000        0.0   \n",
       "1         0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "2         0.00000        0.00        0.00        0.0   0.100000        0.0   \n",
       "3         0.05000        0.45        0.05        0.0   0.000000        0.0   \n",
       "4         0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "...           ...         ...         ...        ...        ...        ...   \n",
       "11831     0.00000        0.00        0.00        0.0   0.272727        0.0   \n",
       "11832     0.00000        0.00        0.00        0.1   0.000000        0.1   \n",
       "11833     0.00000        0.00        0.00        0.0   0.500000        0.0   \n",
       "11834     0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "11835     0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "\n",
       "       hour_rt_3  hour_rt_4  hour_rt_5  hour_rt_6  hour_rt_7  hour_rt_8  \\\n",
       "0       0.000000     0.0375      0.025     0.0125      0.025     0.0625   \n",
       "1       0.000000     0.0000      0.000     0.0000      0.000     0.2500   \n",
       "2       0.100000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "3       0.000000     0.2500      0.000     0.0000      0.500     0.0000   \n",
       "4       0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.272727     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "11832   0.000000     0.0000      0.100     0.0000      0.100     0.0000   \n",
       "11833   0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "11834   0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "11835   0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "\n",
       "       hour_rt_9  hour_rt_10  hour_rt_11  hour_rt_12  hour_rt_13  hour_rt_14  \\\n",
       "0         0.0625        0.05    0.012500      0.0125    0.012500    0.050000   \n",
       "1         0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "2         0.0000        0.00    0.100000      0.0000    0.200000    0.200000   \n",
       "3         0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "4         0.0000        0.00    0.666667      0.0000    0.333333    0.000000   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "11831     0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "11832     0.0000        0.00    0.000000      0.1000    0.200000    0.000000   \n",
       "11833     0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "11834     0.0000        0.00    0.111111      0.0000    0.111111    0.111111   \n",
       "11835     0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "\n",
       "       hour_rt_15  hour_rt_16  hour_rt_17  hour_rt_18  hour_rt_19  hour_rt_20  \\\n",
       "0        0.087500    0.062500    0.112500    0.087500         0.2       0.075   \n",
       "1        0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "2        0.000000    0.000000    0.000000    0.000000         0.0       0.100   \n",
       "3        0.000000    0.250000    0.000000    0.000000         0.0       0.000   \n",
       "4        0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.181818    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "11832    0.100000    0.000000    0.000000    0.000000         0.2       0.000   \n",
       "11833    0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "11834    0.000000    0.444444    0.111111    0.111111         0.0       0.000   \n",
       "11835    0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "\n",
       "       hour_rt_21  hour_rt_22  hour_rt_23  mst_fr_ment_tw_1  mst_fr_ment_tw_2  \\\n",
       "0          0.0125    0.000000    0.000000          2.011174          5.980586   \n",
       "1          0.2500    0.000000    0.500000          5.951101          2.923159   \n",
       "2          0.0000    0.100000    0.100000         -0.100000         -0.100000   \n",
       "3          0.0000    0.000000    0.000000          6.896686          7.102824   \n",
       "4          0.0000    0.000000    0.000000          6.896686         10.566221   \n",
       "...           ...         ...         ...               ...               ...   \n",
       "11831      0.0000    0.181818    0.090909         -0.100000         -0.100000   \n",
       "11832      0.0000    0.000000    0.000000         -0.100000         -0.100000   \n",
       "11833      0.2500    0.250000    0.000000         -0.100000         -0.100000   \n",
       "11834      0.0000    0.000000    0.000000          4.803406          2.923159   \n",
       "11835      0.0000    0.000000    0.000000         -0.100000         -0.100000   \n",
       "\n",
       "       mst_fr_ment_tw_3  mst_fr_ment_rt_1  mst_fr_ment_rt_2  mst_fr_ment_rt_3  \\\n",
       "0              6.887675          2.011174          5.980586          9.177593   \n",
       "1              2.011174          2.011174          9.150005          9.552075   \n",
       "2             -0.100000         11.871754          6.725062          5.916363   \n",
       "3              2.011174          5.161735          5.840240          6.162355   \n",
       "4             10.750414          6.268109          5.161735         -0.100000   \n",
       "...                 ...               ...               ...               ...   \n",
       "11831         -0.100000          7.810123         11.525478          8.102567   \n",
       "11832         -0.100000          2.011174          4.176059          4.977433   \n",
       "11833         -0.100000          4.977433          5.120814         -0.100000   \n",
       "11834         -0.100000          2.923159          4.669607          8.837133   \n",
       "11835         -0.100000         -0.100000         -0.100000         -0.100000   \n",
       "\n",
       "       mst_fr_hs_tw_1  mst_fr_hs_tw_2  mst_fr_hs_tw_3  mst_fr_hs_rt_1  \\\n",
       "0            1.266133        6.667747        3.043134        1.266133   \n",
       "1            1.266133       -0.100000       -0.100000        1.266133   \n",
       "2           -0.100000       -0.100000       -0.100000        1.266133   \n",
       "3            1.266133        3.585302       12.348678        4.551958   \n",
       "4            1.266133       -0.100000       -0.100000        1.266133   \n",
       "...               ...             ...             ...             ...   \n",
       "11831       -0.100000       -0.100000       -0.100000        2.415885   \n",
       "11832       -0.100000       -0.100000       -0.100000        2.084814   \n",
       "11833       -0.100000       -0.100000       -0.100000        2.415885   \n",
       "11834        2.084814        7.183257        7.119354        2.084814   \n",
       "11835        2.084814       -0.100000       -0.100000       -0.100000   \n",
       "\n",
       "       mst_fr_hs_rt_2  mst_fr_hs_rt_3  tw_urls_avg  tw_urls_std  rt_urls_avg  \\\n",
       "0            4.803376        6.667747     0.169492     0.411693     0.187500   \n",
       "1            6.059891       -0.100000     0.000000     0.000000     0.000000   \n",
       "2            2.415885        2.488408     0.000000     0.000000     0.400000   \n",
       "3            8.989075        7.998400     0.700000     0.836660     0.750000   \n",
       "4            6.516144       -0.100000     0.285714     0.534522     0.666667   \n",
       "...               ...             ...          ...          ...          ...   \n",
       "11831        4.150451        3.265572     0.000000     0.000000     0.272727   \n",
       "11832        2.571623        2.415885     0.000000     0.000000     0.400000   \n",
       "11833        2.084814        5.213256     0.000000     0.000000     0.250000   \n",
       "11834        2.488408        3.265572     0.666667     0.816497     0.666667   \n",
       "11835       -0.100000       -0.100000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       rt_urls_std  tw_hash_avg  tw_hash_std  tw_ment_avg  tw_ment_std  \\\n",
       "0         0.433013     5.016949     5.633836     1.988701     2.665960   \n",
       "1         0.000000     1.000000     1.000000     3.000000     3.000000   \n",
       "2         0.632456     0.000000     0.000000     0.000000     0.000000   \n",
       "3         0.866025     0.500000     0.836660     1.750000     1.987461   \n",
       "4         0.816497     0.714286     0.845154     1.142857     1.195229   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     0.522233     0.000000     0.000000     0.000000     0.000000   \n",
       "11832     0.632456     0.000000     0.000000     0.000000     0.000000   \n",
       "11833     0.500000     0.000000     0.000000     0.000000     0.000000   \n",
       "11834     0.816497     2.500000     2.972092     0.333333     0.577350   \n",
       "11835     0.000000     1.000000     1.000000     0.000000     0.000000   \n",
       "\n",
       "       rt_hash_avg  rt_hash_std  rt_ment_avg  rt_ment_std  rt_time_avg  \\\n",
       "0         3.962500     4.602988     1.900000     2.915476   171.483958   \n",
       "1         1.250000     1.322876     1.250000     2.061553   169.037500   \n",
       "2         1.600000     2.049390     0.800000     1.264911   513.326667   \n",
       "3         2.250000     2.692582     0.750000     1.500000   362.737500   \n",
       "4         0.666667     1.154701     0.666667     1.154701   697.972222   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     2.636364     3.343923     0.454545     0.674200   211.589394   \n",
       "11832     2.200000     2.569047     0.800000     1.341641   337.120000   \n",
       "11833     1.750000     1.936492     0.500000     1.000000     6.033333   \n",
       "11834     3.222222     5.364492     1.000000     1.452966   317.183333   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       rt_time_min  rt_time_max  rt_time_std    rt_avg    tw_avg  \\\n",
       "0         0.166667  1292.233333   330.839721  2.758621  6.103448   \n",
       "1         7.183333   432.416667   241.493538  0.137931  0.034483   \n",
       "2         1.600000  1020.416667   635.460221  0.344828  0.000000   \n",
       "3        12.116667   733.633333   475.360941  0.137931  0.689655   \n",
       "4       521.350000   861.083333   711.683184  0.103448  0.241379   \n",
       "...            ...          ...          ...       ...       ...   \n",
       "11831     5.583333  1145.433333   378.904435  0.379310  0.000000   \n",
       "11832    50.016667   962.716667   444.474572  0.344828  0.000000   \n",
       "11833     0.316667    19.533333     9.885420  0.137931  0.000000   \n",
       "11834     0.566667  1312.000000   593.041953  0.310345  0.206897   \n",
       "11835     0.000000     0.000000     0.000000  0.000000  0.034483   \n",
       "\n",
       "       tw_rt_ration  verified  followers  favourites  listed  statuses  \\\n",
       "0          2.212500         0        343        7349       0      3502   \n",
       "1          0.250000         0        172        1463       0      1057   \n",
       "2          0.000000         0        281       36991       4     37368   \n",
       "3          5.000000         0        136        8697       0      4739   \n",
       "4          2.333333         0        241        5757       0      9201   \n",
       "...             ...       ...        ...         ...     ...       ...   \n",
       "11831      0.000000         0        341       88977       0     15392   \n",
       "11832      0.000000         0        266        4146       4      2577   \n",
       "11833      0.000000         0         96       22348       0     29246   \n",
       "11834      0.666667         0       2580        4078      97    122644   \n",
       "11835      0.000000         1    1117713        8621    1244     24671   \n",
       "\n",
       "       followers.1  friends_count  name_len  name_screen_sim  geo  protected  \\\n",
       "0              343            151        14         0.416667    0          0   \n",
       "1              172            403        21         0.461538    0          0   \n",
       "2              281            462        15         0.312500    0          0   \n",
       "3              136            163         4         0.800000    0          0   \n",
       "4              241            492         2         0.111111    0          0   \n",
       "...            ...            ...       ...              ...  ...        ...   \n",
       "11831          341            963         5         0.090909    0          0   \n",
       "11832          266            564         9         0.333333    0          0   \n",
       "11833           96            159         3         0.090909    0          0   \n",
       "11834         2580            961        11         0.150000    0          0   \n",
       "11835      1117713            597        14         0.666667    1          0   \n",
       "\n",
       "       location  description  description_len  bckg_img  default_prof  \\\n",
       "0             0            1               86         1             1   \n",
       "1             0            1               83         1             1   \n",
       "2             0            1               17         1             1   \n",
       "3             1            1               66         1             1   \n",
       "4             0            1               25         1             1   \n",
       "...         ...          ...              ...       ...           ...   \n",
       "11831         1            0                0         1             0   \n",
       "11832         0            1               46         1             1   \n",
       "11833         0            0                0         1             1   \n",
       "11834         1            1               61         1             0   \n",
       "11835         0            1               31         1             0   \n",
       "\n",
       "       entities  rt_self in_degree out_degree w_in_degree w_out_degree  \\\n",
       "0             0        1       115          0         576          166   \n",
       "1             1        0         0          0           0           11   \n",
       "2             0        0         0         34           0           47   \n",
       "3             0        0         6         12           8           13   \n",
       "4             0        0        10         15          19           15   \n",
       "...         ...      ...       ...        ...         ...          ...   \n",
       "11831         0        0         0         31           0           41   \n",
       "11832         0        0         0         25           0           28   \n",
       "11833         0        0         0         20           0           30   \n",
       "11834         0        0        53          0          89           97   \n",
       "11835         1        0       569          0         903            1   \n",
       "\n",
       "      w_degree  target              user_id  hs_tw_v_0  hs_tw_v_1  hs_tw_v_2  \\\n",
       "0          742       1  1258962548503740425   3.493951   7.603045  -1.862264   \n",
       "1           11       1  1277260249687261186   3.493951   7.603045  -1.862264   \n",
       "2           47       1   746102335915556868   0.000000   0.000000   0.000000   \n",
       "3           21       1  1231691345271586824   3.493951   7.603045  -1.862264   \n",
       "4           34       1           1518469190   3.493951   7.603045  -1.862264   \n",
       "...        ...     ...                  ...        ...        ...        ...   \n",
       "11831       41       0           1043726264   0.000000   0.000000   0.000000   \n",
       "11832       28       0           2542010298   0.000000   0.000000   0.000000   \n",
       "11833       30       0           2520645596   0.000000   0.000000   0.000000   \n",
       "11834      186       0             14811111  -6.481514  -0.044037   7.989522   \n",
       "11835      904       0           1064042478  -6.481514  -0.044037   7.989522   \n",
       "\n",
       "       hs_tw_v_3  hs_tw_v_4  hs_tw_v_5  hs_tw_v_6  hs_tw_v_7  hs_tw_v_8  \\\n",
       "0      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "1      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "2       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "4      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11832   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11833   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11834  11.762167   6.681043  -3.275398   4.377571   5.375144  -5.796048   \n",
       "11835  11.762167   6.681043  -3.275398   4.377571   5.375144  -5.796048   \n",
       "\n",
       "       hs_tw_v_9  hs_rt_v_0  hs_rt_v_1  hs_rt_v_2  hs_rt_v_3  hs_rt_v_4  \\\n",
       "0      -4.642322   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "1      -4.642322   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "2       0.000000   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "3      -4.642322  -4.284786   7.481483   6.267010   4.508372  11.494592   \n",
       "4      -4.642322   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.000000  -5.481577  -4.951790  11.181436   8.863973   3.241972   \n",
       "11832   0.000000  -6.481514  -0.044037   7.989522  11.762167   6.681043   \n",
       "11833   0.000000  -5.481577  -4.951790  11.181436   8.863973   3.241972   \n",
       "11834  -2.582479  -6.481514  -0.044037   7.989522  11.762167   6.681043   \n",
       "11835  -2.582479   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "       hs_rt_v_5  hs_rt_v_6  hs_rt_v_7  hs_rt_v_8  hs_rt_v_9  ment_tw_v_0  \\\n",
       "0       4.668619   3.420092  -1.148644  -2.369110  -4.642322     0.282885   \n",
       "1       4.668619   3.420092  -1.148644  -2.369110  -4.642322    -2.666591   \n",
       "2       4.668619   3.420092  -1.148644  -2.369110  -4.642322     0.000000   \n",
       "3      -1.333691  -2.366664   0.944676  -3.689538  -7.882071     1.424672   \n",
       "4       4.668619   3.420092  -1.148644  -2.369110  -4.642322     1.424672   \n",
       "...          ...        ...        ...        ...        ...          ...   \n",
       "11831  -4.242408  -0.542843   3.072947  -7.411512  -3.821877     0.000000   \n",
       "11832  -3.275398   4.377571   5.375144  -5.796048  -2.582479     0.000000   \n",
       "11833  -4.242408  -0.542843   3.072947  -7.411512  -3.821877     0.000000   \n",
       "11834  -3.275398   4.377571   5.375144  -5.796048  -2.582479    -1.483979   \n",
       "11835   0.000000   0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "\n",
       "       ment_tw_v_1  ment_tw_v_2  ment_tw_v_3  ment_tw_v_4  ment_tw_v_5  \\\n",
       "0         4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "1         3.297360     0.108515     1.148872    -4.909317     4.969856   \n",
       "2         0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "3         0.269191     2.436769     3.364856    -2.865908     5.763546   \n",
       "4         0.269191     2.436769     3.364856    -2.865908     5.763546   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "11832     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "11833     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "11834     1.254190     9.255416     5.533646     1.226344     4.875967   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       ment_tw_v_6  ment_tw_v_7  ment_tw_v_8  ment_tw_v_9  ment_rt_v_0  \\\n",
       "0         0.595667    -2.255075    -0.706327    -2.917101     0.282885   \n",
       "1         2.520902     0.120532    -3.473754    -1.942731     0.282885   \n",
       "2         0.000000     0.000000     0.000000     0.000000     0.736998   \n",
       "3         3.289723    -7.134625    -1.938984     3.373724    -0.913174   \n",
       "4         3.289723    -7.134625    -1.938984     3.373724     2.414909   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     0.000000     0.000000     0.000000     0.000000    -4.205255   \n",
       "11832     0.000000     0.000000     0.000000     0.000000     0.282885   \n",
       "11833     0.000000     0.000000     0.000000     0.000000    -6.206959   \n",
       "11834     0.917352    -4.030603    -2.897294    -0.845497    -5.901112   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       ment_rt_v_1  ment_rt_v_2  ment_rt_v_3  ment_rt_v_4  ment_rt_v_5  \\\n",
       "0         4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "1         4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "2        -0.069259    -0.292788    -0.561506    -0.082709     0.042723   \n",
       "3         4.238219    -0.241484     4.171506    -2.921207     6.198004   \n",
       "4        -3.960131    -0.615269     2.886237    -8.917027     2.754604   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831   -13.955618     1.322916     3.720930    -6.821259   -13.958460   \n",
       "11832     4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "11833    -2.618856     1.212161    -3.037526    -1.402811     1.444823   \n",
       "11834     3.408430     5.464797     5.483321    -3.814475     3.054348   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       ment_rt_v_6  ment_rt_v_7  ment_rt_v_8  ment_rt_v_9  \n",
       "0         0.595667    -2.255075    -0.706327    -2.917101  \n",
       "1         0.595667    -2.255075    -0.706327    -2.917101  \n",
       "2         0.180475    -0.226081     0.147200     0.524840  \n",
       "3         3.403417    -6.699732    -2.091649    -0.219933  \n",
       "4        -1.793620    -7.303990    -5.697265     4.512267  \n",
       "...            ...          ...          ...          ...  \n",
       "11831    -3.371006    -5.193164     2.854197     0.453024  \n",
       "11832     0.595667    -2.255075    -0.706327    -2.917101  \n",
       "11833    12.596374     0.780529     6.377641    -2.976061  \n",
       "11834     1.276991     0.458579    -1.790708    -1.426023  \n",
       "11835     0.000000     0.000000     0.000000     0.000000  \n",
       "\n",
       "[11836 rows x 189 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------!!!-------------------\n",
    "#-----------------IMPORTANT-----------------\n",
    "#Execute ONLY of case of data/labels changes\n",
    "#-----------------IMPORTANT-----------------\n",
    "#-------------------------------------------\n",
    "\n",
    "combine_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution main frame\n",
    "### 1) Read CSV file into dataframe \n",
    "### 2) Balance data by target (50/50)\n",
    "### 3) Perform One-Hot of text features\n",
    "### As result, return dataframe, target of the dataframe and user_ids vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "#used for testing of reading functions\n",
    "#Read data from CSV file into dataframe with balancing by target values and one-hot of text features\n",
    "\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "X_train, y_train, X_test, y_test = read_data(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection - Parameter fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAI/CAYAAADQs2XyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAB0y0lEQVR4nO3dd3yN5//H8deVJWJnmIm9ZxBq02Ur1aKULoq2uqvV/f11aukwiypVqjVa3VupUUVi71ErttiJkXH9/rgPRZUgOXfG+/l4eMg55z7nfM7tlrxzTWOtRURERES8x8ftAkRERESyGwUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES/zc7uAKxEaGmpLlizpdhkiIiIilxUTE3PAWht2sccyVQArWbIk0dHRbpchIiIiclnGmG3/9Zi6IEVERES8TAFMRERExMsUwERERES8LFONARMREZH0lZiYSGxsLCdPnnS7lEwjMDCQ8PBw/P39U/0cBTARERE5KzY2ljx58lCyZEmMMW6Xk+FZa4mLiyM2NpZSpUql+nnqghQREZGzTp48SUhIiMJXKhljCAkJueIWQwUwEREROY/C15W5mvOlACYiIiLiZQpgIiIikqFNmzaNSpUqcf3111/T62zdupXJkydf9fPfeOONa3r/c6UqgBljWhpj1htjNhljBlzk8QLGmBnGmBXGmEXGmKqe+wM9t5cbY1YbY/7vguc97Hnd1caYt9PmI4mIiEhWYa3lww8/ZOTIkcyaNStVz0lKSrro/ZkqgBljfIERQCugMtDVGFP5gsOeA5ZZa6sDdwFDPPefAm6w1tYAIoGWxph6nte9HmgPVLfWVgEGX/vHERERkcxu69atVKpUiQcffBAfHx9+/fVX+vbtS//+/Tl58iT33nsv1apVo2bNmmdD2ccff0ynTp1o164dzZs3v+jrDhgwgLlz5xIZGcl7771HcnIy/fv3p06dOlSvXp3Ro0cDsHv3bpo0aUJkZCRVq1Zl7ty5DBgwgBMnThAZGcmdd955zZ8xNctQ1AU2WWv/BjDGfI4TnNacc0xl4E0Aa+06Y0xJY0wha+1e4LjnGH/PH+u5/QAw0Fp7yvO8fdf6YURERCTt/N+3q1mz62iavmblonl5uV2Vyx63fv16xo8fz8iRI2nWrBmDBw8mKiqKd955B4CVK1eybt06mjdvzoYNGwBYsGABK1asIDg4+KKvOXDgQAYPHsx3330HwJgxY8iXLx+LFy/m1KlTNGzYkObNm/Pll1/SokULnn/+eZKTk0lISKBx48YMHz6cZcuWpcl5SE0AKwbsOOd2LHDdBccsBzoC84wxdYESQDiw19OCFgOUBUZYaxd6nlMeaGyMeR04CTxlrV181Z9EREREsowSJUpQr169f90/b948Hn74YQAqVqxIiRIlzgawm2+++T/D18X88ssvrFixgunTpwNw5MgRNm7cSJ06dbjvvvtITEykQ4cOREZGXvsHukBqAtjF5lbaC24PBIYYY5YBK4GlQBKAtTYZiDTG5AdmGGOqWmtXed67AFAPqANMNcaUttae99rGmN5Ab4DixYun8mOJiIjItUpNS1V6yZUr10XvvyAmpOo5/8Vay7Bhw2jRosW/HpszZw7ff/89PXr0oH///tx1111X9NqXk5pB+LFAxDm3w4Fd5x5grT1qrb3XWhuJMwYsDNhywTGHgdlAy3Ne90vrWASkAKEXvrm1doy1NspaGxUWFpaazyQiIiJZVJMmTfj0008B2LBhA9u3b6dChQqpem6ePHk4duzY2dstWrTggw8+IDEx8ezrxcfHs23bNgoWLMj9999Pz549WbJkCQD+/v5nj71WqQlgi4FyxphSxpgA4A7gm3MPMMbk9zwG0AuYY609aowJ87R8YYzJCdwErPMc9xVwg+ex8kAAcODaPo6IiIhkZQ8++CDJyclUq1aNLl268PHHH5MjR45UPbd69er4+flRo0YN3nvvPXr16kXlypWpVasWVatWpU+fPiQlJTF79mwiIyOpWbMmX3zxBY8++igAvXv3pnr16mkyCN9cqinv7EHGtAbeB3yBcdba140xfQGstaOMMfWBT4BknMH5Pa21h4wx1YEJnuf5AFOtta94XjMAGIczO/I0zhiw3y9VR1RUlI2Ojr6azykiIiKpsHbtWipVquR2GZnOxc6bMSbGWht1seNTtRm3tfYH4IcL7ht1ztcLgHIXed4KoOZ/vOZpoHtq3l9EREQkK0lVABMRERHJLFauXEmPHj3Ouy9HjhwsXLjwP57hfQpgIiIikqVUq1YtzdbrSi/aC1JERETEyxTARERERLxMAUxERDKNdXuOcufYv/hx5W63SxG5JgpgIiKSKfy0ajcdR/7Jn5vjeGjyEqbHxLpdkmQis2fP5s8//7yq527dupXJkyenaT0KYCIikqGlpFje/XUDfSctoULhPMx6shkNyoTy1LTlTFyw1e3yJJNQABMREUml46eS6DMphqEzN9Kpdjif965HydBcjL07ipsqFeLFr1cz+o/NbpcpaSw+Pp42bdpQo0YNqlatyoQJE+jcufPZx2fPnk27du0A+Omnn6hVqxY1atTgxhtvvOjrbd26lVGjRvHee+8RGRnJ3Llz2b9/P7fddht16tShTp06zJ8/H4A//viDyMjIsyvhHzt2jAEDBjB37lwiIyN577330uQzahkKERHJkLYeiOf+T6L5+0A8/2tXmbsblMQYA0Cgvy8fdK/F41OW8eaP64g/nczjN5U7+7hkbj/99BNFixbl+++/B+DIkSO8+OKLxMfHkytXLqZMmUKXLl3Yv38/999/P3PmzKFUqVIcPHjwoq9XsmRJ+vbtS+7cuXnqqacA6NatG48//jiNGjVi+/bttGjRgrVr1zJ48GBGjBhBw4YNOX78OIGBgQwcOJDBgwfz3XffpdlnVAATEZEMZ86G/fSbvARfH8PE++rSoGzov47x9/VhyB01CQrwZejMjSScSuL5NpUUwtLSjwNgz8q0fc3C1aDVwEseUq1aNZ566imeeeYZ2rZtS+PGjWnZsiXffvstt99+O99//z1vv/02s2fPpkmTJpQqVQqA4ODgVJfx22+/sWbNmrO3jx49yrFjx2jYsCFPPPEEd955Jx07diQ8PPzqPudlKICJiEiGYa1l7NwtvPnjWsoXysOHd0URERz0n8f7+hgGdqxOUIAfY+dtISExmdfaV8XHRyEsMytfvjwxMTH88MMPPPvsszRv3pwuXbowYsQIgoODqVOnDnny5MFae9WBOyUlhQULFpAzZ87z7h8wYABt2rThhx9+oF69evz2229p8ZH+RQFMREQyhJOJyTz75UpmLN1J62qFGdypBkEBl/8x5eNjeLldZYICfBk5ezMnTicz6Pbq+PlqmPM1u0xLVXrZtWsXwcHBdO/endy5c/Pxxx/z/PPP07NnTz788EO6dOkCQP369XnooYfYsmXL2S7I/2oFy5MnD0ePHj17u3nz5gwfPpz+/fsDsGzZMiIjI9m8eTPVqlWjWrVqLFiwgHXr1hEREcGxY8fS9DPq6hQREdftPnKCTqMW8NWynTzVvDwjutVKVfg6wxjD0y0r0r9FBWYs3Um/yUs5lZScjhVLelq5ciV169YlMjKS119/nRdeeAFfX1/atm3Ljz/+SNu2bQEICwtjzJgxdOzYkRo1apwNZhfTrl07ZsyYcXYQ/tChQ4mOjqZ69epUrlyZUaNGAfD+++9TtWpVatSoQc6cOWnVqhXVq1fHz8+PGjVqpNkgfGOtTZMX8oaoqCgbHR3tdhkiIpKGorcepO+kJZxMTOb9LpHcVLnQNb3euHlbeOW7NTQtH8boHrUJ9PdNo0qzh7Vr11KpUiW3y8h0LnbejDEx1tqoix2vFjAREXHNZ4u20/XDv8gT6MdXDzW45vAFcF+jUrx1WzXmbNzP3eMWcfxUUhpUKpK2NAZMRES87nRSCq98t5pJf22nafkwhnatSb6c/mn2+l3qFCfQ35cnpi6n+9iFTLi3LvmC0u71JeMaP348Q4YMOe++hg0bMmLECJcqujgFMBER8aoDx0/x4KdLWLTlIH2alubpFhXxTYdZi+0ji5HT35d+k5dyx4d/MbFnXUJz50jz95GM5d577+Xee+91u4zLUhekiIh4zaqdR2g/fD7LdxxmyB2RPNuqUrqErzOaVynM2Luj2HLgOF1GL2DPkZPp9l4iV0IBTEREvOKb5bu4fdSfWGuZ3rcB7SOLeeV9m5QP45P7rmPv0VN0Gv0nOw4meOV9RS5FAUxERNJVcopl4I/reOSzpVQvlp9vHm5EtfB8Xq2hbqlgPu11HcdOJtFp1AI27Tvu1fcXuZACmIiIpJsjJxLpOWExo/7YTPd6xZnU6zrXxmHViMjP573rkZSSQpfRC1i7++jlnySSThTAREQkXWzad5xbR8xn3sYDvH5rVV7rUI0AP3d/7FQsnJepfeoT4OfDHWP+YtmOw67WI9mXApiIiKS5mWv3cuuI+Rw9mchnvetx53Ul3C7prNJhuZnapz75cvpz54d/sfDvOLdLkgvkzp071ccePnyYkSNHXvV7vf/++yQkeH9coAKYiIikGWstI2Ztotcn0ZQIDeKbfo2oU/Lie/O5KSI4iGl961Mkf07uHr+IPzbsd7skuYzk5ItvLaUAJiIi2VrC6ST6fbaUQT+v55YaRZnetwFF8+d0u6z/VChvIFN616N0aG56TVjMT6v2uF2SXGD27Nlcf/31dOvWjWrVql30mAEDBrB582YiIyPPbqw9aNAg6tSpQ/Xq1Xn55ZcBiI+Pp02bNtSoUYOqVasyZcoUhg4dyq5du7j++uu5/vrrvfa5QAuxiohIGthxMIHeE2NYv+coz7WuyP2NS2NM+q3vlVZCcufgs971uGf8Ih6avIR3OtWgQ03vLI+RGby16C3WHVyXpq9ZMbgiz9R9JtXHL1q0iFWrVlGqVKmLPj5w4EBWrVrFsmXLAPjll1/YuHEjixYtwlrLLbfcwpw5c9i/fz9Fixbl+++/B+DIkSPky5ePd999l1mzZhEaGnrNn+1KqAVMRESuyYLNcdwyfB47DyUw/t669G5SJlOErzPy5fRnUs/rqFsymMenLmPywu1ulyTnqFu37n+Gr4v55Zdf+OWXX6hZsya1atVi3bp1bNy4kWrVqvHbb7/xzDPPMHfuXPLl8+5SKBdSC5iIiFwVay2fLNjGK9+toVRoLj68K4pSobncLuuq5Mrhx/h76/DApBiem7GShNNJ9Gpc2u2yXHclLVXpJVeuK7umrLU8++yz9OnT51+PxcTE8MMPP/Dss8/SvHlzXnrppbQq84qpBewcy3Yc5qHJS9h7VFtViIhcyqmkZAZ8sZKXv1nN9RXCmPFgg0wbvs4I9PdldI8oWlcrzGvfr2XYzI1Ya90uSy4jT548HDt27OztFi1aMG7cOI4fdxbb3blzJ/v27WPXrl0EBQXRvXt3nnrqKZYsWXLR53uLWsDOsWnfcX5bs5c/1u/nqebl6VG/ZLruUSYikhntO3qSvpNiWLL9MI/cUJbHbiqPTxb5Xhng58PQO2oS6L+Cd37dQPzpZJ5pWSFTdalmNyEhITRs2JCqVavSqlUrBg0axNq1a6lfvz7gLGkxadIkNm3aRP/+/fHx8cHf358PPvgAgN69e9OqVSuKFCnCrFmzvFa3yUzpPioqykZHR6fre2yLi+fFr1czZ8N+qhXLxxu3VvP6lhkiIhnVsh2H6TMxmqMnknincw1aVyvidknpIiXF8tI3q5j013buql+C/7WrkmVC5uWsXbuWSpUquV1GpnOx82aMibHWRl3seHVBXqBESC4m3FuHYV1rsufoSdqPmMf/vlnNsZOJbpcmIuKqL2Ji6Tx6Af6+Pnz5YIMsG74AfHwMr7avSu8mpflkwTae/mIFySmZp8FCMj51QV6EMYZ2NYrStEIYg39ez4QFW/lx1W5ebleFVlULqylaRLKVpOQU3vhhHePmb6FBmRCGd6tFcK4At8tKd8YYnm1VkVwBfrz32wZOJCbzXudI17dTyq7i4uK48cYb/3X/zJkzCQkJcaGia6MAdgl5A/15pX1VOtYK57kvV/Lgp0u4vkIYr7SvSkRwkNvliYiku0Pxp+n32RLmb4rj3oYleb51Jfx8s08AMcbw6E3lCArw5fUf1nLydDIj7qxFoL+v26VlOyEhIWfX+soKss//omsQGZGfb/o15MW2lVm05SA3v/cHI2dvIjE5xe3SRETSzfo9x2g/Yj6Ltxxi0O3VebldlWwVvs51f5PSvNahKr+v30fPCYuJP5XkdknpKjOND88IruZ8Zc//SVfBz9eHno1K8duTTWlWviBv/7SeNkPnsnjrQbdLExFJcz+t2s2tI+dzMjGZz/vUo1NUhNslua57vRK806kGCzbHcde4RRw5kTXHBgcGBhIXF6cQlkrWWuLi4ggMDLyi52kW5FWauXYvL329mp2HT9AlKoIBrSpSIBuMiRCRrC0lxfL+zI0MnbmRyIj8jO5Rm0J5r+wHS1b348rdPPL5UioUzsMn912X5cbDJSYmEhsby8mTWhMztQIDAwkPD8ff3/+8+y81C1IB7BoknE5iyMyNjJ27hXw5/Xm+dSU61iqmQfoikikdP5XE41OW8euavdxeO5zXOlTVWKf/MGv9PvpOjKF4cBCf9rqOggqpchEKYOls3Z6jPPflSpZsP0y90sG81qEaZQvmdrssEZFU23ognvs/iebvA/G80KYS9zQoqV8mL2PB5jh6TlhMWJ4cfNrrOsILaHKWnE/rgKWzioXzMr1vA97sWI01u47Sasgc3v1lPScTk90uTUTksuZs2M8tw+ex//gpJt5Xl3sbllL4SoX6ZUKY1Os6DsWfpvOoBWw5EO92SZKJKIClER8fQ9e6xfn9qWa0rV6Uob9vosX7c5i7cb/bpYmIXJS1lg/n/M094xdRNH9Ovu3XiAZlQ90uK1OpVbwAn/Wux8mkFDqNWsD6Pd7fU1AyJwWwNBaaOwfvdYnk017X4WMMPT5axCOfLWXfMQ1mFJGM42RiMk9MXc7rP6ylRZXCfPFAA61veJWqFM3H1D718PWBLmMWsDL2iNslSSagMWDp6GRiMqP+2MzIWZvJ4e/D0y0r0q1ucW3wLSKu2n3kBL0/iWHlziM8eXN5+t1QVl2OaWB7XALdxv7FkYRExt9bh6iSwW6XJC7TGDCXBPr78thN5fnpscZUD8/Hi1+touMHf7J6l347EhF3RG89SLth89lyIJ4P74ri4RvLKXylkeIhQUztU5+wPDno8dEi5m084HZJkoEpgHlB6bDcTOp5He93iWTnoQTaDZvHq9+tyfIrKYtIxvLZou10/fAvcufwZcaDDbi5ciG3S8pyiubPyZQ+9SkREsR9Hy/mtzV73S5JMigFMC8xxtChZjFmPtGMO+oW56N5W7jp3T/4efUet0sTkSwuMTmFF79axbNfrqR+mVC+fqgR5QrlcbusLCssTw4+712PSkXy0HdSDN8u3+V2SZIBKYB5Wb4gf964tRpfPNCAfDn96TMxhl4Took9lOB2aSKSBR04foo7xy5k4l/b6NOkNOPvqUO+IP/LP1GuSf6gACb1uo5aJQrw6OdLmRq9w+2SJIPRIHwXJSanMH7+Ft77dSMAj99cjnsblsI/m252KyJpa9XOI/SZGMOB46d4+/bqtI8s5nZJ2c6J08n0nhjN3I0H+L9bqnB3g5JulyRepEH4GZS/rw+9m5Thtyeb0rBsKG/8sI52w+YRs+2Q26WJSCb3zfJd3D7qT1KsZXrfBgpfLskZ4MvYu6NoXrkQL3+zmpGzN7ldkmQQCmAZQLH8ORl7dxSje9TmyIlEbh/1J8/NWMmRhES3SxORTCY5xfLWT+t45LOlVCuWj2/6NaJaeD63y8rWcvj5MuLOWrSPLMrbP61n8M/ryUy9T5I+/NwuQP7RokphGpUN5b1fNzD+z638snoPL7SpTPvIopomLiKXdeREIo9+vpTZ6/fT7bri/K9dFQL89Ht2RuDv68O7nSPJ6e/L8FmbSDidzIttK+l7ezamAJbB5MrhxwttK3NrrWI8N2MVj01ZxvSYWF7tUJVSobncLk9EMqhN+47T+5Noth9M4LUOVeler4TbJckFfH0Mb3asRs4AX8bN30LC6SRev7WaFufOpjQIPwNLTrFMXrSdt39ax6mkFB5sVoYHmpUhh5+v26WJSAYyc+1eHvt8GQF+PnzQvTZ1S2kF9ozMWsu7v25g2O+baB9ZlMGdamjyVRZ1qUH4agHLwHx9DD3qlaBFlUK8+t1a3v9tI98s28VrHapqw1wRwVrLyNmbGfzLeqoUzcvoHlEUy5/T7bLkMowxPNm8AkEBfrz10zpOnE5mWLea+uU6m1HkzgQK5glkWNeafHJfXZKtpdvYhTw+ZRkHjp9yuzQRcUnC6ST6fbaUQT+vp131okzr00DhK5N5oFkZ/u+WKvyyZi+9JkRz4nSy2yWJF6kLMpM5mZjMiFmbGPXHZoIC/BjQqiJdoiLw0RgCkWxjx8EEek+MYd2eowxoWZHeTUprMHcmNjV6BwO+WEFUiWA+uieKPIFaKDeruFQXpAJYJrVp33Gen7GShVsOUrtEAV6/tSoVC+d1uywRSWcLNsfx4KcxJKVYhnWtSbMKBd0uSdLAt8t38fiUZVQpmpcJ99Ulf1CA2yVJGtBCrFlQ2YK5+bx3PQZ3qsGWA/G0GTqPN39YS8JpbfAtkhVZa5nw51a6f7SQ4FwBfP1QQ4WvLKRdjaKM6l6btXuOcceYv9h/TENMsjoFsEzMGMPttcOZ+URTbq8Vzug5f3Pzu3OYuXav26WJSBo6lZTMgC9W8vI3q7m+QhhfPdSQ0mG53S5L0thNlQsx7u46bItLoMvoBew+csLtkiQdKYBlAQVyBfDW7dWZ2qc+QQG+9JwQTZ+J0frPK5IF7Dt6kq5j/mJK9A4evqEsY3pojFBW1qhcKBN71mX/sVN0GrWAbXHxbpck6URjwLKY00kpjJ33N0NnbsTXGJ5oXoG765fAT2vMiGQ6y3Ycps/EaI6eSOKdzjVoXa2I2yWJl6yMPcJd4xYS4OfDp72uo2zBPG6XJFdBY8CykQA/Hx5sVpZfH29KnVLBvPrdGtqPmM+yHYfdLk1ErsAXMbF0Hr0Af18fvniggcJXNlMtPB+f965PioXOo/9i9a4jbpckaUwBLIuKCA5i/D11GHlnLQ4cP8WtI+fz0terOHpSG3ynp8TkFE2EkGuSlJzCK9+u4clpy6ldvADf9GtE5aKa4ZwdVSich6l96hPo50PXMX+xZPsht0uSNKQuyGzg2MlE3vllA58s2EpI7hy81LYybasX0bpBVykxOYXYQyfYGhfP1gOeP3EJbI2LJ/bQCXwM3FSpEJ2iwmlSLkzdv5IqsYcS+CJmJ9NidhB76AT3NCjJ820qaYsaIfZQAt3HLmTfsVN8dHcd6pcJcbskSSWtAyYArIg9zPMzVrFy5xGalA/j1fZVKBGiDb4v5sKQtS0ugS0H4tkWF8+OQydITvnn/03uHH6UDA2iREguSoXkIuF0Ml8v20lc/GkK5slBx1rhdIoKp4xmrckFTiYm8/PqPUyLjmX+5gMANCwTSo/6JWhRpbDL1UlGsu/oSbp/tJBtcQmM6l6b6ytqCZLMQAFMzkpOsUxcsJXBv2wgMTmFh28oS+8mZQjwy36/ZScmp7Dz0Am2xMWzzdOKldqQVSIkiFKhuSgRkovQ3AH/ak1MTE7h93X7mBYdy6z1+0hOsUSVKECnqHDaVC9K7hzahjW7stayIvYIU6N38M3yXRw7mUR4gZx0qh3BbbWLEV4gyO0SJYM6GH+au8YtZP2eYwy9oyatNC4ww1MAk3/Zc+Qkr363hu9X7qZswdy81qEq9UpnvWbtJE9L1rkh60yrVuyhEySdE7JyBfhSMjSX8yckiJIhuS4ZslJr37GTzFiyk6nRO9i8P56c/r60rlaEzlHh1C0VrK7gbOLA8VN8tdS5DjbsPU6gvw+tqhahU1Q49UqFaDsxSZWjJxO5d/xilm4/xOBONehYK9ztkuQSFMDkP81at48Xv15F7KET3F47nOdaVyI4V+baAiPpwjFZVxiynK+vLWSlhrWWpTsOMy16B98u383xU0mUDAni9trh3FY7nCL5tJFyVpOYnMLs9fuZFr2D39ftIynFEhmRn85REbStUYS8Ws9LrkLC6STu/ySa+ZvieK1DVbrXK+F2SfIfFMDkkk6cTmbY7xsZM+dvcgf68VyrSnSKCs9QLTNJySnsPHyCLRcMet8Wl8COgwkXD1khuSgZ6t2QlVonTifz46rdTIuOZcHfcfgYaFQujM5R4dxcuRA5/HzdLlGuwca9x5gWE8uXS3Zy4PgpQnPnoGOtYnSqHU65QlrPSa7dycRkHvp0CTPX7eP51pW4v0lpt0uSi1AAk1TZsPcYz89YyeKth6hbMpjXb63q1R8W54asM4PeLxWySni6CM+OzQp1xmaF5c6RIUJWam2PS2B6zA6mx8Sy68hJ8gf5075GUTpFRVC1WD63y5NUOnoyke+W72Zq9A6W7TiMn4/hhooF6RQVQbMKYZrNKGkuMTmFx6Ys4/sVu3nspnI8emO5TPW9LztQAJNUS0mxTI+J5Y0f13L8ZBK9m5Tm4RvKkTMgbVpkLhaytsU5LVoXhqygAN9zxmEFUTI084as1EhOsfy5+QDTomP5afUeTielUKlIXjpHhdMhshgFMlnXcHaQkmL5a0sc06Jj+XHVbk4mplC+UG46R0XQoWYxQnPncLtEyeKSUyzPfLGC6TGx9G5SmmdbVcxy3xszMwUwuWJxx0/xxg/r+GJJLBHBOXmlfVWur5C6ac9nQtbWuAS2Hoi/4pB1puswK4as1DqSkMg3y3cyLSaWFbFHCPD14abKTmtKk3Jh+GrAtqvOrNk1fckOdhw8QZ5AP26pUZTOURFUD8+Xba9bcUdKiuX/vl3NhAXb6F6vOK/cUlWTOjIIBTC5ags2x/HCVyvZvD+eNtWK8FK7yhTKG0hScgq7Dp9ky9mB7/+sl7XjUAKJyf8OWReOx8ruISu11u4+yrToWL5atpOD8acpnDfQGU8UFUGpUK3j5i0XrtllLTQsG0LnqAhaVClMoL/G7Yl7rLW89dN6Rv2xmY61ivH2bdW1CHQGoAAm1+RUUjJj/vibYbM2EeDrQ8E8OS4aspxxWJ6QFfLPTMOwPApZaeF0Ugq/r9t7dm2xFAt1ShagU1QEbaoVIZfWFktzZ9bsmhazg6+X/bNm1+21w7mtVjgRwVqzSzIOay0jZm1i8C8bqBGej8iI/GdnfZcKyUV4gZwKZV6mACZpYuuBeN77zVnA9cyCpApZ7th79CRfLnG2rfl7fzxBAb60qVaEznUiiCpRQP8W1+jMml3TomNZv/cYOfx8aFW1MJ2jIqhXWmt2ScY2eeF2Ji/axtYDCRw/9c/etH4+hvACOc/2QpQ6Z0meYvkVztKDAphIFmWtZcn2Q0xdHMt3K3YRfzqZUqG5zrbQFM4X6HaJmUaSZ82uqRes2dUpKpx2NYpqzS7JdKy1xMWfPjsW1xkq8s86ifGnk88e6+9riCgQ9K8lfEqF5qJo/pwad3qVFMBEsoGE00n8sHIP06J3sHDLQXwMNCkfRueoCG6sVFBri/2HTfuOMS06li/OrtkV4OzfqTW7JAuz1rL/+CknkB2Id3YLiYtni+f2icQLwllw0D+9HqH/bMmmcHZpCmAi2czWA/FMj4nliyWx7PasLdYhshidosKpUlRrix07mch3K5w1u5Zud9bsur5iQTprzS4RrLXsO3bq7Az2M6Fsa5zz52RiytljA/x8KB58prUs6Jz1GXNRJG9gtu+uVwATyaaSUyzzNh1gWvQOflm9l9PJKVQpmpfOURG0jyxK/qDss7bYmTW7pkfH8oNnza5yBf9Zsyssj9bsErmclJR/wtmZrswz6zpujYvnVNI/4SyHnw8lQs4JZZ6uzVKhuSiUJ3uEMwUwEeFwwmm+XraLaTE7WLXzKAG+PtxcpRCdoyJoVDY0y3Yj/GvNrhx+tIt01uyqoTW7RNJMSoplz9GT53RpJpzdPm7bwQROnxPOAv19KBHsGWt2tkvTCWqF8madSV0KYCJyntW7jjAtOpavl+3kUEIiRfIFclutcG6vHU7JLLC22H+t2dWptrNmV1rt7CAiqZOSYtl15MR5oWxrnNN6tuPgCU4n/xPOcvr7UiIkyLNA9z/LG5UKzZXpZtwrgInIRZ1KSmbm2n1Mi97BHxv2k2KhbqlgOkdF0LpaYYICMs/aYhdbs6tY/px0itKaXSIZWXKKZdfhE2e7NM/uohIXz46Dl1lzMoMv7H3NAcwY0xIYAvgCY621Ay94vAAwDigDnATus9auMsYEAnOAHIAfMN1a+/IFz30KGASEWWsPXKoOBTCR9LPnyEm+WBLL9JhYthyIJ1eAL22rF6VTVDi1M/DaYnHHTzFDa3aJZElndl05MwHgn9azf29tlzuH3zlb2gWdt9ZZSK4AV76HXVMAM8b4AhuAm4FYYDHQ1Vq75pxjBgHHrbX/Z4ypCIyw1t5onE+by1p73BjjD8wDHrXW/uV5XgQwFqgI1FYAE3GftZbobYeYFr2D71bsJuF0MqVDc9EpKoKOtYpRKK/7a4udWbNrWswOZq511uyqEZGfzlHhtK1elHw5tWaXSFZ3Zt/hc0PZmVa0HYdOkHxOOMuTw48S53RllgzJRf0yIRTNnzNda7zWAFYf+J+1toXn9rMA1to3zznme+BNa+08z+3NQANr7d5zjgnCCWAPWGsXeu6bDrwKfA1EKYCJZCzxp5L4fuVupkfHsmirs7ZYswoF6VQ7nBsrFSLAz7vLNZxZs+vLpTvZf8xZs+vWms6+mOW1ZpeIeCQmpxB76MR5exVv8XRtxh5KIMXCyDtr0bpakXSt41IBLDUDPIoBO865HQtcd8Exy4GOwDxjTF2gBBAO7PW0oMUAZXFaxs6Er1uAndba5Rm1a0Mku8uVw4/OURF0jopgy4F4psfsYHpMLL+v20dwroCza4tVKpI33Wo4s2bXtOgdLNl+GF8fw/UVCtI5KpzrKxbUml0i8i/+vj6UCnVauy50OimF2EMJhLq89ExqAtjF0tGFzWYDgSHGmGXASmApkARgrU0GIo0x+YEZxpiqwN/A80Dzy765Mb2B3gDFixdPRbkikh5Kheaif4uKPHFzBeZs3M/06Fgm/rWVcfO3UK1YPjpFhdO+RjHyBV17919KimXhloNMi95x3ppdz7WuSIeaxSiYx/1uUBHJnAL8fCgdltvtMtKmC/KC4w2wBahurT16wWMvA/HAz8BMIMHzUDiwC6hrrd3zX7WoC1IkYzkYf5qvl+1kanQsa3cfJcDPhxZVCtOpdjgNr2JtsZ2HT/BFTCzTYs5fs6tT7XAiI/Jn2IkAIiIXc61jwPxwBuHfCOzEGYTfzVq7+pxj8gMJ1trTxpj7gcbW2ruMMWFAorX2sDEmJ/AL8Ja19rsL3mMrGgMmkqmt2nmE6TGxzFi6kyMnEimaL5Dba4dze+0Iiof89xIQZ9bsmh4Ty7xNzppdDcqE0DlKa3aJSOaWFstQtAbex1mGYpy19nVjTF8Aa+0oTyvZJ0AysAboaa09ZIypDkzwPM8HmGqtfeUir78VBTCRLOFkYjK/rd3LtOhY5mzcj7VQr3QwnWpH0Mqztpi1lpU7jzA1egffLNvFUc+aXU5g05pdIpI1aCFWEXHF7iMn+HLJTqZG72BbXAK5c/hxc+VCrN19lHV7nDW7WnrW7KqvNbtEJItRABMRV1lrWbz1EFOjd/Djyt2ULZSHTrXDaVdDa3aJSNalACYiIiLiZZcKYFpAR0RERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvCxVAcwY09IYs94Ys8kYM+AijxcwxswwxqwwxiwyxlT13B/oub3cGLPaGPN/5zxnkDFmnec5M4wx+dPsU4mIiIhkYJcNYMYYX2AE0AqoDHQ1xlS+4LDngGXW2urAXcAQz/2ngBustTWASKClMaae57Ffgaqe52wAnr3GzyIiIiKSKaSmBawusMla+7e19jTwOdD+gmMqAzMBrLXrgJLGmELWcdxzjL/nj/Uc94u1Nsnz2F9A+LV9FBEREZHMITUBrBiw45zbsZ77zrUc6AhgjKkLlMATqIwxvsaYZcA+4Fdr7cKLvMd9wI9XVLmIiIhIJpWaAGYucp+94PZAoIAnaD0MLAWSAKy1ydbaSJxAVvfM+LCzL27M855jP73omxvT2xgTbYyJ3r9/fyrKFREREcnY/FJxTCwQcc7tcGDXuQdYa48C9wIYYwywxfPn3GMOG2NmAy2BVZ5j7wbaAjdaay8MdWeeNwYYAxAVFXXRY0REREQyk9S0gC0GyhljShljAoA7gG/OPcAYk9/zGEAvYI619qgxJuzM7EZjTE7gJmCd53ZL4BngFmttQpp8GhEREZFM4LItYNbaJGNMP+BnwBcYZ61dbYzp63l8FFAJ+MQYkwysAXp6nl4EmOCZSekDTLXWfud5bDiQA/jVaTTjL2tt37T7aCIiIiIZk/mPnr8MKSoqykZHR7tdhoiIiMhlGWNirLVRF3tMK+GLiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIhI9pJ0yu0KFMBEREQkGzmwEYbWgo2/uVqGApiIiIhkD0d3wcRbIfkUBJdytRQ/V99dRERExBsSDsLEjnDiMNzzHYSUcbUctYCJiPdYC5t+g8M73K5ERLKT0wnw2R1wcDN0nQxFI92uSC1gIuIlyUnwY3+IHgc+flC9CzR8DMLKu12ZiGRlyYkw7R7YsQg6T4BSTdyuCFAAExFvOHkUpt/rtH7VexBsCsRMgGWTofIt0OiJDPEbqYhkMdbCN4/Axp+h7XtQub3bFZ2lACYi6etILHzaGfavg3ZDoPY9zv2Nn4KFH8CiD2HN11D2Jue+EvVdLVdEspBfX4Llk6HZcxB1n9vVnEdjwEQk/exaCh/eCEd2QPfp/4QvgNxhcONL8Pgq5+9dy2B8SxjXypkebq1bVYtIVjB/KPw5FOrcD02fdruaf1EAE5H0se57GN8afP3hvp+hzA0XPy4wHzR+Eh5bCS3fgsPb4NPbYHQTWP0VpCR7tWwRyQKWfQa/vghVboVWb4Exblf0LwpgIpK2rIUFI+HzOyGsIvSaCYUqX/55AUFQry88sgxuGQ6n42Ha3TDiOlj6qTOQVkTkcjb8DF8/BKWbwa2jwcfX7YouSgFMRNJOchL88BT8/CxUbAP3fA95Cl3Za/gFQK0e0G8x3D4e/ALh6wdhaE1YOAYST6RP7SKS+W1fCFPvhiLVocsk8MvhdkX/SQFMRNLGqWPOOjuLx0KDh6HzRKdV62r5+ELVjtB3LnSbBnmLOstYvF8N5r3nzKwUETlj7xqY3Mn5XnHndMiRx+2KLsnYTDTQNSoqykZHR7tdhohc6MhOmNwZ9q2FNoPTZ7aRtbDtT5j7DmyeCTnywXW94boHIFdI2r+fiGQeh7fDR82d7xM9f4ECJdyuCABjTIy1Nupij2kZChG5NruWweQuzpitO6c6y0mkB2OgZEPnz84lMO9dmDMIFoxwZlfW7wf5iqXPe4tIxhV/wNnfMTEB7v0xw4Svy1EXpIhcvfU/wvhWzsr2PX9Ov/B1oWK1nPEdDy50FlZcOBqG1IBvHoa4zd6pQUTcd+o4fNrJWW+w6xQoVMXtilJNAUxErs5fo+CzrhBWAe6f6c43voIV4dZR8MhSqH03LJ8Cw6Ngek/Ys8r79YiI9ySdhindYfdy6PRxplvEWQFMRK5MchL80B9+euacmY6F3a2pQAlo846zllj9frDhJxjVECbfATsWu1ubiKS9lBT4qi/8PQtuGQoVWrld0RVTABOR1Dt1DD7vBovGOEGn8ycQkMvtqv6RpxA0f9UJYs2egx1/wUc3wcdtYfMsra4vkhVYCz8NgFVfwE3/BzW7u13RVVEAE5HUObLT2SZo029Oa1OL1zPsAocEBUOzZ+CxVdD8dTiwESZ2gA9vgLXfOb89i0jmNHcwLBrt/BLY8FG3q7lqCmAicnm7l8PYG+HQVug2Fer0crui1MmRGxr0g8dWQNv34cRBmHInfFDfGS+WnOR2hSJyJaLHw++vQfU74OZXM+QWQ6mlACYil7b+J6fly/jAfT9BOS/NdExLfjkg6l7oFwMdPwQMzOgNw2pB9DhIPOl2hSJyOWu+ge+fgHLNof1w8MncESZzVy8i6WvhaPi8K4SWdfZ0LFzV7Yquja8fVO8MD/wJd0yGXKHw3ePOEhZ/DnOmtItIxrNlLnzRC4rVdmY8+vq7XdE1UwATkX9LSYYfn4Efn4byrZzFDfMWcbuqtOPj48zg7DUT7voawsrDLy/A+1Vh9kBIOOh2hSJyxu4VzuSf4FLOEIiMNPHnGiiAicj5Th13vtktHAX1HoIuE7PMN7x/MQZKN4O7v4Wev0Hx+jD7TWe/yV9egGN73K5QJHs7+DdMug1y5IXuXzoTbLKIVAUwY0xLY8x6Y8wmY8yAizxewBgzwxizwhizyBhT1XN/oOf2cmPMamPM/53znGBjzK/GmI2evwuk3ccSkatydJezsv3GX6D1YGj5Rsad6ZjWIupA18+c7skKrZwtjt6vDt894Uw+EBHvOrbX2WIoJQl6zMhyW41dNoAZY3yBEUAroDLQ1RhT+YLDngOWWWurA3cBQzz3nwJusNbWACKBlsaYep7HBgAzrbXlgJme2yLilj0r4cMbnd84u06Buve7XZE7ClWB28ZCv2iocQcs+QSG1oIv+8C+dW5XJ5I9nDwCn94Gx/fBndOcYQJZTGpawOoCm6y1f1trTwOfA+0vOKYyTojCWrsOKGmMKWQdZ0a1+nv+nFkJsT0wwfP1BKDDVX8KEbk2G36BcS2dr+/7Cco3d7eejCCkjLPC9qPL4bq+sPYbGHkdfH4n7IxxuzqRrCvxpPP/bN9aZwhEeJTbFaWL1ASwYsCOc27Heu4713KgI4Axpi5QAgj33PY1xiwD9gG/WmsXep5TyFq7G8Dzd8Gr/AziTcf2QuIJt6uQtLToQ/isCwSXdvZ0LFzN7YoylnzFnK7Yx1ZBk6dh61xnQddPOjgzs7S6vkjaSUmGL3s5/886jIKymXDZm1RKTQC72CpnF37HGQgU8ASth4GlQBKAtTbZWhuJE8jqnhkfllrGmN7GmGhjTPT+/fuv5KmSlnbGOL+RvFMehkTC4rHORqiSeaUkw0/Pwg9PQbkWnpmORd2uKuPKFQI3PO8EsZv+B3tXwYS2MK4FbPhZQUzkWlnrrPO19ltoORCqd3K7onSVmgAWC0Scczsc2HXuAdbao9baez1B6y4gDNhywTGHgdmAp5+DvcaYIgCev/dd7M2ttWOstVHW2qiwsLBUlCtpxlrYMgc+ae/8xr91rrPtQ4GS8P2TMKKOs5p4SrLblcqVOnUcpnSHv0bCdQ/AHZ86q8bL5QXmhUaPO/tNth7sTFyY3BlGNXL2ptP/B5GrM+t1iPkYGj8J9R5wu5p0l5oAthgoZ4wpZYwJAO4Avjn3AGNMfs9jAL2AOdbao8aYMGNMfs8xOYGbgDOjWL8B7vZ8fTfw9TV9Ekk71jqrn3/UHCa0g71r4OZX4PHVzt/3/QTdpkGOPM5q4qMawbrv1QKQWRzdDR+3hg0/QatB0Gpg9pnpmJb8czoTFR5ZCh0+gKRTMP0+GB7lDNxXC7FI6i0cDXMGQa274IYX3a7GK4xNxQ9NY0xr4H3AFxhnrX3dGNMXwFo7yhhTH/gESAbWAD2ttYeMMdVxBtj74oS9qdbaVzyvGQJMBYoD24FO1tpLrn4YFRVlo6Ojr+qDSiqkJMPqGTDvPad7JV9xaPQoRHYH/8CLHJ8Ca75yfmuJ2+SsUHzjS866SpIx7VnltNacOAydxkP5Fm5XlHWkJMO672DOYNizAvIWgwYPOz9Qsuo6aiJpYeV0Z5X7im2g0wRnx4oswhgTY6296CyCVAWwjEIBLJ0knYLln8P8950lCEIrQOMnoOptqdvuITkJlk+G2W/B0Vgo1dQJYll05kqmtfFXmHaP03LZbSoUqe52RVmTtbBpJsx9B7b/CUEhTndKnfshZ363qxPJWDb/Dp92hoi60P0Lp2U5C1EAk4s7HQ8xE5w98I7tgiKRTt97xbZXt8lp4kmIGe+0ACQcgApt4IYXoNCFy8aJ1y0eCz/0d9a46jZVg+29ZdufMPdd2PSrs5J3nV5Q70HIrfGsIuyMgY/bOVsM3fN9lvwFRQFMznfisLP0wF8j4cRBKNHIafEqc4OzNcu1OnUM/hoFfw51vq7eGZoNcJY5EO9KSYZfX4IFw52ZjreP02B7N+xe7gSxNV+DXw6odbfTPZk/4vLPFcmKDmx0xhnnyAM9f4E8hd2uKF0ogInj+D4ndC0aC6ePOT+QGz8Bxetd/rlXI+Gg0625cAykJDpjYZo8nbU2dc7ITsfDF/fD+u+hbh9o+aYG27vtwEaY9z6s+Ny5Xf0OaPQYhJZzsyoR7zq6ywlfSSfhvp+dRY+zKAWw7O7wdqebccknznivKrc60+i9NQbo6G6YO9iZXuzjB3V7O++fhTZVzXCO7YHJXZzB4C3ehHp93a5IznV4h+f/5ATn/2Tl9s4vQ0VquF2ZSPpKOAjjW8ORWLjnOyga6XZF6UoBLLvav8FpgVoxBTDOvnYNH4PQsu7Uc3ALzB7o1JMjj9MFU+8B52tJO3tXO4NaTxxyuhwrtLz8c8QdF7ZKl70ZmjyVfq3SIm46nQATO8CupXDndCjd1O2K0p0CWHazaxnMexfWfAN+gVDbM94kX7jblTn2rYXfX3Om7AeFOAP/o3pefKkLuTIbf/PMdMwN3aaoRSWzOHEYFn8If30ACXFOEOv8CQQEuV2ZSNpITnR2U9n4C3T6GKp0cLsir1AAyy62/elMfd/0mzPjqu79zirnGXXGVWwM/P4K/D3bWTOp6TMQeWeWWgPGqxZ/5Mx0LFjZCV/5LtyyVTK80/HOBJnf/gflmjs7FKRmKRiRjMxa+OpBZ7miNu9CnZ5uV+Q1CmBZ2dk1hwbD9gUQFAr1H3Smuwfmc7u61Pn7D5j5CuyMhuAycP1zUKXj1S2FkR2lpMCvL3pmOjb3zHRUt26mtvgjZ0+8Gl2h/Uj9X5DM7ZcXnVnxzZ6DZs+4XY1XXSqAqakhs0pJdjYsnfvOP6tut3obavbIfN0WpZtCqd9g/Y/w+6vwRU9nptiNLzqBIi2WxsiqTifAl/c73bl17nc2sFULYuZXpyfEH4DZb0CuUGj+mtsViVyd+UOd8FXnfmj6tNvVZCj6Tp3ZJCfCiqnOdkFxG50Wo1uGQ/Uu4Bdw+ednVMZAxdbO1jirvnS2N5rcGSKuc1bVL9nI7QoznmN74bMuzpi/lgPhur4Kq1lJ06chfr8zWzJXGDR81O2KRK7Mss+c1vkqt0Krt/T96QIKYJlF4glYMtH5TeLIDihUDW4f70xfz0prO/n4QvVOzgDNpRPhj7fh4zbOIrE3vgRFa7pdYcawd40TUBPi4I7JTniVrMUY54dWwgFnMd1cYRDZze2qRFJnw8/w9UPO1nS3js5aP6fSiAJYRnfyqLONzF8jnd+GI65zBjGWuzlr/zbh6w9R9zljYBZ96MzqHNMMKt3ibG8UVsHtCt2zaaYz09E/CO79QaE0K/PxdX54JRyEr/tBzmAtKyIZ3/aFMPVuKFzNmUjil8PtijIkDcLPqOLjYOEHziryp45AmRud5RpKNMjaweu/nDwKC0Y4A80TE5wVxJsNgAIl3K7Mu6LHw/dPQsFKnpmOGWRpEUlfp47Bx21h/zq462utEyYZ1941ML6lMyGs5y/OGMZsTLMgM5MjO52QEfOxEzQqtYNGT0CxWm5XljHEH3DGvy36EGwKRN0LjZ+CPIXcrix9paTAby87XdBlb4ZO4zXTMbuJP+Bs35JwAO79SZvcS8ZzeLtzjVrrhK/s9gvyRSiAZQZxm51V65d95gSL6p2dVesLVnS7sozpyE6Y87YzLs4vhzMAveEjkLOA25WlvdMJMKO3M+s1qqcz21UzHbOnQ9ucH3DGOD/g8hd3uyIRR3wcjGsB8fvg3h+hUBW3K8oQFMAysj2rnPFNq2eAjz/U6gENHtFvDqkVtxlmvwkrpzuLzzZ8xAljOXK7XVnaOLYXPrvD2bqjxetQ78Hs2QUt/9i7Gsa3cgbl3/dztu/ikQzg1HGY0A72rYEeX0GJ+m5XlGEogGVEOxY7a3ht+BECcjvr/tR7KOt3paWXPauc7Y02/Oj8YGrSH2rfk7kHf+5b6+zpmHAAbhsLFdu4XZFkFNsWOHvqFawEd3+r7mhxT9JpZ0b2ljnOgPsKrdyuKENRAMsorHW23Zn7Dmyd63SX1XvQ2TIoK3aduWHHImdV/a1zIV+EM1C/+h2Zr8tu8+/OLCL/nND1c40BlH9b/6Ozt16pxtBtWuZeB1Ayp5QU+LIXrPoC2o+Amt3drijDuVQA0/4W3pCSAmu/gw9vcH5rPbARmr8Oj61yFltU+Eo7EXWdFoEeM5yuma8fgg/qw+qvnH+HzCDmY5h0uxMge81U+JKLq9AKbhnq/FI3o0/mub4la7AWfhrghK+b/qfwdRUyWbNAJpOc5Fyc8951po8XKAlt33cWU8zMXWMZnTHOwq2lr3e26Pn9NZh2NxSp4SzmWubGjDmOKiUFZv6fMxmjzI3Q6WMIzOt2VZKR1ezuzI787WXnF45Wb2fMa1uynrmDYdFoqN/PmTAmV0wBLD0knnR2fZ/3PhzeBmGVoONYZzuGzNYVlpkZ4yzjUaG1s33T7Ddg0m1QoqETxDLSWkqJJ5xWjDVfQ+17ofVgXSuSOg0fdRZpXjAcchWEpv3drkiyupiPnV9sq3eBm19V6L9K+g6flk4dh5jx8OdwOL4HitV29ugr3xJ81NvrGh9fiOwKVW+DJROc7Y3GtXA2+r7hRShS3d36ju+Dz7rCzhhn0+X6/fQNTVLPGOeHYPwBmPUa5ApxdpEQSQ9rvoHvHnfWI2w/Qj/broECWFpIOAiLxsBfH8DJw1CqCXQc7eyBpR+kGYdfgDPhIbKb8+81730Y3RiqdITrn4fQst6vad86mNwJju+HLhOdFjuRK+XjA+2Hw4mDzk4JQaFQ+Ra3q5KsZstc+KKX07jQeYKzZZxcNc2CvBbH9jjN/ovHQWK809XV6AmIqON2ZZIaJw7Dn8Oc4Jx0EmreCU2f8d72Pn/Phil3OeMBu33ufFMTuRan4+GT9rB7OXT/wvllUCQt7F4BH7eBPEXgvp8gKNjtijIFLUOR1g5thflDYOmnkJLodG01elwr/2ZWx/fB3Hch+iPndp1eTpDOHZZ+77nkE6cZP7S8s6ejVjSXtJJw0Fmo9chOuPd7Z/KJyLU4+Dd81AJ8A6Dnz9qD9googKWVfWudfQhXTveMK+rmDIANLu1eTZJ2Dm+HP96CZZPBLyfUfxAaPAyB+dLuPVJS4PdXnOuozA2emY5p+Poi4ISvj5pD8ilntfyQMm5XJJnVsb3OmNmTh51rKayC2xVlKgpg12pnjNNCsu478A9yZqk16Ad5i3q/Fkl/+zfArNdhzVcQmN9p3azbGwKCru11E0/AjL7O69a+xzPTUWMoJJ3s3+D84AzMC/f9ol025MqdPOJ0O8ZtdtZXDL9ojpBLUAC7GtbC1nnOqvV/z3JaKer2cfYZzBXinRrEXbuWOVOtN/0KuQs52xvVuvvqVhw/vh8+7wqx0XDzK07LmiZoSHqLjXb26Asu43RHqrVVUivxJHx6O2xf4AyTKHuT2xVlSgpgV8Ja2PCzE7xiFznr6tR/yJnWrUUxs6dtfzrbG21fAPlLwPXPQbVOTjd0auxfD592csaadRyj2WniXZt+g8ldIKKeMzDfP9DtiiSjS0l2Fq9e+y10/BCqd3a7okxLWxGl1qaZMKoRfNbFmeHYejA8tgIaPabwlZ2VaAD3/gh3Tneugxl94IOGzvZSl/sF5u8/YOzNkJgA93yv8CXeV/Ym6DAKts2DL3o6P1xF/ou18P0TTvhqOVDhKx0pgJ3r1FFIPu18s3pkibNmlH9Ot6uSjMAYKHcz9J4Dt493Zr9OudPZ33PzrIs/Z+kkmNQR8hZx9nQM1zIT4pLqnZwfpuu+c2bfZqKeD/GyWa87K903egLqPeB2NVmauiDPdWYzW63sK5eTnATLP4PZA+ForLPe0g0vOWvApaQ4K5LPfcfZj7LzBI29kYxh5ivOddn4KbjxRberkYxm4Wj48Wmo2QNuGaZxqmngUl2QWgn/XApeklq+flCrh9M8Hz0e5gyCj25yFuP19Xf2dKx1F7R5VzMdJeO44UVn38i5gyF3Qbiuj9sVSUaxcjr8+AxUbAtt31f48gIFMJFr4ZcD6vWFmt1h4QcwfxicOgI3/Z+zRpy+iUlGYgy0ec9ZrPXHpyEoBKrd7nZV4rbNvztL5BSvD7eNdX7BlHSnLkiRtJRwEI7ugsJV3a5E5L8lnnTGJ+5Y5Fli4Ea3KxK37IyBj9tBcClnolDO/G5XlKVoFqSItwQFK3xJxucfCHdMdlY1n9IDYmPcrkjccGAjTLodcoU6S5QofHmVApiISHaUM7/zQzdXqLPg5v4Nblck3nR0F0y81VnPsMcMyFPY7YqyHQUwEZHsKk9h54evj6/TJXlkp9sViTckHISJHeHEYWd9Q+0V6goFMBGR7CykjPND+MRhmHSb88NZsq7TCfDZHXBwM9zxKRSNdLuibEsBTEQkuysaCV0nOz+UP7vD+SEtWU9yIky7x5l80fFDKN3U7YqyNQUwERFxFhO+bazzw3naPc4Pa8k6rIVvHoGNP0Obd6BKB7cryvYUwERExFG5PbR91/kh/c3D/+wOIpnfry/B8snQ7Dmo09PtagQtxCoiIueKug+O74fZbzgzJJu/5nZFcq3mD4U/h0Kd+6Hp025XIx4KYCIicr6mTztbFv05DHKFObs6SOa07DP49UWo3AFavaXdOTIQBTARETmfMc4P64QDTtdVrjCI7OZ2VXKlNvwMXz8EpZpCxzHOciOSYSiAiYjIv/n4wq2j4cQh+Lof5AyGCi3drkpSa/tCmHo3FK7mLDfhl8PtiuQCGoQvIiIX55cDukyCItVh2t2w/S+3K5LU2L0cJneGvEWdNd5y5HG7IrkIBTAREflvOfI4P8TzhTs/1Peucbsi+S+HtsFXD8KYZuAX6OxykDvM7arkPyiAiYjIpeUKhe5fgn+Qs2XR4e1uVyTnOrYXfngahtWGldPhugfggflQoITblcklaAyYiIhcXoESzubd41s5mzjf97MTzMQ9Jw45S0wsHAVJp6Bmd2cGa75wtyuTVFALmIiIpE6hKtB1ChyJhU9vh1PH3K4oezodD3PfgSE1YN67UKEVPLQIbhmq8JWJKICJiEjqlagPnT6G3StgSndIOu12RdlH0ilYOAaGRMLMVyCiHvSZC7ePg9CyblcnV0gBTERErkyFVnDLMPh7Nszooy2L0ltKMiz9FIZFwY/9IbSc0wV851RnhqpkShoDJiIiV67mnc5q+b+97IwFa/W2VllPa9bC2m/g99fhwHooEgnt3ocyN+hcZwEKYCIicnUaPuqEsAXDIVdBaNrf7YqyBmth8+9ON+PuZRBaHjp/ApVuUfDKQhTARETk6hgDN78K8Qdg1mtOS1jUvW5XlbltX+gEr23zIF9xaD8SqncBX/24zmr0LyoiIlfPxwfaD4cTB+H7JyAoBCrf4nZVmc+elfD7a7DhJ2fvzVZvQ+17tIVQFqYAJiIi18bX35kZ+UkH+KIn5PwCSjVxu6rMIW4zzHoDVk2HwHxw40twXV8IyOV2ZZLONAtSRESuXUAu6DYFgkvDZ92c/Qjlvx3ZCd88AsPrwPofoNET8OhyaPykwlc2oRYwERFJG0HBzpZFHzWHSbdDz5+dQCb/iD8A896DRR+CTYE6PaHxU5CnkNuViZepBUxERNJOvmLOJtApSc6WRcf2ul1RxnDyqNPVOKQG/DUSqt0OD8dA60EKX9mUApiIiKStsPJw5zQ4vg8m3QYnj7hdkXsSTzj7NQ6pAX+85azh9eBf0GGkNsvO5hTAREQk7YVHQZeJsH+tMyYs8aTbFXlXciJEj4OhNeHXF6FoTbh/lnNOwiq4XZ1kAApgIiKSPsreBB1GOWtafdHT2VInq0tJgRXTnMH13z0O+SLgnu+hx5dQrJbb1UkGokH4IiKSfqp3goQ4+OkZJ5C0G5I1V3O3Ftb/6KzltW81FKoKXadA+RZZ8/PKNVMAExGR9FWvL8Tvg7nvOIuM3vii2xWlrS1znNXrYxc7sz5v+wiqdHQWqRX5DwpgIiKS/m540dk3cu5gyF0QruvjdkXXbmcMzHwV/p4FeYo6rXuRdzoL04pchgKYiIikP2OgzXuQcBB+fNrZsqja7W5XdXX2rYPfX4V13zmfo8UbENUT/APdrkwyEQUwERHxDl8/p3tuUkeY0RdyFoCyN7pdVeod2gqzB8LyzyEgNzR7Duo9AIF53a5MMiEFMBER8R7/QOj6GYxvA1N6wN3fQnhtt6u6tGN7YM4giJkAPr7QoJ+zdVBQsNuVSSamACYiIt4VmA+6T3e2LPr0drjvZ2fx1owm4SDMHwILR0NKItTsAU2fhrxF3a5MsgBN0RAREe/LU9jZssjH1+mSPLLT7Yr+ceq40+I1JNIJYJXawUOLoN37Cl+SZhTARETEHSFloPsXcOKws2VRwkF360k6BX+NgqGRznpeJRtC33lw24dOrSJpSAFMRETcU6QGdJ0MBzfDZ3fA6QTv15CcBEsmwrDazoKxYRWh52/OWLXCVb1fj2QLCmAiIuKuUk3gtrGwYxFMu8fZR9EbUlJg9QwYWQ++6ecsEtvjK2diQEQd79Qg2ZYCmIiIuK9ye2j7Lmz8Gb552AlH6cVa2PgrjGnqBD4fX+jyKdz/O5S5XlsHiVdoFqSIiGQMUfdB/AGY9TrkCoXmr6X9e2xb4GwbtP1PyF8Cbh0N1To5IUzEixTAREQk42jSH47vgz+HOV2CDR9Nm9fdvdwZWL/xF8hdCFoPhlp3g19A2ry+yBVSABMRkYzDGGj1NiTEwa8vOSEsstvVv96BTTDrNWesV2B+uOl/ULcPBASlVcUiV0UBTEREMhYfH7h1FJw4CF/3g5zBUKHllb3G4R3wx1uwbDL4BTota/X7Qc786VKyyJVSABMRkYzHLwd0mQQT2sG0u+Gur6F4vcs/7/h+mPcuLB7r3K7bGxo/AbkLpm+9IldIAUxERDKmHHngzukwrgVM7gz3/gSFKl/82JNHnHFjC0ZC0gmn27LpAMgf4d2aRVIpVctQGGNaGmPWG2M2GWMGXOTxAsaYGcaYFcaYRcaYqp77I4wxs4wxa40xq40xj57znEhjzF/GmGXGmGhjTN20+1giIpIl5AqF7l+Cf5CzZdHh7ec/fjoB5r0P71d3tg8q39zZNqj9CIUvydAuG8CMMb7ACKAVUBnoaoy58FeQ54Bl1trqwF3AEM/9ScCT1tpKQD3goXOe+zbwf9baSOAlz20REZHzFSjhbFmUmAATb3WWqkg67XQzDq0Jv70M4XWgzxzo9DGElnO7YpHLSk0XZF1gk7X2bwBjzOdAe2DNOcdUBt4EsNauM8aUNMYUstbuBnZ77j9mjFkLFPM81wJ5Pc/PB+xKg88jIiJZUaEq0HUKTOwAE26BxHg4tBWK14dO46FEA7crFLkiqQlgxYAd59yOBa674JjlQEdgnqcrsQQQDuw9c4AxpiRQE1jouesx4GdjzGCcljj97xERkf9Wor7TwjWlOxSs5IwPK3uTVq6XTCk1AexiV7a94PZAYIgxZhmwEliK0/3ovIAxuYEvgMestUc9dz8APG6t/cIY0xn4CLjpX29uTG+gN0Dx4sVTUa6IiGRZFVrBUxudNb18tJueZF6puXpjgXNHMoZzQXehtfaotfZez3iuu4AwYAuAMcYfJ3x9aq398pyn3Q2cuT0Np6vzX6y1Y6y1UdbaqLCwsFSUKyIiWVpQsMKXZHqpuYIXA+WMMaWMMQHAHcA35x5gjMnveQygFzDHWnvUGGNwWrbWWmvfveB1dwFNPV/fAGy82g8hIiIikplctgvSWptkjOkH/Az4AuOstauNMX09j48CKgGfGGOScQbY9/Q8vSHQA1jp6Z4EeM5a+wNwP063pR9wEk83o4iIiEhWZ6y9cDhXxhUVFWWjo6PdLkNERETksowxMdbaqIs9pk50ERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRkUxj46GNvPznyyzZu8TtUkSuiZ/bBYiIiFzOoZOHGLFsBNM2TCPFpvDt5m95teGrtCndxu3SRK6KWsBERCTDSkxOZOKaibSZ0YbpG6bTuXxnvrv1O2qE1WDA3AF8sPwDrLVulylyxdQCJiIiGY61lrk75zJo8SC2Ht1Kg6IN6B/Vn7IFygIw5uYx/G/B/xi5bCQ7ju7gfw3+R4BvgMtVi6SeApiIiGQomw9v5u3Fb/Pnrj8pmbckI24cQeNijTHGnD3G39ef1xq+RkSeCEYsG8Hu+N28f/375MuRz8XKRVJPAUxERDKEwycPM3L5SKaun0qQXxD9o/rTtWJX/H39L3q8MYa+NfoSkSeCF+e/SPcfujPyxpFE5I3wcuUiV04BTEREXJWYksjU9VMZuWwkxxOP06l8Jx6KfIgCgQVS9fw2pdtQJFcRHp31KHf+cCdDbhhCzYI107lqkWujQfgiIuKaubFzue2b2xi4aCCVQyozvd10Xqj3QqrD1xm1CtViUutJ5M2Rl14/9+KHv39Ip4pF0oZawERExOv+Pvw3g6IHMW/nPIrnKc7Q64fSLKLZeeO8rlSJvCWY1GoSj856lGfmPkPs8Vjur3b/Nb2mSHpRABMREa85cuoIHyz/gM/XfU5Ov5w8FfUU3Sp2+89xXlcqf2B+Pmz+IS/9+RLDlg5j+9HtvFz/5TR7fZG0ogAmIiLpLiklyRnntXwkx04f47Zyt/FQ5EOE5AxJ8/cK8A3gzUZvUiJPCUYuH8nu+N282+xdzZCUDEUBTERE0tX8nfMZtHgQm49spm7hujxd52kqBFdI1/c0xvBA5AOE5wnnpT9fosePPRhx4wgi8miGpGQMGoQvIiLpYsuRLTw08yH6/taX0ymnef/69xnbfGy6h69ztSvTjg9v/pCDJw9y5/d3smzfMq+9t8ilKICJiEiaOnLqCG8teouOX3ckZm8MT9R+gq/af8WNxW90ZUB8VOEoJrWaRJ6APPT8uSc/bfnJ6zWIXEhdkCIikiaSUpKYvmE6I5aN4MipI3Qs15F+NfsRmjPU7dIoma8kk1o7MyT7z+lP7PFYelbtqRmS4hoFMBERuWYLdi3g7cVvs+nwJqIKRfFM3WeoGFzR7bLOUyCwAB82/5AX57/IkCVD2H50Oy/WfxF/H82QFO9TABMRkau27eg2Bi8ezOzY2RTLXYz3mr3nWldjauTwzcFbjd+ieJ7ijF4xml3Hd/Hu9e+SNyCv26VJNqMAJiIiV+zo6aOMXj6ayesmE+ATwGO1HqN75e7k8M3hdmmXZYyhX81+FM9bnJf/fJkePzgzJMPzhLtdmmQjCmAiIpJqySnJfLHxC4YvHc7hU4e5tdytPFzz4QwxzutK3VLmlvP2kBx6w1BqhNVwuyzJJjQLUkREUmXh7oV0+q4Tr/71KqXyleLztp/zfw3+L1OGrzPqFK7DpNaTCPILoufPPfll6y9ulyTZhAKYiIhc0vaj23n090fp9UsvEhITeKfpO3zc8mMqh1R2u7Q0UTpfaT5t8ykVgyvy5B9PMm7VOKy1bpclWZy6IEVE5KKOnT7Ghys+ZOLaifj7+PNorUfpUblHphjndaWCA4P5qMVHvDDvBd6LeY/tR7fzfL3nNUNS0o0CmIiInCc5JZkZm2YwbOkwDp08RPuy7Xmk5iOEBYW5XVq6yuGbg7eavEVEngg+XPkhO4/v5N1m75InII/bpUkWpAAmIiJnLd6zmLcWvcX6Q+upWbAmI28aSZWQKm6X5TU+xodHaj1CRJ4IXlnwijND8qYRFMtdzO3SJIvRGDAREWHHsR08Putx7vv5Po6ePsqgpoOY0HJCtgpf57q13K2MunkU+xL2cef3d7Jy/0q3S5IsRgFMRCQbO376OO/FvEf7r9ozf9d8+kX245sO39CyZMsMu5iqt1xX5DomtZ5EoF8g9/18H79t+83tkiQLUQATEcmGklOS+XLjl7Sd0ZZxq8bRqlQrvrv1O/rU6EOgX6Db5WUYpfOX5tPWn1I+uDxPzH6Cj1d9rBmSkiY0BkxEJJuJ3hPN24vfZu3BtUSGRTL8xuFUDa3qdlkZVkjOED5q/hHPz3ued2LeYduxbTx33XOaISnXRAFMRCSbiD0Wy7sx7/Lrtl8pnKswbzd5W12NqRToF8igpoMovrQ4Y1eOZdfxXQxuOlgzJOWqKYCJiGRx8YnxjF05lk9Wf4Kvjy8PRT7E3VXuJqdfTrdLy1R8jA+P1nqUiDwRvLrgVe768S5G3jiSIrmLuF2aZEIKYCIiWVSKTeGbzd8wZMkQDpw4QLvS7Xik1iMUzlXY7dIytY7lOlIkVxGemP0E3X7oxvAbhlMlNHvOFpWrp0H4IiJZ0JK9S+j6fVdenP8iRXMX5dPWn/JG4zcUvtJI/aL1mdhqIgE+Adzz0z3M3D7T7ZIkk1EAExHJQnYd38VTfzzF3T/dTdyJOAY2HsikVpOoHlbd7dKynLIFyvJpm08pV6Acj896nAmrJ2iGpKSauiBFRLKAhMQEPlr1ERNWT8BgeKDGA9xT5R6C/IPcLi1LC80ZykctnBmSg6MHs+PYDgbUHYCfj368yqXpChERycRSbArf/f0dQ2KGsO/EPlqXas3jtR9XV6MX5fTLyeCmg3l/yfuMXzWe2OOxDG4ymNwBud0uTTIwBTARkUxq2b5lvLXoLVbFraJaaDXeafYOkQUj3S4rW/IxPjxR+wki8kTw+l+vc/dPdzPixhEKwvKfNAZMRCST2X18N0/PeZoeP/ZgX8I+3mj0BpNaT1L4ygA6le/EyBtHsvP4Trp9343VcavdLkkyKAUwEZFMIiExgRHLRnDLV7fw+/bf6VO9D9/e+i3tyrTDx+jbeUbRoFgDJraaiJ+PH/f+dC+zts9yuyTJgPQ/VkQkg0uxKXy7+VvafdWOUctHcX3E9Xzb4Vv61eynQfYZVLkC5ZjcZjKl85Xm0VmPMmnNJM2QlPNoDJiISAa2fP9y3l70NisOrKBKSBUGNx1MzYI13S5LUiE0ZyjjW47n2bnP8tbit9h+bDtP13laMyQFUAATEcmQ9sTv4f0l7/P9398TljOM1xq+pq7GTCinX07eafoO78W8x4Q1E4g9FsugpoPI5Z/L7dLEZQpgIiIZyMGTB5mybgrjVo0jxaZwf7X76VWtl7oaMzFfH1+eqvMUxfMW542Fb3D3j3cz/MbhmiGZzSmAiYi4wFpL7PFY1h1c98+fuHXsO7EPgBYlW/BE7Scomruoy5VKWulcoTNFcxflqT+e4s7v72T4jcOpFFLJ7bLEJSYzDQqMioqy0dHRbpchInJFElMS+fvw32eD1tqDa1l/cD3HE48D4Gt8KZWvFBWDK1IxuCJRhaOoEqLNnbOq9QfX0+/3fhw5dYRBTQbRNKKp2yVJOjHGxFhroy76mAKYiEjaSUhMYP2h9ayNW3v2702HN5GYkgg4Y4LKFShHpeBKZwNX2fxlCfQLdLly8ab9Cfvp93s/1h1cx9N1nubOSne6XZKkg0sFMHVBiohcpQMnDrD+4HrWHlx7tnVr+9HtWJxfbAvkKEDF4Ip0r9TdCVshFSmRpwS+Pr4uVy5uCwsKY3yL8QyYO4CBiway49gO+kf117WRjSiAiYhcRopNYeexnecFrXUH17H/xP6zxxTLXYyKwRVpW7otlYIrUSG4AoWCCmGMcbFyyciC/IN4r9l7vBPzDhPXTCT2WCxvN3lbEy6yCQUwEZFzJCYnsvnI5vO6EDcc2nDeeK3S+UtTv2h9KhSoQKUQJ2zlDcjrcuWSGfn6+PJ0naeJyBPBwEUDueenexh2wzAK5SrkdmmSzjQGTESyreOnj7Ph0IazLVvrD65n4+GNJKUkAc54rQoFKlAhuIIzZivEGa+VwzeHy5VLVjQndg79/+hP7oDcjLxxJBWCK7hdklwjDcIXkWzvwIkDrI07vwtx+7HtZx8PDgw+Oyj+zJ/ieYprTI541fqD63lw5oMcP32cQU0H0SS8idslyTVQABORbCPFprDj2I7zlnxYF7eOuJNxZ48Jzx1+NmRVCnFmI4blDNN4LckQ9sbv5eHfH2b9ofU8W/dZ7qh4h9slyVXSLEgRyZISkxPZdHjTeWtrrT+0nvjEeAD8jB+l85emYbGGZwfGVwyuSJ6APC5XLvLfCuUqxMctP+bpOU/z+sLX2XZ0G09FPaXW2CxGAUxEMoXjp48747Q8A+PXHVzH5iObz47XCvILokJwBdqVbne2Vats/rIE+Aa4XLnIlQvyD2LI9UMYFD2ISWsnEXs8lrcav6UZklmIApiIZCjWWvaf2H/+Fj0H17Hj2I6zxwQHBlMpuBKNijWiYkhFKhaoSPG8xbVRtWQpvj6+DKg7gIg8Eby9+G3u+ekeht84nIJBBd0uTdKAxoCJiGtSbArbj24/rwtx7cG1HDx58OwxEXkizhsYXym4EmFBYS5WLeJ9f+z4g/5z+pMvRz6G3zBcMyQzCQ3CF5EMITElkV+3/srSfUvPdieeSDoBOOO1yuQvc97A+PIFymu8lojH2ri19JvZj/ikeAY3HUyjYo3cLkkuQwFMRFyVYlP4actPjFg2gu3HthPkF/SvJR/K5C+j8Voil7Enfg/9ZvZj0+FNPFv3WbpU7OJ2SXIJmgUpIq6w1vJH7B8MWzqMDYc2UL5AeYbdMIwm4U00XkvkKhTOVZgJrSbQ/4/+vLbwNbYf284TtZ/QDMlMSAHsHNZajp4+Sr4c+dwuRSTTW7xnMUOWDGH5/uUUz1Oct5u8TYuSLRS8RK5RLv9cDL1hKG8teotP1nxC7LFY3mz8pmZIZjL6TniO6Run0+HrDizes9jtUkQyrdUHVtP7l97c9/N97I7fzcv1X+arDl/RqlQrhS+RNOLn48dz1z3HM3WeYdaOWdz3830cOHHA7bLkCui74TkiwyLJ7Z+bXr/0YuzKsaTYFLdLEsk0Nh/ezOOzHueO7+9g3cF19I/qzw8df+D28rfj7+PvdnkiWY4xhu6VuzPk+iH8feRvun3fjY2HNrpdlqRSqgKYMaalMWa9MWaTMWbARR4vYIyZYYxZYYxZZIyp6rk/whgzyxiz1hiz2hjz6AXPe9jzuquNMW+nzUe6euUKlOPztp/TvERzhiwZwsO/P8yRU0fcLkskQ9t5fCfPz3uejt90ZMHuBTwY+SA/dPyBu6rcpU2rRbzg+uLXM77leJJSkujxYw/m75zvdkmSCpedBWmM8QU2ADcDscBioKu1ds05xwwCjltr/88YUxEYYa290RhTBChirV1ijMkDxAAdrLVrjDHXA88Dbay1p4wxBa21+y5Vi7dmQVpr+Xz957y9+G0K5izIO83eoWpo1XR/X5HM5MCJA4xePprpG6fja3zpWrEr91W9jwKBBdwuTSRb2hO/hwdnPsjfh/+mW6Vu5A3Ie82vaUmjlRLScMGFtKqpeYnmlC1QNk1e679c6yzIusAma+3fnhf7HGgPrDnnmMrAmwDW2nXGmJLGmELW2t3Abs/9x4wxa4Finuc+AAy01p7yPH7J8OVNxhi6VuxKtdBqPDn7SXr82IP+Uf3pWrGrNuuVbO/IqSOMXzWeT9d+SlJKEreWu5U+1ftQKFcht0sTydYK5yrMJy0/4dm5zzJxzUS3y8nwSucvne4B7FJSE8CKATvOuR0LXHfBMcuBjsA8Y0xdoAQQDuw9c4AxpiRQE1jouas80NgY8zpwEnjKWpuhRr9XDa3K1HZTeW7ec7y56E2W7lvK/xr8j1z+udwuTcTrEhIT+HTtp4xfNZ7jicdpXbo1D9Z4kOJ5i7tdmoh45A7IzbAbh5GYkojh6hoMrvZ5wDU1Ulx1vZm0YSQ1Aexin+zC9r+BwBBjzDJgJbAUSDr7AsbkBr4AHrPWHj3nvQsA9YA6wFRjTGl7QZ+oMaY30BugeHHvf6PPlyMfw24YxrhV4xi2dBjrDq7j3WbvUq5AOa/XIuKG08mnmbZhGmNWjOHgyYM0i2hGv8h+2gpFJAPTxJeMLzUBLBaIOOd2OLDr3AM8oepeAONE0S2ePxhj/HHC16fW2i8veN0vPYFrkTEmBQgF9l/w2mOAMeCMAUv1J0tDPsaHXtV6USOsBk/PeZpu33fjhXov0L5sezfKEfGKpJQkvt38LR8s/4Dd8bupW7guj9R6hBphNdwuTUQk00vNLMjFQDljTCljTABwB/DNuQcYY/J7HgPoBcyx1h71hLGPgLXW2ncveN2vgBs8zy8PBAAZehGTOoXrMK3dNKqFVeOF+S/wvz//x8mkk26XJZKmUmwKP2/9mVu/vpWX/nyJkMAQxtw8hrHNxyp8iYikkcu2gFlrk4wx/YCfAV9gnLV2tTGmr+fxUUAl4BNjTDLOAPuenqc3BHoAKz3dkwDPWWt/AMYB44wxq4DTwN0Xdj9mRKE5Qxlz8xhGLhvJhys/ZHXcat5p+o7GwUimZ61l/q75DF0ylLUH11ImXxnev/59boi4IdOOsRARyai0Gfc1mBM7h2fnPkuKTeHVhq9yU4mb3C5J5Kos2buEIUuGsGTfEorlLsZDkQ/RulRr7S8nInINLrUMhQLYNdp1fBdP/fEUKw+spEflHjxe+3ENfpRMY23cWoYtHcbcnXMJzRlKn+p9uK3cbfj76hoWEblW17oOmFxC0dxFmdByAoOjBzNxzURW7l/JoKaDKJyrsNulifynrUe2MnzZcH7e+jN5A/LyeO3H6VqxKzn9crpdmohItqAWsDT009afeHn+y+TwzcHAxgNpUKyB2yWJnGf38d2MWjGKrzd9TYBvAD0q9+DuKnenyYrZIiJyPrWAeUnLki2pUKACT8x+gr6/9aVPjT70rd5X42jEdXEn4hi7cixT1k8BoGvFrvSq1ouQnCEuVyYikj0pgKWxUvlKMbnNZF776zVGLR/Fsn3LGNh4oH7QiSuOnT7Gx6s/ZuKaiZxKPkWHsh3oW70vRXIXcbs0EZFsTV2Q6cRay4xNM3hj4RvkC8jHoKaDqFWolttlSTZxIukEn637jI9WfsTR00dpUbIFD0U+RKl8pdwuTUQk21AXpAuMMXQs15HKIZV5YvYT3PfzfTxe+3HuqnyX1lSSdJOYnMiXG79k9IrR7D+xn0bFGvFIzUeoFFLJ7dJEROQcCmDprGJwRaa0ncJL819icPRgluxdwquNXtWgZ0lTySnJ/LDlB0YuG0ns8VhqFazFoKaDqF2ottuliYjIRagL0kustUxaO4l3o9+lcK7CvNPsHSqHVHa7LMnkrLX8vuN3hi8dzqbDm6gUXImHaz5Mo2KN1NIqIuKyS3VBpmYvSEkDxhh6VO7B+JbjSUxJpMcPPZi2YRqZKQBLxvLX7r+484c7eWzWYySlJDG46WA+b/s5jcMbK3yJiGRwagFzwaGTh3h27rPM3zWftqXb8mK9FwnyD3K7LMkklu9fzrAlw1i4ZyGFcxXmwRoP0q5MO/x8NKJARCQj0SD8DKZAYAFG3jSSMSucTb3Xxq3l3WbvUjp/abdLkwxsw6ENDF86nFk7ZhEcGMyAugPoVL4TAb4BbpcmIiJXSC1gLluwawED5g7gRNIJ/lf/f7Qu3drtkiSD2XF0ByOWj+CHv38gt39u7ql6D90rdVerqYhIBqfNuDO4vfF7eXrO0yzZt4QuFbrwdJ2n1aoh7I3fy5gVY/hy45f4+fjRrVI37qt6H/ly5HO7NBERSQV1QWZwhXIVYmyLsQxdMpSPV3/MqgOrGNx0MOF5wt0uTVxw+ORhPlr1EZ+t+4xkm8zt5W+nd/XehAWFuV2aiIikEbWAZTC/b/+dF+a9AAbeaPQGzSKauV2SeEl8YjyfrPmECasnkJCYQLsy7XigxgMK4iIimZS6IDOZHcd28OTsJ1l7cC33Vr2XR2o+ohluWdip5FNMWTeFsSvHcujUIW4qfhMPRT5E2QJl3S5NRESugbogM5mIPBFMbD2Rtxa9xfhV41m+bzmDmg6iYFBBt0uTNJSYksjXm75m1PJR7E3YS/0i9Xmk1iNUDa3qdmkiIpLO1AKWwX3393e8suAVcvrl5O0mb3NdkevcLkmuUYpN4eetPzNi2Qi2Hd1G9bDqPFrzUeoWqet2aSIikoa0En4m1rZ0Wz5r8xn5c+Sn96+9Gb18NCk2xe2y5CpYa/ljxx90+rYTT89xZroOu2EYk1pNUvgSEclm1AWZCZTJX4bP2nzG/y34P4YvG86y/ct4s9Gb5A/M73ZpkkqL9yxm6JKhLNu/jIg8EQxsPJBWpVrhY/Q7kIhIdqQuyEzEWsu0DdMYuGggITlDGNx0MDXCarhdllzC6rjVDF0ylD93/UnBnAXpG9mXDmU74O/j73ZpIiKSzjQLMotZHbeaJ2c/yd6EvTwV9RTdKnbT5ssZzN+H/2b4suH8uu1X8ufIT69qvehSoQuBfoFulyYiIl6iAJYFHTl1hBfmv8DsHbNpXqI5/9fg/8gdkNvtsrK1FJvC6gOrmbJ+Ct/+/S2BvoHcXeVu7qp8l/5tRESyIS1DkQXly5GPodc7K+cPWTKE9YfW807Td6gQXMHt0rKVQycP8eeuP5m3cx7zd87n0KlDBPgE0KNSD3pW60mBwAJulygiIhmQWsCygJi9MfT/oz9HTx/l+eue59Zyt7pdUpaVYlNYE7eGuTvnMi92HisPrMRiKZCjAA2KNaBRsUY0LNpQwUtERNQFmR0cOHGAAXMGsHDPQjqU7cBz1z1HTr+cbpeVJRw+efifVq5d8zl48iAGQ9XQqjQu1phGxRpROaQyvj6+bpcqIiIZiLogs4HQnKGMvnk0Hyz/gNErRrMmbg3vNnuXEnlLuF1appNiU1gbt5Y5O+cwb+c8Vh1YRYpNIX+O/DQs1pBGxRrRoGgDggOD3S5VREQyKbWAZUHzds7j2bnPkpiSyCsNXqF5yeZul5ThXaqVq1GxRjQq1ogqIVXUyiUiIqmmLshsaE/8Hp7840lW7F9B90rdeaL2E/j7au2pM860cs3dOZd5O52xXGdauRoU9YzlKtZQrVwiInLV1AWZDRXOVZiPW3zMuzHvMmntJFYcWMHgJoMpkruI26W55sipI2dbuebtnHe2latKSBV6V+9No2KNqBpSVa1cIiKS7tQClg38svUXXvrzJfx8/Hiz0Zs0Dm/sdklekWJTWHtwLfNi5zF359yzrVz5cuSjQdEGNC7WmAZFGxCSM8TtUkVEJAtSF6Sw9chWnvzjSTYc2kDv6r15sMaDWbKl52KtXABVQqrQOLyxWrlERMRr1AUplMxXkk9bf8obC99gzIoxLN+3nIFNBhKaM9Tt0q7Jua1c83bOY8WBFWrlEhGRDE8tYNnQjI0zeH3h6+QNyMvbTd4mqvBFw3mGdeTUERbsWsDcnXOZv3M+cSfjAKeV68yMxWqh1dTKJSIirlILmJzn1nK3UjmkMk/+8SS9funFI7Ue4Z4q9+BjfNwu7aJSbArrDq5j3s55zI2de7aVK29AXhoWbUijcGddrszemiciItmHWsCyseOnj/Pyny/zy7ZfaBbejNcavUa+HPncLgvwtHLtXnC2a/FMK1flkMo0KtaIxsUaq5VLREQyNA3Cl/9krWXyuskMjh5MoaBCvNPsHaqEVHGljrOtXDvnsmL/CpJtMnkD8p63LpdauUREJLNQAJPLWrF/BU/+8SRxJ+J4ps4zdK7QGWNMur7nua1c83fN58CJAwBUCq5E4/DGNC7WmKqhVfHzUU+5iIhkPhoDJpdVPaw609pO49l5z/LawteI2RfD/+r/jyD/oDR7j3NbuebtnMfy/ctJtsnkCcjjjOVSK5eIiGQTagGT86TYFMauHMuIZSMombck7zZ7lzL5y1z16x09fZQFuxY4eyzunM/+E/sBp5WrUbFGNA53xnKplUtERLIadUHKFVu0exH95/TnRNIJXqz3Iu3KtEvV86y1rD+0/uyMxXNbuc6M5WpUrJFauUREJMtTAJOrsi9hH0/PeZqYvTHcXv52BtQdQA7fHP867ujpo/y166+z63KplUtERERjwOQqFQwqyNjmYxm2dBjjVo1j9YHVvNPsHcJzh1+8lcs/D/WL1qdxeGMaFm1IWFCY2x9BREQkQ1ILmKTK7B2zeW7ec2Ahp19O9p3YB0DF4Io0LubssVg9rLpauURERDzUAibXrFlEM6a2ncpbi98iwCfg7FgutXKJiIhcOQUwSbXwPOEMu2GY22WIiIhkehlz8z8RERGRLEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvMxYa92uIdWMMfuBben8NqHAgXR+j8xO5+jSdH4uT+fo0nR+Lk/n6NJ0fi7PG+eohLU27GIPZKoA5g3GmGhrbZTbdWRkOkeXpvNzeTpHl6bzc3k6R5em83N5bp8jdUGKiIiIeJkCmIiIiIiXKYD92xi3C8gEdI4uTefn8nSOLk3n5/J0ji5N5+fyXD1HGgMmIiIi4mVqARMRERHxskwbwIwxLY0x640xm4wxAy7yuDHGDPU8vsIYU+tyzzXGBBtjfjXGbPT8XcBz/83GmBhjzErP3zdc5P2+McasOud2DmPMFM97LDTGlEzzk3AJmeD83GOM2W+MWeb50yvtz8KlZZRzZIyZ7XmtM+eioOd+XUNc8vzoGvrnOQHGmDHGmA3GmHXGmNs89+sa4pLnR9eQc3+ec87BMmPMAWPM+57Hsv01dJnzc/XXkLU20/0BfIHNQGkgAFgOVL7gmNbAj4AB6gELL/dc4G1ggOfrAcBbnq9rAkU9X1cFdl7wXh2BycCqc+57EBjl+foOYIrOz3nn5x5guK4hCzAbiLpIjbqGLn1+dA398z7/B7zm+doHCNU1lKrzo2vo4nXFAE10DaXq/Fz1NeTKRZcG/yj1gZ/Puf0s8OwFx4wGup5zez1Q5FLPPXOM5+siwPqLvLcB4oAcntu5gXlAZc4PGD8D9T1f++Es9mZ0fs4ed9UXbRY8R7O5eMDQNXTp86Nr6J9ztAPIpWvois+PriHPOTrn/nKe83VmjLiuoUufn6u+hjJrF2QxnBNwRqznvtQcc6nnFrLW7gbw/F3wIu99G7DUWnvKc/tV4B0g4b/e31qbBBwBQi73wdJIZjg/ALd5moynG2MiLvup0lZGOkcA4z3N1y8aY8yF75/NryG4+PkBXUOnjDH5Pfe9aoxZYoyZZowpdOH7Z9dr6DLnB3QNnbrg/q44rVz2wvfPrtfQBfdfeH7gKq+hzBrAzEXus6k8JjXPvfibGlMFeAvo47kdCZS11s64yhrTS2Y4P98CJa211YHfgAmpeY80lCHOkced1tpqQGPPnx5XUGN6yQznR9eQww8IB+Zba2sBC4DBV1BjeskM50fX0L/dAXx2hTWml8xwfq76GsqsASwWODdlhgO7UnnMpZ671xhTBMDz974zBxljwoEZwF3W2s2eu+sDtY0xW3G62cobY2Zf+P7GGD8gH3DwCj/n1crw58daG3fObxYfArWv+FNem4xyjrDW7vT8fQxnrFzdC98/G19D/3l+dA2dPUdxOC3MZ37RmQacGYisa+gS50fX0D//zzyP1QD8rLUxF3v/bHwNnXnsX+fnmq6h9O7DTY8/OL/R/A2U4p/BdVUuOKYN5w/MW3S55wKDOH9g3tuer/N7jrvtEjWV5PwxTg9x/sDFqTo/552fIud8fSvwV3a8hjyvdWZAsD8wHeirayhV50fX0D/v8zlwg+fre4BpuoZSdX50DZ3/XgOB/7vgPl1Dlz4/V30Nee1CS4d/mNbABpxZDs977uvLP9+cDTDC8/hKzhnEe7Hneu4PAWYCGz1/B3vufwGIB5ad86fgBfWU5PyAEYjzm9YmYBFQWufnvPPzJrDac7HPAipmx2sIyIUzo2aF53wMAXx1DaXq/Oga8vw/A0oAczznaSZQXNdQqs6PrqFzvlfjBJaKF9Sna+jS5+eqryGthC8iIiLiZZl1DJiIiIhIpqUAJiIiIuJlCmAiIiIiXqYAJiIiIuJlCmAiIiIiXqYAJiIiIuJlCmAiIiIiXqYAJiIiIuJl/w8cgCsaxfgX0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAI/CAYAAABEVcwAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyJklEQVR4nO3de5RcZZ3u8edXVX0hSedKAKGBDiNCQiARihyEGU4iIytcJIpLLoooIzLogDPOUQmc4YwunXVwDo4D64CcMAKyRHDUk3VQEEaQGC9A6EAghCQQIEgTB5tLrpBOV9Xv/FG7qndVV3dVX95Udfr7WatX1X4ve7/1Znf10+/eXTF3FwAAAEZXot4DAAAA2BcRsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACCAVL0HUMn+++/vHR0d9R4GAABAVatXr37D3WeWlzdkyOro6FBnZ2e9hwEAAFCVmb1SqZzLhQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAAVQNWWZ2m5n9ycyeHaDezOxGM9tkZs+Y2fGxusVmtjGqWzqaAwcAAGhktaxk3SFp8SD1Z0g6Mvq6TNJ3JcnMkpJuiurnSLrQzOaMZLAAAABjRdWQ5e4rJb01SJMlku70vMckTTWz90haIGmTu7/k7nsk3RO1BQAA2OeNxj1Zh0h6NbbdFZUNVA4AALDPG42QZRXKfJDyyjsxu8zMOs2ss7u7exSGBQAAUD+pUdhHl6RDY9vtkrZIah6gvCJ3XyZpmSSl0+kBw9hoeGLzW9qxu1e5nJRzV84ljx7z2y73vrr8dqGsbzvn5f3j7aVczqu2H/I+y8bU/zVEzxXbzpX3r/Say/q7K5kwJcyUSpqSZkomrKQsYaZUoq+8WJcwJRJlddZXVqyrUBbfZ0lZ8XgJJRPqV5ZIqLSuSlmxzqxfWcIks0q/I6ASd48eo+1YHXO578rlXFl3ZXP5941sLv9eU16WzeXfeyqV50rKVFofnVCpRP57tylpSiUTSiVMTcmEkgkrljUlorqkqSmRf0wljHMPdTcaIeteSVeY2T2S/oukbe7+RzPrlnSkmc2S9JqkCyR9YhSON2L/4/+t0/o/bt+rxzTL/6Av/NBJmGTKPybM8vVRoIi3yW9bSf/S7djzKEyYKvdPJkxNicJ2vL5/e0nFN7pczpWJ3vwy2XzZ7t5c8Q0xk40ec1584y0vK6mLlTWqvmDYF7xSyYQSZkpGZXHFoFHcjh5VOYB48aX3zUH/NgPss6xctfYbYCyqOtbK+xuKSudy+WPfuV3h3EwM9r1Q+Vwuf6ypf6LQvtL3Wv/2Q3oNhbJEX/tc9P2VjcJJLvY9V/j+cy/9Xix5Hv0yV1pWHmBUut8o0PQvK6uPleW80LYvLI0FhV/YmpKF4FUIa/Ewli9LRkGtKd4uatMUhbtUxbK+kNcUhbtkv7JCn9L++ffkynWpWP9Cu0SC0DjWVA1ZZna3pIWS9jezLkn/KKlJktz9Fkn3SzpT0iZJ70i6JKrLmNkVkh6UlJR0m7uvC/Aahuw7589TT2+uYkDp94YYtan0xjqUQMRvVJXFA1w2VxbKYr/xZnOlQa+kfaWgF6+LlRV+GFUKfyXHGWwMZQGx8E9r/batdLt4CljFfoP2LdaXnkfV25f2KxtC9X4DjbVsf5Xau+eD22Arva5BVlkHWKUdyqpsfPU3m8tVXrmtsNLbf9V44ON5jW1qkYh+GcqH+b7V3vLyYn2Fuv79pVQqUaF/5WMVHxPqV1bs169tvL+KK9Gl7VTxWKVtVVIm5QNmbzanTNaVyeXUm/WKZZlsTpmcF5/35lzZXL5Nb8V2ff0zOY/a5bS7N6dMNlPsk8m6enOFtlH/srK9pfDLXjzAlX9vD/U9pKRNjX37v49Urh9of6rWftjvP/3329KU0F2XnqR6qRqy3P3CKvUu6W8GqLtf+RDWUI4+aHK9h4BIImFq5rcz7OO8QnDLuRd/MSuEJX4ZG3vcvS+k5WLBLVta1pvNFUNab1m4K4TC8gBX6BMPlcVjRH3yAb5sFbrmlfS+NrWviFeuH3BFfIir+jWv6JfVq199/klTsr6fuT4alwsBAIOwwgp3xb8Hwlhmlr/U15SU9lOy3sNBg+G/1QEAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAGGvcpWyvlMvWeyQYRKreAwBGnbvkuUG+vOyxlrYVvswkS0qJZOwx0fdYUle+HSvH2FM4L3JZybNlj7Hykja5/m0HPYdi5ZbgHNobclkpuyf66o099lYo3yPlMpXLh9033naQfeei7QJLSIkmKRl99XveLCVTtT1PNo/SvgbqX/Y80bRPn8PjM2Tdfpb0p+fyb3CyITxqiO1Hu7/lx194HPE+qrSrGDhqDTDDrNcoHGOsqfSD06zyD9OBfshW+kFd6Yf13tyHJQYPIIOFD8/m/52rhZhBy3NDaDvQOAbYR6OdZ1X/PRLD+/fvdz6MYB8l+6v1HLIKYWU4gae87wChJdS/qyWj4BELH8mmsrLoeapVapncvzxZFl4KYchzfePPZgZ43ht7nb1SZo+U21VWPkj/0Irz0yQlUpWf1xLYKgXBpv2k//rV8K9hAOMzZL33g9IBR+ffxAs/1Cs+qkp9LY8avf65XGn5kPY1jNcS/0FrCeXDVyL/xhcvL/9KJCVrGqTNQP2thja17GeQepWXDfc4FoW9aqsXQwwWuUJYHG6wiPaRy0q+Z3QDzt4IFiGCXuF8HPE+hjuOsufxtgOdQzWFxcHOobJ/t2GdQxkp01PDPmoZR270z6FkS//Q0e+xOf+DumnCAKGlQvvyvpXKh9Q3OvfGqsJ7QTx8Zff0BbZ4eKslsGV7Y4F4oP5V9tX7rpTd1r+8OK7Y82QLIWuv+4v/Vu8RAGNPrZfI4j/ca17xSO7TlwwQGfI55FKqJbZSUQgw0SoXwjOLLiOm8qtCY01xUaI+xmfIAjB08fuHgOHgHMLeVucwzq+OAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARQU8gys8VmttHMNpnZ0gr108xsuZk9Y2arzGxurO5vzexZM1tnZn83imMHAABoWFVDlpklJd0k6QxJcyRdaGZzyppdI2mNux8n6WJJN0R950r6nKQFkuZJOtvMjhy94QMAADSmWlayFkja5O4vufseSfdIWlLWZo6khyXJ3TdI6jCzAyXNlvSYu7/j7hlJv5b00VEbPQAAQIOqJWQdIunV2HZXVBb3tKRzJcnMFkg6XFK7pGclnWpmM8xsgqQzJR060kEDAAA0ulQNbaxCmZdtXyfpBjNbI2mtpKckZdx9vZl9S9IvJe1UPoxlKh7E7DJJl0nSYYcdVtPgAQAAGlUtK1ldKl19ape0Jd7A3be7+yXuPl/5e7JmSno5qvueux/v7qdKekvSC5UO4u7L3D3t7umZM2cO/ZUAAAA0kFpC1hOSjjSzWWbWLOkCSffGG5jZ1KhOki6VtNLdt0d1B0SPhyl/SfHu0Ro8AABAo6p6udDdM2Z2haQHJSUl3ebu68zs8qj+FuVvcL/TzLKSnpP02dgufmpmMyT1Svobd397tF8EAABAo6nlniy5+/2S7i8ruyX2/FFJFT+awd3/YiQDBAAAGIv4xHcAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQAA1hSwzW2xmG81sk5ktrVA/zcyWm9kzZrbKzObG6r5kZuvM7Fkzu9vMWkfzBQAAADSiqiHLzJKSbpJ0hqQ5ki40szllza6RtMbdj5N0saQbor6HSPqipLS7z5WUlHTB6A0fAACgMdWykrVA0iZ3f8nd90i6R9KSsjZzJD0sSe6+QVKHmR0Y1aUk7WdmKUkTJG0ZlZEDAAA0sFpC1iGSXo1td0VlcU9LOleSzGyBpMMltbv7a5Kul/QHSX+UtM3d/2OkgwYAAGh0tYQsq1DmZdvXSZpmZmskXSnpKUkZM5um/KrXLEkHS5poZhdVPIjZZWbWaWad3d3dtY4fAACgIdUSsrokHRrbblfZJT933+7ul7j7fOXvyZop6WVJfynpZXfvdvdeSf9X0smVDuLuy9w97e7pmTNnDv2VAAAANJBaQtYTko40s1lm1qz8jev3xhuY2dSoTpIulbTS3bcrf5nwJDObYGYm6TRJ60dv+AAAAI0pVa2Bu2fM7ApJDyr/14G3ufs6M7s8qr9F0mxJd5pZVtJzkj4b1T1uZj+R9KSkjPKXEZcFeSUAAAANxNzLb6+qv3Q67Z2dnfUeBgAAQFVmttrd0+XlfOI7AABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACCAmkKWmS02s41mtsnMllaon2Zmy83sGTNbZWZzo/KjzGxN7Gu7mf3dKL8GAACAhpOq1sDMkpJukvQhSV2SnjCze939uVizayStcfePmtnRUfvT3H2jpPmx/bwmafnovgQAAIDGU8tK1gJJm9z9JXffI+keSUvK2syR9LAkufsGSR1mdmBZm9Mkvejur4xwzAAAAA2vlpB1iKRXY9tdUVnc05LOlSQzWyDpcEntZW0ukHT38IYJAAAwttQSsqxCmZdtXydpmpmtkXSlpKckZYo7MGuWdI6kHw94ELPLzKzTzDq7u7trGBYAAEDjqnpPlvIrV4fGttslbYk3cPftki6RJDMzSS9HXwVnSHrS3V8f6CDuvkzSMklKp9PlIQ4AAGBMqWUl6wlJR5rZrGhF6gJJ98YbmNnUqE6SLpW0MgpeBReKS4UAAGAcqbqS5e4ZM7tC0oOSkpJuc/d1ZnZ5VH+LpNmS7jSzrKTnJH220N/MJij/l4l/HWD8AAAADamWy4Vy9/sl3V9Wdkvs+aOSjhyg7zuSZoxgjAAAAGNOTSELAACMXb29verq6tLu3bvrPZQxrbW1Ve3t7WpqaqqpPSELAIB9XFdXl9ra2tTR0aH836dhqNxdb775prq6ujRr1qya+vB/FwIAsI/bvXu3ZsyYQcAaATPTjBkzhrQaSMgCAGAcIGCN3FDnkJAFAAAQACELAADsdT/+8Y81e/ZsLVq0aET72bx5s374wx8Oq+/JJ588omNXQ8gCAAB7lbvr1ltv1c0336xHHnmkpj6ZTKZi+WAha6A+Bb///e9rOvZw8deFAACMI1//2To9t2V79YZDMOfgyfrHDx8zaJvNmzfrjDPO0KJFi/Td735XkvTKK6/onHPO0Te+8Q19/vOfV2dnp1KplP7lX/5FixYt0h133KH77rtPu3fv1q5du/SrX/2q336XLl2q9evXa/78+fr0pz+tadOmlfS59957tWTJEr399tvq7e3VN7/5TS1ZskSSNGnSJO3cuVMrVqzQ1772Ne2///569tlndcIJJ+gHP/jBiO9jI2QBAIC9YuPGjbr99tt18803a+HChbr++uuVTqf17W9/W5K0du1abdiwQaeffrqef/55SdKjjz6qZ555RtOnT6+4z+uuu07XX3+9fv7zn0uS7rjjjpI+mUxGy5cv1+TJk/XGG2/opJNO0jnnnNMvQD311FNat26dDj74YJ1yyin63e9+pz//8z8f0eslZAEAMI5UW3EK6fDDD9dJJ53Ur/y3v/2trrzySknS0UcfrcMPP7wYsj70oQ8NGLAGEu/j7rrmmmu0cuVKJRIJvfbaa3r99dd10EEHlfRZsGCB2tvbJUnz58/X5s2bCVkAAGBsmDhxYsVydx9yn1qPc9ddd6m7u1urV69WU1OTOjo6Kn7WVUtLS/F5Mpmsej9XLbjxHQAA1NWpp56qu+66S5L0/PPP6w9/+IOOOuqomvq2tbVpx44dA9Zv27ZNBxxwgJqamvTII4/olVdeGZUx14KQBQAA6uoLX/iCstmsjj32WJ1//vm64447SlaWBnPccccplUpp3rx5+s53vtOv/pOf/KQ6OzuVTqd111136eijjx7t4Q/IBluiq5d0Ou2dnZ31HgYAAPuE9evXa/bs2fUexj6h0lya2Wp3T5e3ZSULAAAgAG58BwAADW/t2rX61Kc+VVLW0tKixx9/vE4jqo6QBQAAGt6xxx6rNWvW1HsYQ8LlQgAAgAAIWQAAAAEQsgAAAAIgZAEAgDFpxYoV+v3vfz/kfp2dnfriF78YYESluPEdAACMSStWrNCkSZN08skn96vLZDJKpSrHnHQ6rXS638dajTpWsgAAQHC7du3SWWedpXnz5mnu3Ln6/ve/r/POO69Yv2LFCn34wx+WJD3wwAM6/vjjNW/ePJ122mkV97d582bdcsst+s53vqP58+frN7/5jT7zmc/o7//+77Vo0SJdddVVWrVqlU4++WS9//3v18knn6yNGzcWj3X22WdLkr72ta/pr/7qr7Rw4UIdccQRuvHGG0ftNbOSBQDAePKLpdJ/rh3dfR50rHTGdYM2eeCBB3TwwQfrvvvuk5T/PwWvvfZa7dq1SxMnTtSPfvQjnX/++eru7tbnPvc5rVy5UrNmzdJbb71VcX8dHR26/PLLNWnSJH35y1+WJH3ve9/T888/r4ceekjJZFLbt2/XypUrlUql9NBDD+maa67RT3/603772rBhgx555BHt2LFDRx11lD7/+c+rqalphJPCShYAANgLjj32WD300EO66qqr9Jvf/EZTpkzR4sWL9bOf/UyZTEb33XeflixZoscee0ynnnqqZs2aJUmaPn36kI7z8Y9/XMlkUlI+yH384x/X3Llz9aUvfUnr1q2r2Oess85SS0uL9t9/fx1wwAF6/fXXR/ZiI6xkAQAwnlRZcQrlfe97n1avXq37779fV199tU4//XSdf/75uummmzR9+nSdeOKJamtrk7vLzIZ9nIkTJxafX3vttVq0aJGWL1+uzZs3a+HChRX7xP8z6mQyqUwmM+zjx7GSBQAAgtuyZYsmTJigiy66SF/+8pf15JNPauHChXryySd166236vzzz5ckfeADH9Cvf/1rvfzyy5I04OVCSWpra9OOHTsGrN+2bZsOOeQQSdIdd9wxei+mRoQsAAAQ3Nq1a7VgwQLNnz9f//RP/6R/+Id/UDKZ1Nlnn61f/OIXxRvRZ86cqWXLluncc8/VvHnziuGrkg9/+MNavnx58cb3cl/96ld19dVX65RTTlE2mw322gZi7r7XD1pNOp32zs7Oeg8DAIB9wvr16zV79ux6D2OfUGkuzWy1u/f7TAhWsgAAAALgxncAANDQbr/9dt1www0lZaeccopuuummOo2oNoQsAADQ0C655BJdcskl9R7GkHG5EAAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAQHCTJk2que3WrVt18803D+s4Z555prZu3TqsvqONkAUAAOpioE9hHyxkVfvk9vvvv19Tp04d6dBGBSELAADsNStWrNCiRYv0iU98Qscee2zFNkuXLtWLL76o+fPn6ytf+UrFPh/5yEd0wgkn6JhjjtGyZcuKfTs6OvTGG29o8+bNmj17tj73uc/pmGOO0emnn6533313r7zGAj4nCwCAceRbq76lDW9tGNV9Hj39aF214Kqa269atUrPPvusZs2aVbH+uuuu07PPPqs1a9ZIygez8j633Xabpk+frnfffVcnnniiPvaxj2nGjBkl+3nhhRd0991369Zbb9V5552nn/70p7rooouG9yKHgZAFAAD2qgULFgwYsGrtc+ONN2r58uWSpFdffVUvvPBCv5A1a9YszZ8/X5J0wgknaPPmzSMa91ARsgAAGEeGsuIUysSJE0fUZ8WKFXrooYf06KOPasKECVq4cKF2797dr09LS0vxeTKZ3OuXC7knCwAANJS2tjbt2LFjwPpt27Zp2rRpmjBhgjZs2KDHHntsL46udoQsAADQUGbMmKFTTjlFc+fO1Ve+8pV+9YsXL1Ymk9Fxxx2na6+9VieddFIdRlmduXu9x9BPOp32zs7Oeg8DAIB9wvr16zV79ux6D2OfUGkuzWy1u6fL27KSBQAAEAA3vgMAgLp48803ddppp/Urf/jhh/v9peBYRMgCAAB1MWPGjOJnYe2LuFwIAMA40Ij3YI81Q51DQhYAAPu41tZWvfnmmwStEXB3vfnmm2ptba25D5cLAQDYx7W3t6urq0vd3d31HsqY1traqvb29prbE7IAANjHNTU1Dfm/scHIcbkQAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgABqCllmttjMNprZJjNbWqF+mpktN7NnzGyVmc2N1U01s5+Y2QYzW29mHxjNFwAAANCIqoYsM0tKuknSGZLmSLrQzOaUNbtG0hp3P07SxZJuiNXdIOkBdz9a0jxJ60dj4AAAAI2slpWsBZI2uftL7r5H0j2SlpS1mSPpYUly9w2SOszsQDObLOlUSd+L6va4+9bRGjwAAECjqiVkHSLp1dh2V1QW97SkcyXJzBZIOlxSu6QjJHVLut3MnjKzfzOziSMeNQAAQIOrJWRZhTIv275O0jQzWyPpSklPScpISkk6XtJ33f39knZJ6ndPlySZ2WVm1mlmnd3d3TUOHwAAoDHVErK6JB0a226XtCXewN23u/sl7j5f+XuyZkp6Oerb5e6PR01/onzo6sfdl7l72t3TM2fOHNqrAAAAaDC1hKwnJB1pZrPMrFnSBZLujTeI/oKwOdq8VNLKKHj9p6RXzeyoqO40Sc+N0tgBAAAaVqpaA3fPmNkVkh6UlJR0m7uvM7PLo/pbJM2WdKeZZZUPUZ+N7eJKSXdFIewlSZeM8msAAABoOOZefntV/aXTae/s7Kz3MAAAAKoys9Xuni4v5xPfAQAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAAB1BSyzGyxmW00s01mtrRC/TQzW25mz5jZKjObG6vbbGZrzWyNmXWO5uABAAAaVapaAzNLSrpJ0ockdUl6wszudffnYs2ukbTG3T9qZkdH7U+L1S9y9zdGcdwAAAANrZaVrAWSNrn7S+6+R9I9kpaUtZkj6WFJcvcNkjrM7MBRHSkAAMAYUkvIOkTSq7Htrqgs7mlJ50qSmS2QdLik9qjOJf2Hma02s8tGNlwAAICxoerlQklWoczLtq+TdIOZrZG0VtJTkjJR3SnuvsXMDpD0SzPb4O4r+x0kH8Auk6TDDjusxuEDAAA0plpWsrokHRrbbpe0Jd7A3be7+yXuPl/SxZJmSno5qtsSPf5J0nLlLz/24+7L3D3t7umZM2cO9XUAAAA0lFpC1hOSjjSzWWbWLOkCSffGG5jZ1KhOki6VtNLdt5vZRDNri9pMlHS6pGdHb/gAAACNqerlQnfPmNkVkh6UlJR0m7uvM7PLo/pbJM2WdKeZZSU9J+mzUfcDJS03s8KxfujuD4z+ywAAAGgs5l5+e1X9pdNp7+zkI7UAAEDjM7PV7p4uL+cT3wEAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAAdQUssxssZltNLNNZra0Qv00M1tuZs+Y2Sozm1tWnzSzp8zs56M1cAAAgEZWNWSZWVLSTZLOkDRH0oVmNqes2TWS1rj7cZIulnRDWf3fSlo/8uECAACMDbWsZC2QtMndX3L3PZLukbSkrM0cSQ9LkrtvkNRhZgdKkpm1SzpL0r+N2qgBAAAaXC0h6xBJr8a2u6KyuKclnStJZrZA0uGS2qO6f5X0VUm5kQwUAABgLKklZFmFMi/bvk7SNDNbI+lKSU9JypjZ2ZL+5O6rqx7E7DIz6zSzzu7u7hqGBQAA0LhSNbTpknRobLtd0pZ4A3ffLukSSTIzk/Ry9HWBpHPM7ExJrZImm9kP3P2i8oO4+zJJyyQpnU6XhzgAAIAxpZaVrCckHWlms8ysWfngdG+8gZlNjeok6VJJK919u7tf7e7t7t4R9ftVpYAFAACwr6m6kuXuGTO7QtKDkpKSbnP3dWZ2eVR/i6TZku40s6yk5yR9NuCYAQAAGp65N96VuXQ67Z2dnfUeBgAAQFVmttrd0+XlfOI7AABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQACELAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABEDIAgAACICQBQAAEAAhCwAAIABCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAACFkAAAABELIAAAACIGQBAAAEQMgCAAAIgJAFAAAQQKreA0BjcHf1ZHu0s3endu7ZqZ5sj6a0TNGUlinaL7VfvYcHAMCYQ8jaB+Q8p529O7Vrzy7t6N2hnXt2FsPSzt6+5zv27NCu3r42u3p3aceeHcU2mVym4v5bk62a0jJF01qn5R9bphW3p7ZM1dSWqfmy1nzd1Jap2i+1n8xsL88E9gZ31+7sbm3r2abte7YXH7f3bC8pezfzrpqTzWpNtaol2aLWVKtak2XPUy1qTbaW1LWkWrRfar98u2SrUokU5xKAMYmQVWd7sntKgs7OPaXhqBiMBmmzq3dX1eMkLKGJTRPV1tSmSc2TNKlpkmZOmKlZU2ZpUtMkTWqepLbmNk1smqhJTZPUkmzRjj079HbP29rWs01v744ee97W+l3rtbVnq7b3bJfLKx6vOdGcD2CtU0tC2YAhrXWaJqQm8MN0L8rmstqxZ4e27dlWEo4qhqc920oe9+T2DLjfpCU1uXmyJjRNUE+2Rz2ZHu3O7lZvrndY40xashjMBgtrLcm+cBZ/3ppq7etbFuzK+7SmWpUw7qIAGknOc8rmsurN9SrrWWVzWWU8o0wu/1UoK9QXygoLBycedGLdxk7IGqac5/RO7zslK0blK0Ulq0llbQrPa/nB05Js0aSmWAhqnqSZ+83Mh6bmvtBUCEvF0NTUVmwTYmUpm8tq+57tFYPY1t1btbVna7Hu+bef17aebdras3XAYJZKpCoGsHgQK4S0QoCb1DRpXAczd9e7mXdLQtFggSkenHb07hh03xNSE4qXjCc3T9YRU4/Q5ObJmtwyWVOap/R7LLSb2DSx4r9JNpdVTzYfuHoyPXo3+656Mj3Fst2Z3cW64vNsT2l51C7eZ2vP1r6yWL+c54Y1p82J5n5BrBjOYgGvGM4qtB0o/BXbRX0Sluj7foh9WxTKio9eebtf+/J2A7UvazfYPgbqU3O7gdrX8HrjCueUqfSx78EGbVfpnBywT1nboe57wLFWqBuwT42vN+e5fNjwTD58xAJGyfOy+t5cr7K5bEl9JpcpLYuHmFifgY5Ry3Fr6RMPUVnPDvt7WcpfiXnioieG3X+kxmXI6s32akfvjv6X1+IhaM/OAdsULrUNFBYKTFYMPIWws/9++6tjckc+DMXCUSFAtTW39QtMTcmmvTQzQ5NMJDWtdZqmtU6ruU/Oc/kVst1va2tPFMRi4awQ1rb2bNWLW1/U1p6t2tazTVnPVtxfylKa0jKl4qpZSUhr7Xve1tzWcKsVmVymZNWo2qW4+MrSQJd5pfz8TG6ZrMnNkzWlZYpmTpipP5v6Z8VAVP4YD05NidE975KJpCYkJmhC04RR3W8l7q7eXG/lcFYW2nqyPXo3825x1W2w8Ldt9za9nn29X9vBVvaAfU0qkVJToklJSyqZSCplKSUTydKyREopSymVSBXLmpPNmpCaUNKn0K7wPGmD7CeRVNIqlzUlmkr3G9WnEvWNOeMyZJ338/O0aeumQds0J5r7rQpNb51eclktfumtfDWpsHrUaD/M6y1hieLqSK0KwaxSECsPaZu3by5uZ7xy+Ehasi+YVQhhhVWz+PbklslV/y3dXe9k3hkwKMVXlrb3bC8JTNUu+U5qmtQXhlom670T3lsSkAZaWRqv98aZmZqTzWpONkvN4Y+X81wxxMWDXKUVtt2ZvvLCb+iVVjdGvJoyxFWaIe27SruhrtIM1lYaxgpelXYlZTWu9o3WSt9o77sYYgrhI1EabCqFnUKIKYSUeH28f6X6ZCLZ7/VgYOMyZH36mE9rV++u0stqzaWhqTm5F96ZUZN4MDtMh9XUx921s3dnPojt3loxnBW+Xt35qta+sVZv97w94KpQwhKa3Dy5JHhlPdtvtWmgYCflf/srhJ8pLVN0wIQDdOS0I6tegmtrbqv7b2MYXMISmtC0d1bpAIwd4/Kd+yPv/Ui9h4DAzExtzW1qa27ToW2H1tSnsBJVcm9ZFNJKgtnurdqya0txReygiQf1u/RWCErx8vG6qgQA49W4DFlAJWamiU0TNbFpotrb2us9HADAGMcNQwAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABAAIQsAACAAQhYAAEAAhCwAAIAAagpZZrbYzDaa2SYzW1qhfpqZLTezZ8xslZnNjcpbo+2nzWydmX19tF8AAABAI6oasswsKekmSWdImiPpQjObU9bsGklr3P04SRdLuiEq75H0QXefJ2m+pMVmdtIojR0AAKBh1bKStUDSJnd/yd33SLpH0pKyNnMkPSxJ7r5BUoeZHeh5O6M2TdFX//+SHAAAYB9TS8g6RNKrse2uqCzuaUnnSpKZLZB0uKT2aDtpZmsk/UnSL9398RGOGQAAoOHVErKsQln5atR1kqZFYepKSU9JykiSu2fdfb7yoWtB4X6tfgcxu8zMOs2ss7u7u8bhAwAANKZaQlaXpENj2+2StsQbuPt2d78kClMXS5op6eWyNlslrZC0uNJB3H2Zu6fdPT1z5sxaxw8AANCQaglZT0g60sxmmVmzpAsk3RtvYGZTozpJulTSSnffbmYzzWxq1GY/SX8pacOojR4AAKBBpao1cPeMmV0h6UFJSUm3ufs6M7s8qr9F0mxJd5pZVtJzkj4bdX+PpO9Hf6GYkPTv7v7zAK8DAACgoZh74/2xXzqd9s7OznoPAwAAoCozW+3u6fJyPvEdAAAgAEIWAABAAIQsAACAAAhZAAAAARCyAAAAAiBkAQAABNCQH+FgZt2SXgl8mP0lvRH4GGMZ81MdczQ45qc65mhwzE91zNHg9tb8HO7u/f67moYMWXuDmXVW+kwL5DE/1TFHg2N+qmOOBsf8VMccDa7e88PlQgAAgAAIWQAAAAGM55C1rN4DaHDMT3XM0eCYn+qYo8ExP9UxR4Or6/yM23uyAAAAQhrPK1kAAADBNHzIMrPFZrbRzDaZ2dIK9WZmN0b1z5jZ8dX6mtl0M/ulmb0QPU6Lyj9kZqvNbG30+MEKx7vXzJ6NbbeY2Y+iYzxuZh2jPgmDGAPz8xkz6zazNdHXpaM/C4NrlDkysxXRvgpzcUBUzjmkQeenrudQA81Ps5ktM7PnzWyDmX0sKq/r+TPY64zV13uOxv05ZGZtsde/xszeMLN/jeo4h1R1joZ3Drl7w35JSkp6UdIRkpolPS1pTlmbMyX9QpJJOknS49X6SvpnSUuj50slfSt6/n5JB0fP50p6rexY50r6oaRnY2VfkHRL9PwCST9ifkrm5zOS/jfnkEvSCknpCmPkHBp8fup2DjXY/Hxd0jej5wlJ+9f7/BlDc8Q51H9cqyWdyjlU8xwN6xza6yfcECf+A5IejG1fLenqsjb/R9KFse2Nkt4zWN9Cm+j5eyRtrHBsk/SmpJZoe5Kk30qao9IQ8aCkD0TPU8p/6JkxP8V2wzox99E5WqHKIYJzaPD5qds51GDz86qkiY10/oyhOeIcKi0/Mpqrwn3ZnEPV52hY51CjXy48RPkXWdAVldXSZrC+B7r7HyUpejygwrE/Jukpd++Jtr8h6duS3hno+O6ekbRN0oxqL2yUjIX5kaSPRcu7PzGzQ6u+qtHVSHMkSbdHS83XmpmVH3+cn0NS5fmR6ncONcT8mNnUqOwbZvakmf3YzA4sP34dzp+S40cacY6kcX4OlZVfqPxqlZcffzyfQ2Xl5XMkDeMcavSQZRXKvMY2tfStfFCzYyR9S9JfR9vzJb3X3ZcPc4yhjIX5+ZmkDnc/TtJDkr5fyzFGUUPMUeST7n6spL+Ivj41hDGGMhbmp57nUKPMT0pSu6Tfufvxkh6VdP0QxhjSWJgjzqFSF0i6e4hjDGkszNGwzqFGD1ldkuJpsV3SlhrbDNb3dTN7jyRFj38qNDKzdknLJV3s7i9GxR+QdIKZbVb+ktj7zGxF+fHNLCVpiqS3hvg6h6vh58fd34z9hnCrpBOG/CpHplHmSO7+WvS4Q/l71xaUH38cn0MDzk+dz6FGmZ83lV8lLvwi82NJhRt/63n+lBw/0nBzxDnUx8zmSUq5++pKxx/n51Chrt8cDfsc2hvXW4f7pfxvJi9JmqW+G9qOKWtzlkpvhltVra+k/6XSm+H+OXo+NWr3sUHG1KHSe47+RqU3DP4781MyP++JPf+opMfG4zkU7atwE26TpJ9IupxzqKb5qds51CjzE9XdI+mD0fPPSPpxvc+fMTRHnEN9x7pO0tfLyjiHqs/RsM6hvTaJI5j8MyU9r/xfD/z3qOxy9b0Bm6Sbovq1it04W6lvVD5D0sOSXogep0fl/yBpl6Q1sa8DysbTodIQ0ar8b0ybJK2SdATzUzI//1PSuuiEfkTS0ePxHJI0Ufm/VHkmmo8bJCU5h2qan7qeQ40wP1Hd4ZJWRnP0sKTDGuH8GSNzxDnU1++l8tfPOVTTHA3rHOIT3wEAAAJo9HuyAAAAxiRCFgAAQACELAAAgAAIWQAAAAEQsgAAAAIgZAEAAARAyAIAAAiAkAUAABDA/weBcrrPJX1T6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.9274082603996057, 0.9322021010753989)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Est:50 ccp_alpha:0.000010 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9995651701271877 \n",
    "#Est:100 ccp_alpha:0.000030 min_samples_split:3 accuracy test max:0.9426832361146336 accuracy train max:0.999891292531797 \n",
    "#Est:200 ccp_alpha:0.000070 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9989129253179694 \n",
    "def feature_check(alpha, n_est=50,ccp_alpha=0.00005,min_samples_split=2, C=None, kernel=None, solver=None,penalty=None):\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=0,test_size=0.3,stratify=target)\n",
    "    \n",
    "    results = defaultdict(lambda: [])\n",
    "    \n",
    "    for n in alpha:\n",
    "        kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "        all_features = None\n",
    "\n",
    "        #print(\"Alpha:{:.6f}\".format(number_of_features))\n",
    "\n",
    "        #make 5 fold cross valdation with use of features selected by train split\n",
    "\n",
    "        for train_index, test_index in kf.split(df,target):\n",
    "            X_train = df.iloc[train_index]\n",
    "            X_train, X_test, y_train, y_test = df.iloc[train_index], df.iloc[test_index], target[train_index], target[test_index]\n",
    "\n",
    "            #select K features with use of random forest feature selection\n",
    "            #X_train_f, ft = rforest_features(X_train, y_train, n)\n",
    "            X_train_f, ft = lasso(X_train, y_train, alpha=n)\n",
    "            if all_features == None:\n",
    "                all_features = set(ft)\n",
    "            else:\n",
    "                all_features = all_features.intersection(set(ft))\n",
    "            #for f in ft:\n",
    "            #    all_features.append(f)\n",
    "\n",
    "            #keep only selected features in train and test splits\n",
    "            X_test_f = X_test[ft]\n",
    "\n",
    "    \n",
    "            lr_model = LogisticRegression(max_iter=10000,penalty=\"l2\",\n",
    "                                                            solver=\"saga\",C=77.9)\n",
    "            lr_model.fit(X_train_f,y_train)\n",
    "\n",
    "            results[\"k_lr_test\"].append(accuracy_score(y_test, lr_model.predict(X_test_f)))\n",
    "            results[\"k_lr_train\"].append(accuracy_score(y_train, lr_model.predict(X_train_f)))\n",
    "            \n",
    "            \n",
    "            #Kernel:rbf C:19 accuracy test max:0.9264951920766673 accuracy train max:0.9580352710950134 \n",
    "            #Kernel:rbf C:20 accuracy test max:0.9258100000405267 accuracy train max:0.9572173665580053 \n",
    "            #Kernel:rbf C:21 accuracy test max:0.9262675763191611 accuracy train max:0.9585108039737607 \n",
    "            svc_model = SVC(kernel=\"rbf\", C=19)\n",
    "            svc_model.fit(X_train_f,y_train)\n",
    "            results[\"k_svc_test\"].append(accuracy_score(y_test, svc_model.predict(X_test_f)))\n",
    "            results[\"k_svc_train\"].append(accuracy_score(y_train, svc_model.predict(X_train_f)))\n",
    "\n",
    "            #Est:50 ccp_alpha:0.000010 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9995651701271877 \n",
    "            #Est:100 ccp_alpha:0.000030 min_samples_split:3 accuracy test max:0.9426832361146336 accuracy train max:0.999891292531797 \n",
    "            #Est:200 ccp_alpha:0.000070 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9989129253179694 \n",
    "\n",
    "            rfor_model = RandomForestClassifier(n_estimators=100,#bootstrap=False,  #best 150 - 100\n",
    "                                                                criterion='gini',\n",
    "                                                                ccp_alpha=0.000030, #best 0.0001 - 0.00005\n",
    "                                                                min_samples_split=3) #best 2 \n",
    "            rfor_model.fit(X_train_f,y_train)\n",
    "            results[\"k_rfor_test\"].append(accuracy_score(y_test, rfor_model.predict(X_test_f)))\n",
    "            results[\"k_rfor_train\"].append(accuracy_score(y_train, rfor_model.predict(X_train_f)))\n",
    "            results[\"k_rfor_unseen\"].append(accuracy_score(target_test, rfor_model.predict(df_test[ft])))\n",
    "        results[\"lr_test\"].append( sum(results[\"k_lr_test\"]) / len(results[\"k_lr_test\"]))\n",
    "        results[\"lr_train\"].append(sum(results[\"k_lr_train\"])/ len(results[\"k_lr_train\"]) )\n",
    "        results[\"lr_unseen\"].append(sum(results[\"k_lr_unseen\"])/ len(results[\"k_lr_unseen\"]) )\n",
    "        results[\"k_lr_train\"] = []\n",
    "        results[\"k_lr_test\"] = []\n",
    "        \n",
    "        results[\"svc_test\"].append( sum(results[\"k_svc_test\"]) / len(results[\"k_svc_test\"]))\n",
    "        results[\"svc_train\"].append(sum(results[\"k_svc_train\"])/ len(results[\"k_svc_train\"]) )\n",
    "        results[\"k_svc_train\"] = []\n",
    "        results[\"k_svc_test\"] = []\n",
    "        \n",
    "        results[\"rfor_test\"].append( sum(results[\"k_rfor_test\"]) / len(results[\"k_rfor_test\"]))\n",
    "        results[\"rfor_train\"].append(sum(results[\"k_rfor_train\"])/ len(results[\"k_rfor_train\"]) )\n",
    "        results[\"k_rfor_train\"] = []\n",
    "        results[\"k_rfor_test\"] = []\n",
    "    \n",
    "    plt.figure(figsize=(10.0,10.0))\n",
    "    plt.plot(alpha, results[\"rfor_test\"],label=\"rfor_test\")\n",
    "    plt.plot(alpha, results[\"svc_test\"],label=\"svc_test\")\n",
    "    plt.plot(alpha, results[\"lr_test\"],label=\"lr_test\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10.0,10.0))\n",
    "    plt.plot(alpha, results[\"rfor_train\"],label=\"rfor_train\")\n",
    "    plt.plot(alpha, results[\"svc_train\"],label=\"svc_train\")\n",
    "    plt.plot(alpha, results[\"lr_train\"],label=\"lr_train\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    #print(results[\"rfor_test\"])\n",
    "    #print(results[\"rfor_train\"])\n",
    "    return max(results[\"lr_test\"]), results[\"lr_train\"][results[\"lr_test\"].index(max(results[\"lr_test\"]))]\n",
    "#-------------------------------------\n",
    "#random forest parameters fine tuning\n",
    "#-------------------------------------\n",
    "#for n_est in [50,100,150,200]:\n",
    "#    for ccp_alpha in [0.00001, 0.00002,0.00003, 0.00004,0.00005, 0.00006,0.00007]:\n",
    "#        for min_samples_split in [2,3,4]:\n",
    "#            res_test, res_train = feature_check(range(30,90,5),n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split =min_samples_split )\n",
    "#            print(\"Est:{} ccp_alpha:{:.6f} min_samples_split:{} accuracy test max:{} accuracy train max:{} \".format(n_est,ccp_alpha,min_samples_split,res_test, res_train))\n",
    "\n",
    "#---------------------------\n",
    "#SVM parameters fine tuning\n",
    "#---------------------------\n",
    "#for kernel in [\"rbf\"]:\n",
    "#    for c in [18,19,20,21,22]:\n",
    "#        res_test, res_train = feature_check(range(30,90,5),C=c, kernel= kernel)#,n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split =min_samples_split )\n",
    "#        print(\"Kernel:{} C:{} accuracy test max:{} accuracy train max:{} \".format(kernel, c,res_test, res_train))\n",
    "\"\"\"\n",
    "for penalty in ['elasticnet','none']:\n",
    "    for C in np.logspace(-4,4,20):\n",
    "        for solver in ['lbfgs','newton-cg','liblinear','sag','saga']:\n",
    "            if penalty in ['l1','elasticnet'] and solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:\n",
    "                continue\n",
    "            if (penalty == 'elasticnet' and solver != \"saga\") or (penalty=='none' and solver =='liblinear'):\n",
    "                continue\n",
    "            #if solver in ['newton-cg', 'lbfgs', 'sag', 'saga'] and penalty in ['l2', 'none']\n",
    "            res_test, res_train = feature_check(range(30,50,5),C=C, solver= solver, penalty=penalty)# kernel= kernel)#,n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split \n",
    "            print(\"Penalty:{} C:{} solver:{} accuracy test max:{} accuracy train max:{} \".format(penalty, C, solver,res_test, res_train))\n",
    "\"\"\"\n",
    "#for penalty in [\"l2\"]:\n",
    "#    for solver in [\"saga\"]:#\"saga\"]:#,\"sag\",\"liblinear\"]:\n",
    "#        if penalty == \"l1\" and solver != \"liblinear\":\n",
    "#            continue\n",
    "#        for C in np.arange(77.0,79.0,0.1):\n",
    "#            res_test, res_train = feature_check(range(30,50,5),C=C, solver= solver, penalty=penalty)# kernel= kernel)#,n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split \n",
    "#            print(\"Penalty:{} C:{} solver:{} accuracy test max:{} accuracy train max:{} \".format(penalty, C, solver,res_test, res_train))\n",
    "\n",
    "#print(\"Test:\",res_test)\n",
    "#print(\"Train:\",res_train)\n",
    "\n",
    "feature_check(list(np.arange(0.00024,0.00028,0.000005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models parameter fine tuning with feature selection based on model\n",
    "### - We perform the following procedure for two best models in our case (XGBoost and Random forest)\n",
    "\n",
    "### - Since procedure take a lot of time to execute, we store some best case values in code.\n",
    "\n",
    "### -We working on small ranges of model parameters in order to reduce execution time and find reasonable amount of time the best model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost fine tune and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:307, Shape:(9190, 307)\n",
      "N:206, Shape:(9190, 206)\n",
      "Time spend:1.3833333333333333 min. left comp: 99 left time:136.95\n",
      "N:235, Shape:(9190, 235)\n",
      "N:239, Shape:(9190, 239)\n",
      "N:241, Shape:(9190, 241)\n",
      "Time spend:1.95 min. left comp: 98 left time:95.55\n",
      "N:307, Shape:(9198, 307)\n",
      "N:206, Shape:(9198, 206)\n",
      "Time spend:3.2666666666666666 min. left comp: 97 left time:105.62222222222222\n",
      "N:235, Shape:(9198, 235)\n",
      "N:239, Shape:(9198, 239)\n",
      "N:241, Shape:(9198, 241)\n",
      "Time spend:3.8 min. left comp: 96 left time:91.19999999999999\n",
      "N:307, Shape:(9222, 307)\n",
      "N:206, Shape:(9222, 206)\n",
      "Time spend:5.133333333333334 min. left comp: 95 left time:97.53333333333335\n",
      "N:235, Shape:(9222, 235)\n",
      "N:239, Shape:(9222, 239)\n",
      "N:241, Shape:(9222, 241)\n",
      "Time spend:5.666666666666667 min. left comp: 94 left time:88.77777777777779\n",
      "N:307, Shape:(9220, 307)\n",
      "N:206, Shape:(9220, 206)\n",
      "Time spend:7.05 min. left comp: 93 left time:93.66428571428571\n",
      "N:235, Shape:(9220, 235)\n",
      "N:239, Shape:(9220, 239)\n",
      "N:241, Shape:(9220, 241)\n",
      "Time spend:7.583333333333333 min. left comp: 92 left time:87.20833333333333\n",
      "N:307, Shape:(9210, 307)\n",
      "N:206, Shape:(9210, 206)\n",
      "Time spend:8.933333333333334 min. left comp: 91 left time:90.32592592592593\n",
      "N:235, Shape:(9210, 235)\n",
      "N:239, Shape:(9210, 239)\n",
      "N:241, Shape:(9210, 241)\n",
      "Time spend:9.483333333333333 min. left comp: 90 left time:85.35\n",
      "N:307, Shape:(9196, 307)\n",
      "N:206, Shape:(9196, 206)\n",
      "Time spend:10.866666666666667 min. left comp: 89 left time:87.92121212121212\n",
      "N:235, Shape:(9196, 235)\n",
      "N:239, Shape:(9196, 239)\n",
      "N:241, Shape:(9196, 241)\n",
      "Time spend:11.416666666666666 min. left comp: 88 left time:83.72222222222221\n",
      "N:307, Shape:(9214, 307)\n",
      "N:206, Shape:(9214, 206)\n",
      "Time spend:12.8 min. left comp: 87 left time:85.66153846153847\n",
      "N:235, Shape:(9214, 235)\n",
      "N:239, Shape:(9214, 239)\n",
      "N:241, Shape:(9214, 241)\n",
      "Time spend:13.333333333333334 min. left comp: 86 left time:81.90476190476191\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:14.7 min. left comp: 85 left time:83.3\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:15.233333333333333 min. left comp: 84 left time:79.975\n",
      "N:307, Shape:(9204, 307)\n",
      "N:206, Shape:(9204, 206)\n",
      "Time spend:16.616666666666667 min. left comp: 83 left time:81.12843137254903\n",
      "N:235, Shape:(9204, 235)\n",
      "N:239, Shape:(9204, 239)\n",
      "N:241, Shape:(9204, 241)\n",
      "Time spend:17.166666666666668 min. left comp: 82 left time:78.20370370370371\n",
      "N:307, Shape:(9196, 307)\n",
      "N:206, Shape:(9196, 206)\n",
      "Time spend:18.55 min. left comp: 81 left time:79.08157894736843\n",
      "N:235, Shape:(9196, 235)\n",
      "N:239, Shape:(9196, 239)\n",
      "N:241, Shape:(9196, 241)\n",
      "Time spend:19.1 min. left comp: 80 left time:76.4\n",
      "N:307, Shape:(9208, 307)\n",
      "N:206, Shape:(9208, 206)\n",
      "Time spend:20.633333333333333 min. left comp: 79 left time:77.62063492063491\n",
      "N:235, Shape:(9208, 235)\n",
      "N:239, Shape:(9208, 239)\n",
      "N:241, Shape:(9208, 241)\n",
      "Time spend:21.216666666666665 min. left comp: 78 left time:75.22272727272727\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:22.6 min. left comp: 77 left time:75.6608695652174\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:23.2 min. left comp: 76 left time:73.46666666666667\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:24.716666666666665 min. left comp: 75 left time:74.14999999999999\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:25.333333333333332 min. left comp: 74 left time:72.1025641025641\n",
      "N:307, Shape:(9190, 307)\n",
      "N:206, Shape:(9190, 206)\n",
      "Time spend:26.833333333333332 min. left comp: 73 left time:72.54938271604938\n",
      "N:235, Shape:(9190, 235)\n",
      "N:239, Shape:(9190, 239)\n",
      "N:241, Shape:(9190, 241)\n",
      "Time spend:27.383333333333333 min. left comp: 72 left time:70.41428571428571\n",
      "N:307, Shape:(9216, 307)\n",
      "N:206, Shape:(9216, 206)\n",
      "Time spend:28.883333333333333 min. left comp: 71 left time:70.71436781609195\n",
      "N:235, Shape:(9216, 235)\n",
      "N:239, Shape:(9216, 239)\n",
      "N:241, Shape:(9216, 241)\n",
      "Time spend:29.5 min. left comp: 70 left time:68.83333333333333\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:30.95 min. left comp: 69 left time:68.88870967741936\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:31.533333333333335 min. left comp: 68 left time:67.00833333333334\n",
      "N:307, Shape:(9230, 307)\n",
      "N:206, Shape:(9230, 206)\n",
      "Time spend:33.016666666666666 min. left comp: 67 left time:67.03383838383839\n",
      "N:235, Shape:(9230, 235)\n",
      "N:239, Shape:(9230, 239)\n",
      "N:241, Shape:(9230, 241)\n",
      "Time spend:33.6 min. left comp: 66 left time:65.2235294117647\n",
      "N:307, Shape:(9190, 307)\n",
      "N:206, Shape:(9190, 206)\n",
      "Time spend:35.13333333333333 min. left comp: 65 left time:65.24761904761904\n",
      "N:235, Shape:(9190, 235)\n",
      "N:239, Shape:(9190, 239)\n",
      "N:241, Shape:(9190, 241)\n",
      "Time spend:35.733333333333334 min. left comp: 64 left time:63.525925925925925\n",
      "N:307, Shape:(9214, 307)\n",
      "N:206, Shape:(9214, 206)\n",
      "Time spend:37.266666666666666 min. left comp: 63 left time:63.45405405405405\n",
      "N:235, Shape:(9214, 235)\n",
      "N:239, Shape:(9214, 239)\n",
      "N:241, Shape:(9214, 241)\n",
      "Time spend:37.85 min. left comp: 62 left time:61.75526315789474\n",
      "N:307, Shape:(9224, 307)\n",
      "N:206, Shape:(9224, 206)\n",
      "Time spend:39.28333333333333 min. left comp: 61 left time:61.44316239316239\n",
      "N:235, Shape:(9224, 235)\n",
      "N:239, Shape:(9224, 239)\n",
      "N:241, Shape:(9224, 241)\n",
      "Time spend:39.85 min. left comp: 60 left time:59.775000000000006\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:41.25 min. left comp: 59 left time:59.359756097560975\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:41.833333333333336 min. left comp: 58 left time:57.76984126984127\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:43.2 min. left comp: 57 left time:57.26511627906977\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:43.78333333333333 min. left comp: 56 left time:55.72424242424242\n",
      "N:307, Shape:(9212, 307)\n",
      "N:206, Shape:(9212, 206)\n",
      "Time spend:45.21666666666667 min. left comp: 55 left time:55.26481481481481\n",
      "N:235, Shape:(9212, 235)\n",
      "N:239, Shape:(9212, 239)\n",
      "N:241, Shape:(9212, 241)\n",
      "Time spend:45.8 min. left comp: 54 left time:53.76521739130434\n",
      "N:307, Shape:(9204, 307)\n",
      "N:206, Shape:(9204, 206)\n",
      "Time spend:47.18333333333333 min. left comp: 53 left time:53.20673758865249\n",
      "N:235, Shape:(9204, 235)\n",
      "N:239, Shape:(9204, 239)\n",
      "N:241, Shape:(9204, 241)\n",
      "Time spend:47.78333333333333 min. left comp: 52 left time:51.765277777777776\n",
      "N:307, Shape:(9210, 307)\n",
      "N:206, Shape:(9210, 206)\n",
      "Time spend:49.3 min. left comp: 51 left time:51.312244897959175\n",
      "N:235, Shape:(9210, 235)\n",
      "N:239, Shape:(9210, 239)\n",
      "N:241, Shape:(9210, 241)\n",
      "Time spend:49.95 min. left comp: 50 left time:49.95\n",
      "N:307, Shape:(9204, 307)\n",
      "N:206, Shape:(9204, 206)\n",
      "Time spend:51.45 min. left comp: 49 left time:49.43235294117647\n",
      "N:235, Shape:(9204, 235)\n",
      "N:239, Shape:(9204, 239)\n",
      "N:241, Shape:(9204, 241)\n",
      "Time spend:52.05 min. left comp: 48 left time:48.04615384615384\n",
      "N:307, Shape:(9196, 307)\n",
      "N:206, Shape:(9196, 206)\n",
      "Time spend:53.483333333333334 min. left comp: 47 left time:47.42861635220125\n",
      "N:235, Shape:(9196, 235)\n",
      "N:239, Shape:(9196, 239)\n",
      "N:241, Shape:(9196, 241)\n",
      "Time spend:54.05 min. left comp: 46 left time:46.042592592592584\n",
      "N:307, Shape:(9220, 307)\n",
      "N:206, Shape:(9220, 206)\n",
      "Time spend:55.45 min. left comp: 45 left time:45.368181818181824\n",
      "N:235, Shape:(9220, 235)\n",
      "N:239, Shape:(9220, 239)\n",
      "N:241, Shape:(9220, 241)\n",
      "Time spend:56.03333333333333 min. left comp: 44 left time:44.02619047619048\n",
      "N:307, Shape:(9224, 307)\n",
      "N:206, Shape:(9224, 206)\n",
      "Time spend:57.46666666666667 min. left comp: 43 left time:43.35204678362574\n",
      "N:235, Shape:(9224, 235)\n",
      "N:239, Shape:(9224, 239)\n",
      "N:241, Shape:(9224, 241)\n",
      "Time spend:58.03333333333333 min. left comp: 42 left time:42.02413793103448\n",
      "N:307, Shape:(9210, 307)\n",
      "N:206, Shape:(9210, 206)\n",
      "Time spend:59.46666666666667 min. left comp: 41 left time:41.32429378531073\n",
      "N:235, Shape:(9210, 235)\n",
      "N:239, Shape:(9210, 239)\n",
      "N:241, Shape:(9210, 241)\n",
      "Time spend:60.05 min. left comp: 40 left time:40.03333333333333\n",
      "N:307, Shape:(9200, 307)\n",
      "N:206, Shape:(9200, 206)\n",
      "Time spend:61.55 min. left comp: 39 left time:39.35163934426229\n",
      "N:235, Shape:(9200, 235)\n",
      "N:239, Shape:(9200, 239)\n",
      "N:241, Shape:(9200, 241)\n",
      "Time spend:62.13333333333333 min. left comp: 38 left time:38.08172043010753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:307, Shape:(9216, 307)\n",
      "N:206, Shape:(9216, 206)\n",
      "Time spend:63.583333333333336 min. left comp: 37 left time:37.342592592592595\n",
      "N:235, Shape:(9216, 235)\n",
      "N:239, Shape:(9216, 239)\n",
      "N:241, Shape:(9216, 241)\n",
      "Time spend:64.15 min. left comp: 36 left time:36.084375\n",
      "N:307, Shape:(9222, 307)\n",
      "N:206, Shape:(9222, 206)\n",
      "Time spend:65.63333333333334 min. left comp: 35 left time:35.341025641025645\n",
      "N:235, Shape:(9222, 235)\n",
      "N:239, Shape:(9222, 239)\n",
      "N:241, Shape:(9222, 241)\n",
      "Time spend:66.2 min. left comp: 34 left time:34.10303030303031\n",
      "N:307, Shape:(9212, 307)\n",
      "N:206, Shape:(9212, 206)\n",
      "Time spend:67.56666666666666 min. left comp: 33 left time:33.27910447761194\n",
      "N:235, Shape:(9212, 235)\n",
      "N:239, Shape:(9212, 239)\n",
      "N:241, Shape:(9212, 241)\n",
      "Time spend:68.15 min. left comp: 32 left time:32.07058823529412\n",
      "N:307, Shape:(9210, 307)\n",
      "N:206, Shape:(9210, 206)\n",
      "Time spend:69.56666666666666 min. left comp: 31 left time:31.254589371980675\n",
      "N:235, Shape:(9210, 235)\n",
      "N:239, Shape:(9210, 239)\n",
      "N:241, Shape:(9210, 241)\n",
      "Time spend:70.13333333333334 min. left comp: 30 left time:30.057142857142864\n",
      "N:307, Shape:(9202, 307)\n",
      "N:206, Shape:(9202, 206)\n",
      "Time spend:71.51666666666667 min. left comp: 29 left time:29.211032863849766\n",
      "N:235, Shape:(9202, 235)\n",
      "N:239, Shape:(9202, 239)\n",
      "N:241, Shape:(9202, 241)\n",
      "Time spend:72.1 min. left comp: 28 left time:28.038888888888888\n",
      "N:307, Shape:(9218, 307)\n",
      "N:206, Shape:(9218, 206)\n",
      "Time spend:73.55 min. left comp: 27 left time:27.20342465753425\n",
      "N:235, Shape:(9218, 235)\n",
      "N:239, Shape:(9218, 239)\n",
      "N:241, Shape:(9218, 241)\n",
      "Time spend:74.13333333333334 min. left comp: 26 left time:26.046846846846847\n",
      "N:307, Shape:(9216, 307)\n",
      "N:206, Shape:(9216, 206)\n",
      "Time spend:75.56666666666666 min. left comp: 25 left time:25.18888888888889\n",
      "N:235, Shape:(9216, 235)\n",
      "N:239, Shape:(9216, 239)\n",
      "N:241, Shape:(9216, 241)\n",
      "Time spend:76.13333333333334 min. left comp: 24 left time:24.042105263157893\n",
      "N:307, Shape:(9200, 307)\n",
      "N:206, Shape:(9200, 206)\n",
      "Time spend:77.6 min. left comp: 23 left time:23.179220779220778\n",
      "N:235, Shape:(9200, 235)\n",
      "N:239, Shape:(9200, 239)\n",
      "N:241, Shape:(9200, 241)\n",
      "Time spend:78.2 min. left comp: 22 left time:22.056410256410256\n",
      "N:307, Shape:(9206, 307)\n",
      "N:206, Shape:(9206, 206)\n",
      "Time spend:79.63333333333334 min. left comp: 21 left time:21.16835443037975\n",
      "N:235, Shape:(9206, 235)\n",
      "N:239, Shape:(9206, 239)\n",
      "N:241, Shape:(9206, 241)\n",
      "Time spend:80.21666666666667 min. left comp: 20 left time:20.054166666666667\n",
      "N:307, Shape:(9194, 307)\n",
      "N:206, Shape:(9194, 206)\n",
      "Time spend:81.7 min. left comp: 19 left time:19.164197530864197\n",
      "N:235, Shape:(9194, 235)\n",
      "N:239, Shape:(9194, 239)\n",
      "N:241, Shape:(9194, 241)\n",
      "Time spend:82.28333333333333 min. left comp: 18 left time:18.06219512195122\n",
      "N:307, Shape:(9194, 307)\n",
      "N:206, Shape:(9194, 206)\n",
      "Time spend:83.66666666666667 min. left comp: 17 left time:17.136546184738958\n",
      "N:235, Shape:(9194, 235)\n",
      "N:239, Shape:(9194, 239)\n",
      "N:241, Shape:(9194, 241)\n",
      "Time spend:84.26666666666667 min. left comp: 16 left time:16.05079365079365\n",
      "N:307, Shape:(9218, 307)\n",
      "N:206, Shape:(9218, 206)\n",
      "Time spend:85.73333333333333 min. left comp: 15 left time:15.129411764705882\n",
      "N:235, Shape:(9218, 235)\n",
      "N:239, Shape:(9218, 239)\n",
      "N:241, Shape:(9218, 241)\n",
      "Time spend:86.33333333333333 min. left comp: 14 left time:14.054263565891473\n",
      "N:307, Shape:(9204, 307)\n",
      "N:206, Shape:(9204, 206)\n",
      "Time spend:87.8 min. left comp: 13 left time:13.119540229885057\n",
      "N:235, Shape:(9204, 235)\n",
      "N:239, Shape:(9204, 239)\n",
      "N:241, Shape:(9204, 241)\n",
      "Time spend:88.38333333333334 min. left comp: 12 left time:12.052272727272728\n",
      "N:307, Shape:(9222, 307)\n",
      "N:206, Shape:(9222, 206)\n",
      "Time spend:89.88333333333334 min. left comp: 11 left time:11.109176029962548\n",
      "N:235, Shape:(9222, 235)\n",
      "N:239, Shape:(9222, 239)\n",
      "N:241, Shape:(9222, 241)\n",
      "Time spend:90.46666666666667 min. left comp: 10 left time:10.051851851851852\n",
      "N:307, Shape:(9198, 307)\n",
      "N:206, Shape:(9198, 206)\n",
      "Time spend:91.98333333333333 min. left comp: 9 left time:9.097252747252746\n",
      "N:235, Shape:(9198, 235)\n",
      "N:239, Shape:(9198, 239)\n",
      "N:241, Shape:(9198, 241)\n",
      "Time spend:92.61666666666666 min. left comp: 8 left time:8.053623188405796\n",
      "N:307, Shape:(9224, 307)\n",
      "N:206, Shape:(9224, 206)\n",
      "Time spend:94.1 min. left comp: 7 left time:7.0827956989247305\n",
      "N:235, Shape:(9224, 235)\n",
      "N:239, Shape:(9224, 239)\n",
      "N:241, Shape:(9224, 241)\n",
      "Time spend:94.7 min. left comp: 6 left time:6.04468085106383\n",
      "N:307, Shape:(9204, 307)\n",
      "N:206, Shape:(9204, 206)\n",
      "Time spend:96.15 min. left comp: 5 left time:5.060526315789474\n",
      "N:235, Shape:(9204, 235)\n",
      "N:239, Shape:(9204, 239)\n",
      "N:241, Shape:(9204, 241)\n",
      "Time spend:96.75 min. left comp: 4 left time:4.03125\n",
      "N:307, Shape:(9198, 307)\n",
      "N:206, Shape:(9198, 206)\n",
      "Time spend:98.2 min. left comp: 3 left time:3.0371134020618555\n",
      "N:235, Shape:(9198, 235)\n",
      "N:239, Shape:(9198, 239)\n",
      "N:241, Shape:(9198, 241)\n",
      "Time spend:98.81666666666666 min. left comp: 2 left time:2.0166666666666666\n",
      "N:307, Shape:(9204, 307)\n",
      "N:206, Shape:(9204, 206)\n",
      "Time spend:100.3 min. left comp: 1 left time:1.013131313131313\n",
      "N:235, Shape:(9204, 235)\n",
      "N:239, Shape:(9204, 239)\n",
      "N:241, Shape:(9204, 241)\n",
      "Time spend:100.91666666666667 min. left comp: 0 left time:0.0\n",
      "Model: 0 param:Obj:multi:softprob LR:0.078 N_EST:490 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
      "n:307 Accuracy evaluation : 94.659 Accuracy train : 99.775\n",
      "n:206 Accuracy evaluation : 94.668 Accuracy train : 99.704\n",
      "Model: 1 param:Obj:multi:softprob LR:0.079 N_EST:400 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
      "n:241 Accuracy evaluation : 94.653 Accuracy train : 99.442\n",
      "n:239 Accuracy evaluation : 94.624 Accuracy train : 99.436\n",
      "n:235 Accuracy evaluation : 94.633 Accuracy train : 99.430\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------#\n",
    "# Very expensive in computations with all parameters\n",
    "# now while i know the best parameters we need to run only 2 models comparison\n",
    "# need to implement ROC plot for each model\n",
    "# ----------------------------------------------------------------#\n",
    "# definition of multiple parameters, we run combination of them in order to find best one\n",
    "\n",
    "\"\"\"\n",
    "#old values \n",
    "objective = [\"multi:softprob\"]  # ,\"multi:softmax\"]\n",
    "learning_rate = [0.078, 0.078, 0.078, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079]\n",
    "n_estimators = [430, 440, 490, 400, 400, 410, 440, 450, 450, 480, 490, 480]\n",
    "max_depth = [4, 3, 4, 3, 3, 4, 3, 4, 4, 4, 4, 3]\n",
    "colsample_bytree = [0.35, 0.35, 0.5, 0.3, 0.45, 0.3, 0.45, 0.35, 0.45, 0.45, 0.45, 0.35]\n",
    "# -----------------------------\n",
    "# Model: 0 param:Obj:multi:softprob LR:0.078 N_EST:490 MAX_DEPTH:4 ByTREE:0.5 EvalTree:['aucpr']\n",
    "# best 95 on 319 features\n",
    "# best 94.9 on 317, 309 , 282, 260, 258, 255, 230, 198,197 , 191, 187, 179, 167, 147,\n",
    "# Model: 1 param:Obj:multi:softprob LR:0.079 N_EST:400 MAX_DEPTH:4 ByTREE:0.45 EvalTree:['aucpr']\n",
    "# best 94.9 on 194, 189\n",
    "objective = [\"multi:softprob\"]\n",
    "learning_rate = [0.078, 0.079]\n",
    "n_estimators = [490, 400]\n",
    "max_depth = [4, 4]\n",
    "colsample_bytree = [0.5, 0.45]\n",
    "eval_metric = [\"aucpr\"]\n",
    "\n",
    "\n",
    "10 iterations\n",
    "Model: 0 param:Obj:multi:softprob LR:0.078 N_EST:490 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:319 Accuracy evaluation : 94.487 Accuracy train : 99.736\n",
    "n:317 Accuracy evaluation : 94.641 Accuracy train : 99.749\n",
    "n:309 Accuracy evaluation : 94.612 Accuracy train : 99.746\n",
    "n:282 Accuracy evaluation : 94.610 Accuracy train : 99.749\n",
    "n:260 Accuracy evaluation : 94.587 Accuracy train : 99.739\n",
    "n:258 Accuracy evaluation : 94.594 Accuracy train : 99.747\n",
    "n:255 Accuracy evaluation : 94.618 Accuracy train : 99.740  <- best\n",
    "n:230 Accuracy evaluation : 94.597 Accuracy train : 99.721\n",
    "n:198 Accuracy evaluation : 94.571 Accuracy train : 99.678\n",
    "n:197 Accuracy evaluation : 94.590 Accuracy train : 99.675\n",
    "n:194 Accuracy evaluation : 94.562 Accuracy train : 99.672\n",
    "n:191 Accuracy evaluation : 94.619 Accuracy train : 99.665  <- best\n",
    "n:189 Accuracy evaluation : 94.590 Accuracy train : 99.651\n",
    "n:187 Accuracy evaluation : 94.554 Accuracy train : 99.651\n",
    "n:179 Accuracy evaluation : 94.566 Accuracy train : 99.627\n",
    "n:167 Accuracy evaluation : 94.584 Accuracy train : 99.595\n",
    "n:147 Accuracy evaluation : 94.539 Accuracy train : 99.509\n",
    "Model: 1 param:Obj:multi:softprob LR:0.079 N_EST:400 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:319 Accuracy evaluation : 94.390 Accuracy train : 99.437\n",
    "n:317 Accuracy evaluation : 94.523 Accuracy train : 99.430\n",
    "n:309 Accuracy evaluation : 94.559 Accuracy train : 99.425\n",
    "n:282 Accuracy evaluation : 94.589 Accuracy train : 99.425\n",
    "n:260 Accuracy evaluation : 94.592 Accuracy train : 99.414\n",
    "n:258 Accuracy evaluation : 94.620 Accuracy train : 99.423  <- best\n",
    "n:255 Accuracy evaluation : 94.583 Accuracy train : 99.424\n",
    "n:230 Accuracy evaluation : 94.570 Accuracy train : 99.397\n",
    "n:198 Accuracy evaluation : 94.566 Accuracy train : 99.325\n",
    "n:197 Accuracy evaluation : 94.537 Accuracy train : 99.321\n",
    "n:194 Accuracy evaluation : 94.561 Accuracy train : 99.314\n",
    "n:191 Accuracy evaluation : 94.567 Accuracy train : 99.312\n",
    "n:189 Accuracy evaluation : 94.563 Accuracy train : 99.303\n",
    "n:187 Accuracy evaluation : 94.577 Accuracy train : 99.289\n",
    "n:179 Accuracy evaluation : 94.571 Accuracy train : 99.251\n",
    "n:167 Accuracy evaluation : 94.557 Accuracy train : 99.201\n",
    "n:147 Accuracy evaluation : 94.527 Accuracy train : 99.085\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "20 iterations \n",
    "Model: 0 param:Obj:multi:softprob LR:0.078 N_EST:490 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:258 Accuracy evaluation : 94.567 Accuracy train : 99.747\n",
    "n:255 Accuracy evaluation : 94.575 Accuracy train : 99.746\n",
    "n:191 Accuracy evaluation : 94.584 Accuracy train : 99.653\n",
    "Model: 1 param:Obj:multi:softprob LR:0.079 N_EST:400 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:258 Accuracy evaluation : 94.574 Accuracy train : 99.429\n",
    "n:255 Accuracy evaluation : 94.559 Accuracy train : 99.432\n",
    "n:191 Accuracy evaluation : 94.594 Accuracy train : 99.293\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "Model: 0 param:Obj:multi:softprob LR:0.078 N_EST:490 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:307 Accuracy evaluation : 94.682 Accuracy train : 99.735\n",
    "n:206 Accuracy evaluation : 94.661 Accuracy train : 99.687\n",
    "Model: 1 param:Obj:multi:softprob LR:0.079 N_EST:400 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:239 Accuracy evaluation : 94.665 Accuracy train : 99.418\n",
    "n:241 Accuracy evaluation : 94.700 Accuracy train : 99.434\n",
    "n:235 Accuracy evaluation : 94.674 Accuracy train : 99.413\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "Model: 0 param:Obj:multi:softprob LR:0.078 N_EST:490 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:307 Accuracy evaluation : 94.659 Accuracy train : 99.775\n",
    "n:206 Accuracy evaluation : 94.668 Accuracy train : 99.704\n",
    "Model: 1 param:Obj:multi:softprob LR:0.079 N_EST:400 MAX_DEPTH:4 ByTREE:[0.5, 0.45] EvalTree:aucpr\n",
    "n:241 Accuracy evaluation : 94.653 Accuracy train : 99.442\n",
    "n:239 Accuracy evaluation : 94.624 Accuracy train : 99.436\n",
    "n:235 Accuracy evaluation : 94.633 Accuracy train : 99.430\n",
    "\"\"\"\n",
    "\n",
    "#store model paramters for printing reason.\n",
    "#Used in order to identify model parameters and resulting values\n",
    "model_param = []\n",
    "#list of ech model scores (test, train)\n",
    "model_scores = []\n",
    "#fill model params and scores\n",
    "for i in range(len(colsample_bytree)):\n",
    "    model_scores.append(defaultdict(lambda: {\"test\": [], \"train\": []}))\n",
    "    model_param.append(\"Obj:{} LR:{} N_EST:{} MAX_DEPTH:{} ByTREE:{} EvalTree:{}\".format(\n",
    "        objective[0], learning_rate[i], n_estimators[i], max_depth[i], colsample_bytree, eval_metric[0]))\n",
    "\n",
    "#internal counter of computer iterations \n",
    "c = 0\n",
    "#define file name of csv dataset\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "#define number of iterations\n",
    "number_of_iter = 10\n",
    "\n",
    "#define number of features that used in computations \n",
    "features = [206,235,241,307]\n",
    "\n",
    "#starting time of computations, used in order to compute remaining time.\n",
    "start_time = datetime.now()\n",
    "\n",
    "#compute number of computations that should be executed, used for computation of remaining time and computations.\n",
    "all_comp = len(model_scores) * number_of_iter * 5\n",
    "\n",
    "# iterate on data with multiple monte-carlo folds \n",
    "# at each we re-read data and shuffle them in random order\n",
    "# after split the shuffled data into train/evaluation and test\n",
    "# train/evaluation are used by k-fold in order to train model and get accuracy.\n",
    "# Since we utilize only train/evaluation data portion, we scale those data and perform oversample of them in order to balance classes\n",
    "# Test portion of data are not used in multiple folds, this data portion are used at the end of process when we have single model\n",
    "# Test data are not oversampled, we only scale them in order to have similar vector values.\n",
    "for fold in range(number_of_iter):\n",
    "    # split entire dataset into 2 parts:\n",
    "    #                    1st with all data used in training/evaluation(80%)\n",
    "    #                    2nd with data used in final testing (20%)\n",
    "    df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "\n",
    "    #define kfold\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(df, target):\n",
    "        #train_index and test_index provide indexes of df that shoud be used \n",
    "        #in order to select train ant evaluation portions\n",
    "        X_train, X_eval = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_eval = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "        # scale data separatly\n",
    "        X_train, X_eval, df_test_scaled = scale_data(X_train, X_eval, df_test)\n",
    "\n",
    "        # oversamplong clean on training set\n",
    "        X_train, y_train = oversample(X_train, y_train)\n",
    "        # oversampling clean on evaluation set\n",
    "        X_eval, y_eval = oversample(X_eval, y_eval)\n",
    "\n",
    "        #for each k-fold folds select our model parameters in iterative way\n",
    "        for i in range(len(colsample_bytree)):\n",
    "            obj = objective[0]\n",
    "            lr = learning_rate[i]\n",
    "            n_est = n_estimators[i]\n",
    "            mx_d = max_depth[i]\n",
    "            col_b = colsample_bytree[i]\n",
    "            ev_metric = eval_metric[0]\n",
    "\n",
    "            #train model with all features in order to select N best features based in training set \n",
    "            model = XGBClassifier(objective=obj,\n",
    "                                  num_class=2,\n",
    "                                  learning_rate=lr,\n",
    "                                  n_estimators=n_est,\n",
    "                                  max_depth=mx_d,\n",
    "                                  colsample_bytree=col_b,\n",
    "                                  eval_metric=ev_metric,  # \"rmse\",\n",
    "                                  use_label_encoder=False)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            #case of 2 last models when we know best range of features, select only them\n",
    "            #for t in thresholds:\n",
    "            #if i == 0:\n",
    "            #    features = [307,206]\n",
    "            #else:\n",
    "            #    features = [235,239,241]\n",
    "            \n",
    "            #train and evaluate model for all features range starting from 2 till end,\n",
    "            # by growing number of top features\n",
    "            for n in features:\n",
    "                # select N top features from pre-trained model\n",
    "                selection = SelectFromModel(model, threshold=-np.inf, max_features=n,prefit=True)\n",
    "                \n",
    "                #transform datasets based on selected features\n",
    "                select_X_train = selection.transform(X_train)\n",
    "                select_X_eval = selection.transform(X_eval)\n",
    "                \n",
    "                #print(\"N:{}, Shape:{}\".format(n, select_X_train.shape))\n",
    "                \n",
    "                #if np.sum(selection.get_support()) not in [191,255,258]:\n",
    "                #    continue\n",
    "                #print(df.columns[selection.get_support()])\n",
    "                #break\n",
    "                \n",
    "                #Create and train new model that know onyl those N features\n",
    "                selection_model = XGBClassifier(objective=obj,\n",
    "                                                num_class=2,\n",
    "                                                learning_rate=lr,\n",
    "                                                n_estimators=n_est,\n",
    "                                                max_depth=mx_d,\n",
    "                                                colsample_bytree=col_b,\n",
    "                                                eval_metric=ev_metric,  # \"rmse\",\n",
    "                                                use_label_encoder=False)\n",
    "                selection_model.fit(select_X_train, y_train)\n",
    "                \n",
    "                # evaluate model, by calculating acuracy of train and evalutation data portion\n",
    "                accuracy_eval = accuracy_score(y_eval, selection_model.predict(select_X_eval))\n",
    "                accuracy_train = accuracy_score(y_train, selection_model.predict(select_X_train))\n",
    "                \n",
    "                #store acuracy values in model_scores list\n",
    "                #          I: present model number\n",
    "                #          select_X_train.shape[1] is same with N : show number of selected features\n",
    "                model_scores[i][select_X_train.shape[1]][\"test\"].append(accuracy_eval * 100.0)\n",
    "                model_scores[i][select_X_train.shape[1]][\"train\"].append(accuracy_train * 100.0)\n",
    "            \n",
    "            #increase count value since we finish with single iteration of k-fold of particular model\n",
    "            c += 1\n",
    "\n",
    "            #calculate time that is spend till now in minutes\n",
    "            spend_time = (datetime.now() - start_time).seconds / 60.0\n",
    "            #print time that was spend, number of comutation left and time that is left till the end\n",
    "            print(\"Time spend:{} min. left comp: {} left time:{}\".format(spend_time, all_comp - c,\n",
    "                                                                 (spend_time / c) * (all_comp - c)))\n",
    "\n",
    "\n",
    "#for each model in our experiment print parameters of model and for each number of features \n",
    "# print accuracy of evaluation and train (AVERAGE values)\n",
    "for i in range(len(model_scores)):\n",
    "    print(\"Model: {} param:{}\".format(i, model_param[i]))\n",
    "    n = list(model_scores[i].keys())\n",
    "    n.sort(reverse=True)\n",
    "    for sc in n:\n",
    "        print(\"n:{} Accuracy evaluation : {:.3f} Accuracy train : {:.3f}\"\n",
    "              .format(sc,\n",
    "                      sum(model_scores[i][sc][\"test\"]) / len(model_scores[i][sc][\"test\"]),\n",
    "                      sum(model_scores[i][sc][\"train\"]) / len(model_scores[i][sc][\"train\"]),))                      ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest fine tune and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spend:1.5666666666666667 min. left comp: 99 left time:155.1\n",
      "Time spend:3.15 min. left comp: 98 left time:154.35\n",
      "Time spend:4.7 min. left comp: 97 left time:151.96666666666667\n",
      "Time spend:6.266666666666667 min. left comp: 96 left time:150.4\n",
      "Time spend:7.816666666666666 min. left comp: 95 left time:148.51666666666665\n",
      "Time spend:1.65 min. left comp: 94 left time:25.849999999999998\n",
      "Time spend:3.316666666666667 min. left comp: 93 left time:44.06428571428572\n",
      "Time spend:4.966666666666667 min. left comp: 92 left time:57.11666666666667\n",
      "Time spend:6.683333333333334 min. left comp: 91 left time:67.57592592592593\n",
      "Time spend:8.35 min. left comp: 90 left time:75.14999999999999\n",
      "Time spend:1.5833333333333333 min. left comp: 89 left time:12.810606060606059\n",
      "Time spend:3.183333333333333 min. left comp: 88 left time:23.344444444444445\n",
      "Time spend:4.816666666666666 min. left comp: 87 left time:32.23461538461538\n",
      "Time spend:6.45 min. left comp: 86 left time:39.621428571428574\n",
      "Time spend:8.0 min. left comp: 85 left time:45.333333333333336\n",
      "Time spend:1.6166666666666667 min. left comp: 84 left time:8.4875\n",
      "Time spend:3.2333333333333334 min. left comp: 83 left time:15.78627450980392\n",
      "Time spend:5.0 min. left comp: 82 left time:22.77777777777778\n",
      "Time spend:6.733333333333333 min. left comp: 81 left time:28.705263157894738\n",
      "Time spend:8.4 min. left comp: 80 left time:33.6\n",
      "Time spend:1.4333333333333333 min. left comp: 79 left time:5.3920634920634924\n",
      "Time spend:2.8833333333333333 min. left comp: 78 left time:10.222727272727274\n",
      "Time spend:4.3 min. left comp: 77 left time:14.395652173913044\n",
      "Time spend:5.866666666666666 min. left comp: 76 left time:18.577777777777776\n",
      "Time spend:7.55 min. left comp: 75 left time:22.65\n",
      "Time spend:1.6666666666666667 min. left comp: 74 left time:4.7435897435897445\n",
      "Time spend:3.316666666666667 min. left comp: 73 left time:8.967283950617285\n",
      "Time spend:4.983333333333333 min. left comp: 72 left time:12.814285714285715\n",
      "Time spend:6.65 min. left comp: 71 left time:16.28103448275862\n",
      "Time spend:8.3 min. left comp: 70 left time:19.366666666666667\n",
      "Time spend:1.5833333333333333 min. left comp: 69 left time:3.5241935483870965\n",
      "Time spend:3.1666666666666665 min. left comp: 68 left time:6.729166666666666\n",
      "Time spend:4.783333333333333 min. left comp: 67 left time:9.711616161616162\n",
      "Time spend:6.483333333333333 min. left comp: 66 left time:12.585294117647058\n",
      "Time spend:8.166666666666666 min. left comp: 65 left time:15.166666666666664\n",
      "Time spend:1.7833333333333334 min. left comp: 64 left time:3.1703703703703705\n",
      "Time spend:3.5166666666666666 min. left comp: 63 left time:5.987837837837838\n",
      "Time spend:5.25 min. left comp: 62 left time:8.56578947368421\n",
      "Time spend:6.916666666666667 min. left comp: 61 left time:10.818376068376068\n",
      "Time spend:8.433333333333334 min. left comp: 60 left time:12.65\n",
      "Time spend:1.7333333333333334 min. left comp: 59 left time:2.4943089430894307\n",
      "Time spend:3.3833333333333333 min. left comp: 58 left time:4.6722222222222225\n",
      "Time spend:5.083333333333333 min. left comp: 57 left time:6.738372093023255\n",
      "Time spend:6.733333333333333 min. left comp: 56 left time:8.56969696969697\n",
      "Time spend:8.433333333333334 min. left comp: 55 left time:10.307407407407407\n",
      "Time spend:1.6833333333333333 min. left comp: 54 left time:1.9760869565217392\n",
      "Time spend:3.533333333333333 min. left comp: 53 left time:3.9843971631205672\n",
      "Time spend:5.416666666666667 min. left comp: 52 left time:5.868055555555555\n",
      "Time spend:7.266666666666667 min. left comp: 51 left time:7.563265306122449\n",
      "Time spend:9.066666666666666 min. left comp: 50 left time:9.066666666666666\n",
      "Time spend:1.6666666666666667 min. left comp: 49 left time:1.6013071895424837\n",
      "Time spend:3.4 min. left comp: 48 left time:3.138461538461539\n",
      "Time spend:5.1 min. left comp: 47 left time:4.522641509433962\n",
      "Time spend:6.8 min. left comp: 46 left time:5.792592592592592\n",
      "Time spend:8.483333333333333 min. left comp: 45 left time:6.9409090909090905\n",
      "Time spend:1.6833333333333333 min. left comp: 44 left time:1.3226190476190476\n",
      "Time spend:3.4 min. left comp: 43 left time:2.5649122807017544\n",
      "Time spend:5.1 min. left comp: 42 left time:3.693103448275862\n",
      "Time spend:6.8 min. left comp: 41 left time:4.725423728813559\n",
      "Time spend:8.466666666666667 min. left comp: 40 left time:5.644444444444444\n",
      "Time spend:1.65 min. left comp: 39 left time:1.0549180327868852\n",
      "Time spend:3.3333333333333335 min. left comp: 38 left time:2.043010752688172\n",
      "Time spend:5.083333333333333 min. left comp: 37 left time:2.9854497354497354\n",
      "Time spend:6.733333333333333 min. left comp: 36 left time:3.7875\n",
      "Time spend:8.383333333333333 min. left comp: 35 left time:4.514102564102564\n",
      "Time spend:1.5833333333333333 min. left comp: 34 left time:0.8156565656565656\n",
      "Time spend:3.183333333333333 min. left comp: 33 left time:1.567910447761194\n",
      "Time spend:4.783333333333333 min. left comp: 32 left time:2.2509803921568627\n",
      "Time spend:6.533333333333333 min. left comp: 31 left time:2.9352657004830918\n",
      "Time spend:8.316666666666666 min. left comp: 30 left time:3.564285714285714\n",
      "Time spend:1.6333333333333333 min. left comp: 29 left time:0.6671361502347417\n",
      "Time spend:3.3 min. left comp: 28 left time:1.2833333333333332\n",
      "Time spend:5.0 min. left comp: 27 left time:1.8493150684931505\n",
      "Time spend:6.683333333333334 min. left comp: 26 left time:2.3481981981981983\n",
      "Time spend:8.35 min. left comp: 25 left time:2.783333333333333\n",
      "Time spend:1.7 min. left comp: 24 left time:0.5368421052631579\n",
      "Time spend:3.45 min. left comp: 23 left time:1.0305194805194806\n",
      "Time spend:5.033333333333333 min. left comp: 22 left time:1.4196581196581195\n",
      "Time spend:6.616666666666666 min. left comp: 21 left time:1.7588607594936707\n",
      "Time spend:8.183333333333334 min. left comp: 20 left time:2.0458333333333334\n",
      "Time spend:1.6166666666666667 min. left comp: 19 left time:0.3792181069958848\n",
      "Time spend:3.433333333333333 min. left comp: 18 left time:0.7536585365853659\n",
      "Time spend:5.166666666666667 min. left comp: 17 left time:1.0582329317269077\n",
      "Time spend:6.7 min. left comp: 16 left time:1.2761904761904763\n",
      "Time spend:8.233333333333333 min. left comp: 15 left time:1.452941176470588\n",
      "Time spend:1.5833333333333333 min. left comp: 14 left time:0.2577519379844961\n",
      "Time spend:3.25 min. left comp: 13 left time:0.485632183908046\n",
      "Time spend:4.966666666666667 min. left comp: 12 left time:0.6772727272727272\n",
      "Time spend:6.666666666666667 min. left comp: 11 left time:0.8239700374531835\n",
      "Time spend:8.366666666666667 min. left comp: 10 left time:0.9296296296296297\n",
      "Time spend:1.7 min. left comp: 9 left time:0.16813186813186812\n",
      "Time spend:3.316666666666667 min. left comp: 8 left time:0.2884057971014493\n",
      "Time spend:4.866666666666666 min. left comp: 7 left time:0.3663082437275985\n",
      "Time spend:6.5 min. left comp: 6 left time:0.4148936170212766\n",
      "Time spend:8.266666666666667 min. left comp: 5 left time:0.43508771929824563\n",
      "Time spend:1.8 min. left comp: 4 left time:0.075\n",
      "Time spend:3.566666666666667 min. left comp: 3 left time:0.11030927835051546\n",
      "Time spend:5.15 min. left comp: 2 left time:0.10510204081632654\n",
      "Time spend:6.766666666666667 min. left comp: 1 left time:0.06835016835016836\n",
      "Time spend:8.316666666666666 min. left comp: 0 left time:0.0\n",
      "Model: 0 param:N_estim:150 CC_alpha:7e-05 Min_sample_split:2\n",
      "n:134 Accuracy evaluation : 93.355 Accuracy train : 99.991\n",
      "n:132 Accuracy evaluation : 93.405 Accuracy train : 99.992\n",
      "n:105 Accuracy evaluation : 93.421 Accuracy train : 99.992\n",
      "n:102 Accuracy evaluation : 93.429 Accuracy train : 99.994\n",
      "n:101 Accuracy evaluation : 93.412 Accuracy train : 99.992\n",
      "n:90 Accuracy evaluation : 93.404 Accuracy train : 99.991\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Est:50 ccp_alpha:0.000010 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9995651701271877 \n",
    "#Est:100 ccp_alpha:0.000030 min_samples_split:3 accuracy test max:0.9426832361146336 accuracy train max:0.999891292531797 \n",
    "#Est:200 ccp_alpha:0.000070 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9989129253179694 \n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators=[50,100,150,200]\n",
    "ccp_alpha =list(np.arange(0.00001,0.00008,0.00001))\n",
    "min_samples_split=[2,3,4]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators = [200,200,200,100, 100,100,150,150,150,150,200]\n",
    "ccp_alpha = [0.00003,0.00005,0.00007,0.00001,0.00002,0.00003,0.00002,0.00005,0.00006,0.00007,0.00003]\n",
    "min_samples_split = [3,4,2,3,3,2,3,3,2,2,3]\n",
    "#Model: 70 param:N_estim:200 CC_alpha:3.0000000000000004e-05 Min_sample_split:3\n",
    "#Model: 77 param:N_estim:200 CC_alpha:5e-05 Min_sample_split:4\n",
    "#Model: 81 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:2\n",
    "#Model: 22 param:N_estim:100 CC_alpha:1e-05 Min_sample_split:3\n",
    "#Model: 25 param:N_estim:100 CC_alpha:2e-05 Min_sample_split:3\n",
    "#Model: 27 param:N_estim:100 CC_alpha:3.0000000000000004e-05 Min_sample_split:2\n",
    "#Model: 46 param:N_estim:150 CC_alpha:2e-05 Min_sample_split:3\n",
    "#Model: 55 param:N_estim:150 CC_alpha:5e-05 Min_sample_split:3\n",
    "#Model: 57 param:N_estim:150 CC_alpha:6e-05 Min_sample_split:2\n",
    "#Model: 60 param:N_estim:150 CC_alpha:7.000000000000001e-05 Min_sample_split:2\n",
    "#Model: 70 param:N_estim:200 CC_alpha:3.0000000000000004e-05 Min_sample_split:3\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#n_estimators=[50]\n",
    "#ccp_alpha =[0.00001]\n",
    "#min_samples_split=[3]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#best 11 model with all features has best ones in :\n",
    "#Model: 0 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "#n:193 Accuracy evaluation : 93.515 Accuracy train : 99.976\n",
    "#Model: 1 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "#n:144 Accuracy evaluation : 93.524\n",
    "#n:135 Accuracy evaluation : 93.524 Accuracy train : 99.650\n",
    "#Model: 4 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "#n:177 Accuracy evaluation : 93.506 Accuracy train : 99.954\n",
    "#Model: 9 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "#n:253 Accuracy evaluation : 93.541 Accuracy train : 99.937\n",
    "#n:126 Accuracy evaluation : 93.532 Accuracy train : 99.967\n",
    "#n:69 Accuracy evaluation : 93.524 Accuracy train : 99.939\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators = [50,50,50,100,100,100,100,150,200,200,200, 100,150]\n",
    "ccp_alpha = [0.00001,0.00005,0.00006,0.00001,0.00005,0.00007,0.00009,0.00007,0.00005,0.00003,0.00005,0.00002,0.00007]\n",
    "min_samples_split = [3,4,4,4,4,3,2,3,2,3,4,3,2]\n",
    "features = [185,277,146,81,162,169,302,312,292,132,193,69]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators = [200,200, 100,150]\n",
    "ccp_alpha = [0.00003,0.00005,0.00002,0.00007]\n",
    "min_samples_split = [3,4,3,2]\n",
    "#for those models we have :\n",
    "Model: 0 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "n:253 Accuracy evaluation : 93.338 Accuracy train : 99.970\n",
    "n:193 Accuracy evaluation : 93.411 Accuracy train : 99.963   <-best\n",
    "n:177 Accuracy evaluation : 93.373 Accuracy train : 99.960\n",
    "n:144 Accuracy evaluation : 93.364 Accuracy train : 99.952\n",
    "n:135 Accuracy evaluation : 93.318 Accuracy train : 99.971\n",
    "n:126 Accuracy evaluation : 93.353 Accuracy train : 99.968\n",
    "n:69 Accuracy evaluation : 93.309 Accuracy train : 99.957\n",
    "Model: 1 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "n:253 Accuracy evaluation : 93.329 Accuracy train : 99.588\n",
    "n:193 Accuracy evaluation : 93.350 Accuracy train : 99.578\n",
    "n:177 Accuracy evaluation : 93.347 Accuracy train : 99.574\n",
    "n:144 Accuracy evaluation : 93.367 Accuracy train : 99.592\n",
    "n:135 Accuracy evaluation : 93.344 Accuracy train : 99.610\n",
    "n:126 Accuracy evaluation : 93.347 Accuracy train : 99.595\n",
    "n:69 Accuracy evaluation : 93.344 Accuracy train : 99.595\n",
    "Model: 2 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "n:253 Accuracy evaluation : 93.344 Accuracy train : 99.944\n",
    "n:193 Accuracy evaluation : 93.376 Accuracy train : 99.940\n",
    "n:177 Accuracy evaluation : 93.268 Accuracy train : 99.930\n",
    "n:144 Accuracy evaluation : 93.295 Accuracy train : 99.933\n",
    "n:135 Accuracy evaluation : 93.330 Accuracy train : 99.943\n",
    "n:126 Accuracy evaluation : 93.330 Accuracy train : 99.947\n",
    "n:69 Accuracy evaluation : 93.309 Accuracy train : 99.941\n",
    "Model: 3 param:N_estim:200 CC_alpha:7.000000000000001e-05 Min_sample_split:4\n",
    "n:253 Accuracy evaluation : 93.350 Accuracy train : 99.952\n",
    "n:193 Accuracy evaluation : 93.329 Accuracy train : 99.950\n",
    "n:177 Accuracy evaluation : 93.350 Accuracy train : 99.955\n",
    "n:144 Accuracy evaluation : 93.312 Accuracy train : 99.952\n",
    "n:135 Accuracy evaluation : 93.292 Accuracy train : 99.954\n",
    "n:126 Accuracy evaluation : 93.373 Accuracy train : 99.946\n",
    "n:69 Accuracy evaluation : 93.434 Accuracy train : 99.952   <-best\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#run more that 10 fold iterations for kfold validation for features 69, 193\n",
    "n_estimators = [200,150]\n",
    "ccp_alpha = [0.00003,0.00007]\n",
    "min_samples_split = [3,2]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "Model: 1 param:N_estim:50 CC_alpha:1e-05 Min_sample_split:3\n",
    "n:185 Accuracy evaluation : 94.103 Accuracy train : 99.887\n",
    "\n",
    "Model: 14 param:N_estim:50 CC_alpha:5e-05 Min_sample_split:4\n",
    "n:277 Accuracy evaluation : 94.131 Accuracy train : 99.587\n",
    "\n",
    "Model: 17 param:N_estim:50 CC_alpha:6e-05 Min_sample_split:4\n",
    "n:146 Accuracy evaluation : 94.103 Accuracy train : 99.500\n",
    "\n",
    "Model: 29 param:N_estim:100 CC_alpha:1e-05 Min_sample_split:4\n",
    "n:81 Accuracy evaluation : 94.103 Accuracy train : 99.775\n",
    "\n",
    "Model: 41 param:N_estim:100 CC_alpha:5e-05 Min_sample_split:4\n",
    "n:162 Accuracy evaluation : 94.103 Accuracy train : 99.587\n",
    "\n",
    "Model: 46 param:N_estim:100 CC_alpha:7.000000000000001e-05 Min_sample_split:3\n",
    "n:169 Accuracy evaluation : 94.103 Accuracy train : 99.862\n",
    "\n",
    "Model: 51 param:N_estim:100 CC_alpha:9e-05 Min_sample_split:2\n",
    "n:302 Accuracy evaluation : 94.160 Accuracy train : 99.875\n",
    "\n",
    "Model: 65 param:N_estim:150 CC_alpha:4e-05 Min_sample_split:4\n",
    "n:312 Accuracy evaluation : 94.103 Accuracy train : 99.737\n",
    "\n",
    "Model: 73 param:N_estim:150 CC_alpha:7.000000000000001e-05 Min_sample_split:3\n",
    "n:292 Accuracy evaluation : 94.131 Accuracy train : 99.800\n",
    "\n",
    "Model: 93 param:N_estim:200 CC_alpha:5e-05 Min_sample_split:2\n",
    "n:132 Accuracy evaluation : 94.131 Accuracy train : 100.000\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators = [50,50,50,100,100,100,100,150,200,200,200, 100,150]\n",
    "ccp_alpha = [0.00001,0.00005,0.00006,0.00001,0.00005,0.00007,0.00009,0.00007,0.00005,0.00003,0.00005,0.00002,0.00007]\n",
    "min_samples_split = [3,4,4,4,4,3,2,3,2,3,4,3,2]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "Model: 1 param:N_estim:200 CC_alpha:5e-05 Min_sample_split:4\n",
    "\n",
    "n:99 Accuracy evaluation : 93.510 Accuracy train : 99.811\n",
    "n:98 Accuracy evaluation : 93.506 Accuracy train : 99.807\n",
    "n:97 Accuracy evaluation : 93.507 Accuracy train : 99.811\n",
    "n:96 Accuracy evaluation : 93.517 Accuracy train : 99.803\n",
    "n:95 Accuracy evaluation : 93.513 Accuracy train : 99.796\n",
    "n:94 Accuracy evaluation : 93.500 Accuracy train : 99.810\n",
    "n:93 Accuracy evaluation : 93.501 Accuracy train : 99.802\n",
    "n:91 Accuracy evaluation : 93.492 Accuracy train : 99.794\n",
    "n:90 Accuracy evaluation : 93.515 Accuracy train : 99.794\n",
    "n:89 Accuracy evaluation : 93.503 Accuracy train : 99.791\n",
    "n:88 Accuracy evaluation : 93.486 Accuracy train : 99.793\n",
    "n:87 Accuracy evaluation : 93.508 Accuracy train : 99.792\n",
    "n:85 Accuracy evaluation : 93.505 Accuracy train : 99.785\n",
    "n:82 Accuracy evaluation : 93.502 Accuracy train : 99.773 \n",
    "\n",
    "Model: 2 param:N_estim:150 CC_alpha:7e-05 Min_sample_split:2\n",
    "\n",
    "n:99 Accuracy evaluation : 93.496 Accuracy train : 99.991\n",
    "n:98 Accuracy evaluation : 93.515 Accuracy train : 99.990\n",
    "n:97 Accuracy evaluation : 93.510 Accuracy train : 99.991\n",
    "\n",
    "n:95 Accuracy evaluation : 93.514 Accuracy train : 99.992\n",
    "n:94 Accuracy evaluation : 93.481 Accuracy train : 99.992\n",
    "n:93 Accuracy evaluation : 93.483 Accuracy train : 99.993\n",
    "n:92 Accuracy evaluation : 93.493 Accuracy train : 99.992\n",
    "n:91 Accuracy evaluation : 93.520 Accuracy train : 99.990\n",
    "\n",
    "n:89 Accuracy evaluation : 93.508 Accuracy train : 99.992\n",
    "\n",
    "n:87 Accuracy evaluation : 93.481 Accuracy train : 99.991\n",
    "n:86 Accuracy evaluation : 93.488 Accuracy train : 99.990\n",
    "\n",
    "n:84 Accuracy evaluation : 93.488 Accuracy train : 99.991\n",
    "n:83 Accuracy evaluation : 93.503 Accuracy train : 99.990\n",
    "\n",
    "n:81 Accuracy evaluation : 93.502 Accuracy train : 99.989\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "features = [185,277,146,81,162,169,302,312,292,132,193,69]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#BEST model and features based on 5 iterations:\n",
    "#Model: 3 param:N_estim:100 CC_alpha:1e-05 Min_sample_split:4\n",
    "#n:81 Accuracy evaluation : 93.344 Accuracy train : 99.675  <- best\n",
    "#Model: 10 param:N_estim:200 CC_alpha:5e-05 Min_sample_split:4\n",
    "#n:193 Accuracy evaluation : 93.336 Accuracy train : 99.577  <- best \n",
    "#n:169 Accuracy evaluation : 93.356 Accuracy train : 99.590  <- best\n",
    "#Model: 12 param:N_estim:150 CC_alpha:7e-05 Min_sample_split:2\n",
    "#n:302 Accuracy evaluation : 93.344 Accuracy train : 99.952  <- best\n",
    "#n:193 Accuracy evaluation : 93.330 Accuracy train : 99.952  <- best\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators = [100,200,150]\n",
    "ccp_alpha = [0.00001,0.00005,0.00007]\n",
    "min_samples_split = [4,4,2]\n",
    "\n",
    "features = [81,169,193,302]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "n_estimators = [150]\n",
    "ccp_alpha = [0.00007]\n",
    "min_samples_split = [2]\n",
    "\n",
    "features = [81,169,193,302]\n",
    "\n",
    "n:134 Accuracy evaluation : 93.529 Accuracy train : 99.996\n",
    "n:132 Accuracy evaluation : 93.543 Accuracy train : 99.993\n",
    "n:105 Accuracy evaluation : 93.538 Accuracy train : 99.993\n",
    "n:102 Accuracy evaluation : 93.524 Accuracy train : 99.996\n",
    "n:101 Accuracy evaluation : 93.524 Accuracy train : 99.995\n",
    "n:90 Accuracy evaluation : 93.536 Accuracy train : 99.994\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------\n",
    "features = [90,101,102,105,132,134]\n",
    "Model: 0 param:N_estim:150 CC_alpha:7e-05 Min_sample_split:2\n",
    "n:134 Accuracy evaluation : 93.355 Accuracy train : 99.991\n",
    "n:132 Accuracy evaluation : 93.405 Accuracy train : 99.992\n",
    "n:105 Accuracy evaluation : 93.421 Accuracy train : 99.992\n",
    "n:102 Accuracy evaluation : 93.429 Accuracy train : 99.994 <- best with 20 iterations of 5-fold cross validation\n",
    "n:101 Accuracy evaluation : 93.412 Accuracy train : 99.992\n",
    "n:90 Accuracy evaluation : 93.404 Accuracy train : 99.991\n",
    "\"\"\"\n",
    "#features.sort()\n",
    "\n",
    "model_param = []\n",
    "model_scores = []\n",
    "#for n_est in n_estimators:\n",
    "#    for c_alpha in ccp_alpha:\n",
    "#        for min_sam in min_samples_split:\n",
    "#            model_scores.append(defaultdict(lambda: {\"test\": [], \"train\": []}))\n",
    "#            model_param.append(\"N_estim:{} CC_alpha:{} Min_sample_split:{}\".format(\n",
    "#                                    n_est, c_alpha, min_sam))\n",
    "for i in range(len(min_samples_split)):\n",
    "    model_scores.append(defaultdict(lambda: {\"test\": [], \"train\": []}))\n",
    "    model_param.append(\"N_estim:{} CC_alpha:{} Min_sample_split:{}\".format(\n",
    "                                    n_estimators[i], \n",
    "                                    ccp_alpha[i], \n",
    "                                    min_samples_split[i]))\n",
    "\n",
    "c = 0\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "number_of_iter = 20 \n",
    "start_time = datetime.now()\n",
    "all_comp = len(model_scores) * number_of_iter * 5\n",
    "for fold in range(number_of_iter):\n",
    "    #split entire dataset into 2 parts: \n",
    "    #                    1st with all data used in training/evaluation(80%) \n",
    "    #                    2nd with data used in final testing (20%) \n",
    "    df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "    \n",
    "    models = []\n",
    "    predictions = []\n",
    "    accuracy = []\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    #all_comp = len(objective) * len(learning_rate) * len(n_estimators) * len(max_depth) * len(colsample_bytree)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    #all_features = None\n",
    "\n",
    "    # print(\"Alpha:{:.6f}\".format(number_of_features))\n",
    "    \n",
    "    #at this step we use only 1st splited part with 80% of our data that is used on train/eval split (70/30)\n",
    "    #X_train, X_eval, y_train, y_eval = train_test_split(df, target, test_size=0.3)\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(df, target):\n",
    "        #i = 0\n",
    "        X_train, X_eval = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_eval = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "        # scale data separatly\n",
    "        X_train, X_eval, df_test_scaled = scale_data(X_train, X_eval, df_test)\n",
    "\n",
    "        # oversamplong clean on training set\n",
    "        X_train, y_train = oversample(X_train, y_train)\n",
    "        #oversampling clean on evaluation set\n",
    "        X_eval, y_eval = oversample(X_eval, y_eval)\n",
    "        #for n_est in n_estimators:\n",
    "        #    for c_alpha in ccp_alpha:\n",
    "        #        for min_sam in min_samples_split:\n",
    "\n",
    "        for i in range(len(min_samples_split)):\n",
    "            scores = defaultdict(lambda: {\"test\": [], \"train\": [], \"unseen\": []})\n",
    "\n",
    "            #for n in list(range(1,5)) + list(range(int(X_train.shape[1]/2),int(X_train.shape[1]/2) + 5)) + list(range(X_train.shape[1] - 5 ,X_train.shape[1])):\n",
    "            #for n in range(1,X_train.shape[1]):\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators[i],\n",
    "                                            criterion='gini',\n",
    "                                            ccp_alpha=ccp_alpha[i], #best 0.0001 - 0.00005\n",
    "                                            min_samples_split=min_samples_split[i]) #best \n",
    "            model.fit(X_train,y_train)\n",
    "            \n",
    "            \n",
    "            for n in features:\n",
    "\n",
    "                # select features using threshold\n",
    "                #X_train_f, ft = rforest_features(X_train, y_train, n)\n",
    "                #X_eval_f = X_eval[ft]\n",
    "                #print(\"Features:{}\".format(n))\n",
    "                \n",
    "\n",
    "            \n",
    "                selection = SelectFromModel(model, threshold=-np.inf, max_features=n,prefit=True)\n",
    "                X_train_f = selection.transform(X_train)\n",
    "                X_eval_f = selection.transform(X_eval)\n",
    "                #print(\"Shape:{}\".format(X_train_f.shape[1]))\n",
    "\n",
    "                # train model\n",
    "                selection_model = RandomForestClassifier(n_estimators=n_estimators[i],\n",
    "                                            criterion='gini',\n",
    "                                            ccp_alpha=ccp_alpha[i], #best 0.0001 - 0.00005\n",
    "                                            min_samples_split=min_samples_split[i]) #best \n",
    "\n",
    "                selection_model.fit(X_train_f, y_train)\n",
    "                # eval model\n",
    "\n",
    "                predictions = selection_model.predict(X_eval_f)\n",
    "                accuracy = accuracy_score(y_eval, predictions)\n",
    "\n",
    "                predictions_train = selection_model.predict(X_train_f)\n",
    "                accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "                \"\"\"\n",
    "                select_df_test = selection.transform(df_test)\n",
    "                predictions_df_test = selection_model.predict(select_df_test)\n",
    "                accuracy_df_test = accuracy_score(target_test, predictions_df_test)\n",
    "                \"\"\"\n",
    "                #print(i)\n",
    "                #print(model_scores[i])\n",
    "                #print(n)\n",
    "                #print(model_scores[i][n])\n",
    "                #print(\"-----------------\")\n",
    "\n",
    "                model_scores[i][X_train_f.shape[1]][\"test\"].append(accuracy*100.0)\n",
    "                model_scores[i][X_train_f.shape[1]][\"train\"].append(accuracy_train*100.0)\n",
    "            c += 1\n",
    "            #i += 1\n",
    "\n",
    "            spend_time = (datetime.now() - start_time).seconds / 60.0\n",
    "            print(\"Time spend:{} min. left comp: {} left time:{}\".format(spend_time, all_comp - c,\n",
    "                                                                 (spend_time / c) * (all_comp - c)))\n",
    "\n",
    "\n",
    "for i in range(len(model_scores)):\n",
    "    print(\"Model: {} param:{}\".format(i, model_param[i]))\n",
    "    n = list(model_scores[i].keys())\n",
    "    n.sort(reverse=True)\n",
    "    for sc in n:\n",
    "        print(\"n:{} Accuracy evaluation : {:.3f} Accuracy train : {:.3f}\"\n",
    "                        #+\"Accuracy df test : {:.3f}\"\n",
    "                        .format(sc,\n",
    "                             sum(model_scores[i][sc][\"test\"])/len(model_scores[i][sc][\"test\"]),\n",
    "                             sum(model_scores[i][sc][\"train\"])/len(model_scores[i][sc][\"train\"]),\n",
    "                             #sum(model_scores[sc][\"unseen\"])/len(model_scores[sc][\"unseen\"])\n",
    "                             ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (after fine tuning) 5 fold cross validation with testing portion of data accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spend:1.0833333333333333 min. left comp: 99 left time:107.24999999999999\n",
      "Time spend:2.183333333333333 min. left comp: 98 left time:106.98333333333332\n",
      "Time spend:3.3666666666666667 min. left comp: 97 left time:108.85555555555555\n",
      "Time spend:4.566666666666666 min. left comp: 96 left time:109.6\n",
      "Time spend:5.766666666666667 min. left comp: 95 left time:109.56666666666666\n",
      "Time spend:6.883333333333334 min. left comp: 94 left time:107.8388888888889\n",
      "Time spend:7.966666666666667 min. left comp: 93 left time:105.84285714285714\n",
      "Time spend:9.083333333333334 min. left comp: 92 left time:104.45833333333334\n",
      "Time spend:10.233333333333333 min. left comp: 91 left time:103.47037037037035\n",
      "Time spend:11.35 min. left comp: 90 left time:102.15\n",
      "Time spend:12.483333333333333 min. left comp: 89 left time:101.00151515151515\n",
      "Time spend:13.583333333333334 min. left comp: 88 left time:99.61111111111111\n",
      "Time spend:14.566666666666666 min. left comp: 87 left time:97.48461538461538\n",
      "Time spend:15.65 min. left comp: 86 left time:96.13571428571429\n",
      "Time spend:16.816666666666666 min. left comp: 85 left time:95.29444444444445\n",
      "Time spend:18.05 min. left comp: 84 left time:94.7625\n",
      "Time spend:19.216666666666665 min. left comp: 83 left time:93.82254901960785\n",
      "Time spend:20.383333333333333 min. left comp: 82 left time:92.8574074074074\n",
      "Time spend:21.516666666666666 min. left comp: 81 left time:91.72894736842105\n",
      "Time spend:22.666666666666668 min. left comp: 80 left time:90.66666666666666\n",
      "Time spend:23.85 min. left comp: 79 left time:89.72142857142858\n",
      "Time spend:25.016666666666666 min. left comp: 78 left time:88.69545454545454\n",
      "Time spend:26.2 min. left comp: 77 left time:87.71304347826086\n",
      "Time spend:27.433333333333334 min. left comp: 76 left time:86.87222222222222\n",
      "Time spend:28.65 min. left comp: 75 left time:85.94999999999999\n",
      "Time spend:29.766666666666666 min. left comp: 74 left time:84.72051282051281\n",
      "Time spend:30.816666666666666 min. left comp: 73 left time:83.31913580246913\n",
      "Time spend:31.966666666666665 min. left comp: 72 left time:82.19999999999999\n",
      "Time spend:33.03333333333333 min. left comp: 71 left time:80.87471264367817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_estimators = 150\n",
    "ccp_alpha = 0.00007\n",
    "min_samples_split = 2\n",
    "\n",
    "#features.sort()\n",
    "\n",
    "\n",
    "\n",
    "model_scores = {\"test\": [], \"train\": [], \"unseen\":[]}\n",
    "\n",
    "c = 0\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "number_of_iter = 20 \n",
    "start_time = datetime.now()\n",
    "all_comp = number_of_iter * 5\n",
    "\n",
    "start_time = datetime.now()\n",
    "#Make multiple folds of data\n",
    "\n",
    "for fold in range(number_of_iter):\n",
    "    #split entire dataset into 2 parts: \n",
    "    #                    1st (80%) with all data used in training/evaluation\n",
    "    #                    2nd (20%) with data used in final testing\n",
    "    #Each time when we read file, we random shufle it\n",
    "    df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "    \n",
    "    #df , target is 80% of dataset from file\n",
    "    #df_test, target_test is 20% of dataset used only for evalution\n",
    "    \n",
    "    \n",
    "    #all_comp = len(objective) * len(learning_rate) * len(n_estimators) * len(max_depth) * len(colsample_bytree)\n",
    "    \n",
    "    ##############################\n",
    "    #Make 5 Fold cross validation#\n",
    "    ##############################\n",
    "    \n",
    "    #Split the df and target by 70/30 for train and validation, in stratified form \n",
    "    #X_train, X_val, y_train, y_val = train_test_split(df, target, test_size=0.30,stratify=target)\n",
    "    \n",
    "    #Make Stratified K-Fold cross validation of K=5 with random shufle \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kf.split(df, target):\n",
    "        #i = 0\n",
    "        X_train, X_val = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_val = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "        ##############################################\n",
    "        #Scale data (train,evaluation and final test)#\n",
    "        ##############################################\n",
    "\n",
    "        # scale train,evaluation and final testing data \n",
    "        X_train, X_val, Final_X = scale_data(X_train, X_val, df_test)\n",
    "\n",
    "        ####################################################\n",
    "        #Oversample train and validation portion separately#\n",
    "        ####################################################\n",
    "\n",
    "        # oversampling clean on training set\n",
    "        X_train, y_train = oversample(X_train, y_train)\n",
    "\n",
    "\n",
    "        #oversampling clean on validation set\n",
    "        X_val, y_val = oversample(X_val, y_val)\n",
    "\n",
    "        ####################################################\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "        #Important, we do not oversample the final test set#\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "        ####################################################\n",
    "\n",
    "        ############################################\n",
    "        #Train model based on train portion of data#\n",
    "        #This model used for feature selection.    #\n",
    "        ############################################\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                        criterion='gini',\n",
    "                                        ccp_alpha=ccp_alpha, #best 0.0001 - 0.00005\n",
    "                                        min_samples_split=min_samples_split) #best \n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        #########################################################\n",
    "        #Select from trained model the N most important features#\n",
    "        #########################################################\n",
    "\n",
    "        selection = SelectFromModel(model, threshold=-np.inf, max_features=102,prefit=True)\n",
    "\n",
    "        ######################################################################################\n",
    "        #Based on those selected features, transform the train,validation and final test sets#\n",
    "        ######################################################################################\n",
    "\n",
    "        X_train_f = selection.transform(X_train)\n",
    "        X_val_f = selection.transform(X_val)\n",
    "        Final_X = selection.transform(Final_X)\n",
    "\n",
    "        ########################################################################\n",
    "        #Train model based on train portion of data with selected features only#\n",
    "        ########################################################################\n",
    "\n",
    "        selection_model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                    criterion='gini',\n",
    "                                    ccp_alpha=ccp_alpha, #best 0.0001 - 0.00005\n",
    "                                    min_samples_split=min_samples_split) #best \n",
    "\n",
    "        selection_model.fit(X_train_f, y_train)\n",
    "\n",
    "        ################################################\n",
    "        #Evaluation of model by pretiction of val data#\n",
    "        ################################################\n",
    "\n",
    "        #predict train labels\n",
    "        predictions_train = selection_model.predict(X_train_f)\n",
    "\n",
    "        #predict validation labels\n",
    "        predictions = selection_model.predict(X_val_f)\n",
    "\n",
    "        #predict the final test labels\n",
    "        predictions_final = selection_model.predict(Final_X)\n",
    "\n",
    "\n",
    "        #########################\n",
    "        #Compute accuracy scores#\n",
    "        #########################\n",
    "\n",
    "        accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "        accuracy = accuracy_score(y_val, predictions)\n",
    "        accuracy_df_test = accuracy_score(target_test, predictions_final)\n",
    "\n",
    "        ########################################\n",
    "        #Score model scores for particular fold#\n",
    "        ########################################\n",
    "\n",
    "        model_scores[\"test\"].append(accuracy*100.0)\n",
    "        model_scores[\"train\"].append(accuracy_train*100.0)\n",
    "        model_scores[\"unseen\"].append(accuracy_df_test*100.0)\n",
    "        \n",
    "        c += 1 \n",
    "        spend_time = (datetime.now() - start_time).seconds / 60.0\n",
    "        print(\"Time spend:{} min. left comp: {} left time:{}\".format(spend_time, all_comp - c,\n",
    "                                                                 (spend_time / c) * (all_comp - c)))\n",
    "\n",
    "\n",
    "print(\"Random forest: Accuracy validation : {:.3f} Accuracy train : {:.3f} Accuracy test : {:.3f}\"\n",
    "                            .format(\n",
    "                             sum(model_scores[\"test\"])/len(model_scores[\"test\"]),\n",
    "                             sum(model_scores[\"train\"])/len(model_scores[\"train\"]),\n",
    "                             sum(model_scores[\"unseen\"])/len(model_scores[\"unseen\"]),\n",
    "                             ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (after fine tuning) 5 fold cross validation with testing portion of data accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spend:1.25 min. left comp: 19 left time:23.75\n",
      "Time spend:1.2333333333333334 min. left comp: 18 left time:11.100000000000001\n",
      "Time spend:1.1 min. left comp: 17 left time:6.233333333333334\n",
      "Time spend:1.1666666666666667 min. left comp: 16 left time:4.666666666666667\n",
      "Time spend:1.1333333333333333 min. left comp: 15 left time:3.4\n",
      "Time spend:1.0333333333333334 min. left comp: 14 left time:2.4111111111111114\n",
      "Time spend:1.0833333333333333 min. left comp: 13 left time:2.0119047619047614\n",
      "Time spend:1.0833333333333333 min. left comp: 12 left time:1.625\n",
      "Time spend:1.05 min. left comp: 11 left time:1.2833333333333334\n",
      "Time spend:1.0833333333333333 min. left comp: 10 left time:1.0833333333333333\n",
      "Time spend:1.0666666666666667 min. left comp: 9 left time:0.8727272727272727\n",
      "Time spend:1.0833333333333333 min. left comp: 8 left time:0.7222222222222222\n",
      "Time spend:1.0666666666666667 min. left comp: 7 left time:0.5743589743589743\n",
      "Time spend:1.0666666666666667 min. left comp: 6 left time:0.4571428571428571\n",
      "Time spend:1.1 min. left comp: 5 left time:0.3666666666666667\n",
      "Time spend:1.05 min. left comp: 4 left time:0.2625\n",
      "Time spend:1.0666666666666667 min. left comp: 3 left time:0.18823529411764706\n",
      "Time spend:1.05 min. left comp: 2 left time:0.11666666666666667\n",
      "Time spend:1.0833333333333333 min. left comp: 1 left time:0.05701754385964912\n",
      "Time spend:1.1333333333333333 min. left comp: 0 left time:0.0\n",
      "Random forest: Accuracy evaluation : 94.401 Accuracy train : 99.823 Accuracy unseen : 93.254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_estimators = 150\n",
    "ccp_alpha = 0.00007\n",
    "min_samples_split = 2\n",
    "\n",
    "#features.sort()\n",
    "\n",
    "objective = \"multi:softprob\"\n",
    "learning_rate = 0.078\n",
    "n_estimators = 490\n",
    "max_depth = 4\n",
    "colsample_bytree = 0.5\n",
    "eval_metric = \"aucpr\"\n",
    "\n",
    "model_scores = {\"test\": [], \"train\": [], \"unseen\":[]}\n",
    "\n",
    "c = 0\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "number_of_iter = 20 \n",
    "start_time = datetime.now()\n",
    "all_comp = number_of_iter\n",
    "start_time = datetime.now()\n",
    "\n",
    "#Make multiple folds of data\n",
    "for fold in range(number_of_iter):\n",
    "    #split entire dataset into 2 parts: \n",
    "    #                    1st(80%) with all data used in training/validation\n",
    "    #                    2nd(20%) with data used in final testing (20%) \n",
    "    #Each time when we read file, we random shufle it\n",
    "    \n",
    "    #df , target is 80% of dataset from file\n",
    "    #df_test, target_test is 20% of dataset used only for evalution\n",
    "    df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "    \n",
    "    \n",
    "   \n",
    "    #X_train, X_eval, y_train, y_eval = train_test_split(df, target, test_size=0.30,stratify=target)\n",
    "    \n",
    "    ##############################\n",
    "    #Make 5 Fold cross validation#\n",
    "    ##############################\n",
    "    \n",
    "    #Split the df and target by 70/30 for train and validation, in stratified form \n",
    "    #X_train, X_val, y_train, y_val = train_test_split(df, target, test_size=0.30,stratify=target)\n",
    "    \n",
    "    #Make Stratified K-Fold cross validation of K=5 with random shufle \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kf.split(df, target):\n",
    "        #i = 0\n",
    "        X_train, X_val = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_val = target.iloc[train_index], target.iloc[test_index]\n",
    "        \n",
    "        ##############################################\n",
    "        #Scale data (train,evaluation and final test)#\n",
    "        ##############################################\n",
    "\n",
    "        # scale train,evaluation and final testing data \n",
    "        X_train, X_val, Final_X = scale_data(X_train, X_val, df_test)\n",
    "\n",
    "        ####################################################\n",
    "        #Oversample train and validation portion separately#\n",
    "        ####################################################\n",
    "\n",
    "        # oversampling clean on training set\n",
    "        X_train, y_train = oversample(X_train, y_train)\n",
    "\n",
    "\n",
    "        #oversampling clean on validation set\n",
    "        X_val, y_val = oversample(X_val, y_val)\n",
    "\n",
    "        ####################################################\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "        #Important, we do not oversample the final test set#\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "        ####################################################\n",
    "\n",
    "        ############################################\n",
    "        #Train model based on train portion of data#\n",
    "        #This model used for feature selection.    #\n",
    "        ############################################\n",
    "\n",
    "        model = XGBClassifier(objective=objective,\n",
    "                                num_class=2,\n",
    "                                learning_rate=learning_rate,\n",
    "                                n_estimators=n_estimators,\n",
    "                                max_depth=max_depth,\n",
    "                                colsample_bytree=colsample_bytree,\n",
    "                                eval_metric=eval_metric,  # \"rmse\",\n",
    "                                use_label_encoder=False)\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        #########################################################\n",
    "        #Select from trained model the N most important features#\n",
    "        #########################################################\n",
    "\n",
    "        selection = SelectFromModel(model, threshold=-np.inf, max_features=206,prefit=True)\n",
    "        \n",
    "        ######################################################################################\n",
    "        #Based on those selected features, transform the train,validation and final test sets#\n",
    "        ######################################################################################\n",
    "\n",
    "        X_train_f = selection.transform(X_train)\n",
    "        X_val_f = selection.transform(X_val)\n",
    "        Final_X = selection.transform(Final_X)\n",
    "        \n",
    "        ########################################################################\n",
    "        #Train model based on train portion of data with selected features only#\n",
    "        ########################################################################\n",
    "    \n",
    "        selection_model = XGBClassifier(objective=objective,\n",
    "                                num_class=2,\n",
    "                                learning_rate=learning_rate,\n",
    "                                n_estimators=n_estimators,\n",
    "                                max_depth=max_depth,\n",
    "                                colsample_bytree=colsample_bytree,\n",
    "                                eval_metric=eval_metric,  # \"rmse\",\n",
    "                                use_label_encoder=False)\n",
    "        \n",
    "        selection_model.fit(X_train_f, y_train)\n",
    "\n",
    "        ################################################\n",
    "        #Evaluation of model by pretiction of val data#\n",
    "        ################################################\n",
    "\n",
    "        #predict train labels\n",
    "        predictions_train = selection_model.predict(X_train_f)\n",
    "\n",
    "        #predict validation labels\n",
    "        predictions = selection_model.predict(X_val_f)\n",
    "\n",
    "        #predict the final test labels\n",
    "        predictions_final = selection_model.predict(Final_X)\n",
    "\n",
    "\n",
    "        #########################\n",
    "        #Compute accuracy scores#\n",
    "        #########################\n",
    "\n",
    "        accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "        accuracy = accuracy_score(y_val, predictions)\n",
    "        accuracy_df_test = accuracy_score(target_test, predictions_final)\n",
    "\n",
    "        ########################################\n",
    "        #Score model scores for particular fold#\n",
    "        ########################################\n",
    "\n",
    "        model_scores[\"test\"].append(accuracy*100.0)\n",
    "        model_scores[\"train\"].append(accuracy_train*100.0)\n",
    "        model_scores[\"unseen\"].append(accuracy_df_test*100.0)\n",
    "        \n",
    "        c += 1 \n",
    "        spend_time = (datetime.now() - start_time).seconds / 60.0\n",
    "        print(\"Time spend:{} min. left comp: {} left time:{}\".format(spend_time, all_comp - c,\n",
    "                                                                 (spend_time / c) * (all_comp - c)))\n",
    "\n",
    "\n",
    "print(\"XGBoost: Accuracy validation : {:.3f} Accuracy train : {:.3f} Accuracy test : {:.3f}\"\n",
    "                            .format(\n",
    "                             sum(model_scores[\"test\"])/len(model_scores[\"test\"]),\n",
    "                             sum(model_scores[\"train\"])/len(model_scores[\"train\"]),\n",
    "                             sum(model_scores[\"unseen\"])/len(model_scores[\"unseen\"]),\n",
    "                             ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGboost Score:0.9405343292157425\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Random forest Score:0.9346330275229358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\npredictions = selection_model.predict(X_eval_f)\\naccuracy = accuracy_score(y_eval, predictions)\\n\\npredictions_train = selection_model.predict(X_train_f)\\naccuracy_train = accuracy_score(y_train, predictions_train)\\n\\nselect_df_test = selection.transform(df_test_scaled)\\npredictions_df_test = selection_model.predict(select_df_test)\\naccuracy_df_test = accuracy_score(target_test, predictions_df_test)\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_curve, auc, precision_recall_curve, accuracy_score, average_precision_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "objective = \"multi:softprob\"\n",
    "learning_rate = 0.078\n",
    "n_estimators = 490\n",
    "max_depth = 4\n",
    "colsample_bytree = 0.5\n",
    "eval_metric = \"aucpr\"\n",
    "\n",
    "model_scores = {\"test\": [], \"train\": [], \"unseen\":[]}\n",
    "\n",
    "c = 0\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "\n",
    "df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df, target, test_size=0.30,stratify=target)\n",
    "\n",
    "# scale data separatly\n",
    "X_train, X_eval, df_test_scaled = scale_data(X_train, X_eval, df_test)\n",
    "\n",
    "# oversamplong clean on training set\n",
    "X_train, y_train = oversample(X_train, y_train)\n",
    "#oversampling clean on evaluation set\n",
    "X_eval, y_eval = oversample(X_eval, y_eval)\n",
    "\n",
    "\n",
    "\n",
    "model = XGBClassifier(objective=objective,\n",
    "                        num_class=2,\n",
    "                        learning_rate=learning_rate,\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        colsample_bytree=colsample_bytree,\n",
    "                        eval_metric=eval_metric,  # \"rmse\",\n",
    "                        use_label_encoder=False)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "selection = SelectFromModel(model, threshold=-np.inf, max_features=206,prefit=True)\n",
    "\n",
    "X_train_f = selection.transform(X_train)\n",
    "X_eval_f = selection.transform(X_eval)\n",
    "#print(\"Shape:{}\".format(X_train_f.shape[1]))\n",
    "\n",
    "# train model\n",
    "selection_model = XGBClassifier(objective=objective,\n",
    "                        num_class=2,\n",
    "                        learning_rate=learning_rate,\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        colsample_bytree=colsample_bytree,\n",
    "                        eval_metric=eval_metric,  # \"rmse\",\n",
    "                        use_label_encoder=False)\n",
    "selection_model.fit(X_train_f, y_train)\n",
    "# eval model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf_opt_pred_prob = selection_model.predict_proba(X_eval_f)\n",
    "print(\"F1 XGboost Score:{}\".format(f1_score(y_eval, selection_model.predict(X_eval_f))))\n",
    "\n",
    "plot_confusion_matrix(selection_model, X_eval_f, y_eval)  \n",
    "plt.show()\n",
    "plt.clf()\n",
    "fpr_xg, tpr_xg, thresholds_xg = roc_curve(y_eval, rf_opt_pred_prob[:, 1])\n",
    "roc_auc_xg = auc(fpr_xg, tpr_xg)\n",
    "#random forest\n",
    "n_estimators = 150\n",
    "ccp_alpha = 0.00007\n",
    "min_samples_split = 2\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                criterion='gini',\n",
    "                                ccp_alpha=ccp_alpha, #best 0.0001 - 0.00005\n",
    "                                min_samples_split=min_samples_split) #best \n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "selection = SelectFromModel(model, threshold=-np.inf, max_features=102,prefit=True)\n",
    "X_train_f = selection.transform(X_train)\n",
    "X_eval_f = selection.transform(X_eval)\n",
    "#print(\"Shape:{}\".format(X_train_f.shape[1]))\n",
    "\n",
    "# train model\n",
    "selection_model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                            criterion='gini',\n",
    "                            ccp_alpha=ccp_alpha, #best 0.0001 - 0.00005\n",
    "                            min_samples_split=min_samples_split) #best \n",
    "\n",
    "selection_model.fit(X_train_f, y_train)\n",
    "\n",
    "rf_opt_pred_prob = selection_model.predict_proba(X_eval_f)\n",
    "\n",
    "print(\"F1 Random forest Score:{}\".format(f1_score(y_eval, selection_model.predict(X_eval_f))))\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_eval, rf_opt_pred_prob[:, 1])\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "lw = 2\n",
    "plt.plot(fpr_xg, tpr_xg, label='ROC XGBoost(area = %0.2f)' % (roc_auc_xg))\n",
    "plt.plot(fpr_rf, tpr_rf, label='ROC RandomForest(area = %0.2f)' % (roc_auc_rf))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plot_confusion_matrix(selection_model, X_eval_f, y_eval)  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "predictions = selection_model.predict(X_eval_f)\n",
    "accuracy = accuracy_score(y_eval, predictions)\n",
    "\n",
    "predictions_train = selection_model.predict(X_train_f)\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "select_df_test = selection.transform(df_test_scaled)\n",
    "predictions_df_test = selection_model.predict(select_df_test)\n",
    "accuracy_df_test = accuracy_score(target_test, predictions_df_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Version 2 (manual implementation of K-fold)\n",
    " - Select features at each fold based by train split , each iteration select new features\n",
    " - at the end of execution make stratified 70/30 split with union of all selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#read csv file data with oversampling\n",
    "#filename = \"../data/features_large_lexical.csv\"\n",
    "#df, target, df_test,target_test = read_data(filename)\n",
    "\n",
    "\n",
    "\n",
    "def k_fold_val_2(number_of_features, \n",
    "                 objective=\"multi:softprob\",\n",
    "                 num_class = 2,\n",
    "                 learning_rate =0.2,\n",
    "                 n_estimators=100,\n",
    "                 max_depth=5,\n",
    "                 colsample_bytree = 0.7,\n",
    "                 eval_metric=\"aucpr\"#\"rmse\"\n",
    "                                           ):\n",
    "    accur_scores = []\n",
    "    accur_scores_train = []\n",
    "    accur_scores_unseen = []\n",
    "    #create Strafied 5 fold class\n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "    all_features = None\n",
    "    for folds in range(5):\n",
    "        filename = \"../data/features_large_lexical.csv\"\n",
    "        df, target, df_test,target_test = read_data(filename)\n",
    "\n",
    "        #print(\"Alpha:{:.6f}\".format(number_of_features))\n",
    "\n",
    "        #make 5 fold cross valdation with use of features selected by train split\n",
    "\n",
    "        for train_index, test_index in kf.split(df,target):\n",
    "            X_train = df.iloc[train_index]\n",
    "            X_train, X_test, y_train, y_test = df.iloc[train_index], df.iloc[test_index], target[train_index], target[test_index]\n",
    "\n",
    "            #select K features with use of random forest feature selection\n",
    "            X_train_f, ft = rforest_features(X_train, y_train, number_of_features)\n",
    "            #X_train_f, ft = lasso(X_train, y_train, alpha=number_of_features)\n",
    "            if all_features == None:\n",
    "                all_features = set(ft)\n",
    "            else:\n",
    "                all_features = all_features.intersection(set(ft))\n",
    "            #for f in ft:\n",
    "            #    all_features.append(f)\n",
    "\n",
    "            #keep only selected features in train and test splits\n",
    "            X_test_f = X_test[ft]\n",
    "            \"\"\"\n",
    "            #another classifiers for cross validation, that have pour performance till now\n",
    "            reg_scores = cross_val_score(LogisticRegression(max_iter=10000,penalty='l1',\n",
    "                                                            solver='liblinear',C=1.0), X, target,cv=5)\n",
    "\n",
    "            svc_scores = cross_val_score(SVC(kernel='rbf', C=0.9), X, target,cv=5)\n",
    "\n",
    "            for_scores = cross_val_score(RandomForestClassifier(n_estimators=35,\n",
    "                                                                criterion='gini',\n",
    "                                                                ccp_alpha=0.0003,\n",
    "                                                                min_samples_split=4), X, target,cv=5)\n",
    "\n",
    "            \"\"\"\n",
    "            xGb_cl = XGBClassifier(objective=objective,\n",
    "                                   num_class = num_class,\n",
    "                                   learning_rate =learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   eval_metric=eval_metric,#\"rmse\",\n",
    "                                   use_label_encoder=False)\n",
    "            #fit model with train dataset\n",
    "            xGb_cl.fit(X_train_f, y_train)\n",
    "\n",
    "            #predict test dataset and keep prediction labels\n",
    "            pred = xGb_cl.predict(X_test_f)\n",
    "            pred_train = xGb_cl.predict(X_train_f)\n",
    "            pred_unseen = xGb_cl.predict(df_test[ft])\n",
    "            #print(\"Fold accuracy score: \",accuracy_score(y_test, pred))\n",
    "            accur_scores.append(accuracy_score(y_test, pred))\n",
    "            accur_scores_train.append(accuracy_score(y_train, pred_train))\n",
    "            accur_scores_unseen.append(accuracy_score(target_test, pred_unseen))\n",
    "    #print(all_features)\n",
    "    accur_scores = sum(accur_scores) / len(accur_scores)\n",
    "    accur_scores_train = sum(accur_scores_train) / len(accur_scores_train)\n",
    "    accur_scores_unseen = sum(accur_scores_unseen) / len(accur_scores_unseen)\n",
    "    #print(\"Avg k-fold:{} for :{} features\".format(accur_scores, number_of_features))\n",
    "    #plt.figure(num=None, figsize=(60, 60))\n",
    "    #list_features = list(set(all_features))\n",
    "    #print(len(list_features))\n",
    "    #print(len(ft))\n",
    "    \n",
    "    #count_features = [all_features.count(f) for f in list_features]\n",
    "    #print(count_features)\n",
    "    #plt.bar(range(len(list_features)), count_features)\n",
    "    #plt.xticks(range(len(list_features)), list_features, rotation=45)\n",
    "    #plt.savefig(\"features.png\",dpi=600)\n",
    "\n",
    "    #after k-folds keep all selected features and make 70/30 stratified split and train and predict\n",
    "    all_features = list(set(all_features))\n",
    "    #print(\"70/30\")\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[all_features], \n",
    "                                                        target,\n",
    "                                                        test_size=0.3,\n",
    "                                                        stratify=target)\n",
    "\n",
    "    xGb_cl = XGBClassifier(objective=\"multi:softprob\",\n",
    "                           num_class = 2,\n",
    "                           learning_rate =0.2,\n",
    "                           n_estimators=100,\n",
    "                           max_depth=5,\n",
    "                           colsample_bytree = 0.7,\n",
    "                           eval_metric=\"aucpr\",#\"rmse\",\n",
    "                           use_label_encoder=False)\n",
    "\n",
    "    xGb_cl.fit(X_train, y_train)\n",
    "    pred = xGb_cl.predict(X_test)\n",
    "    print(\"Final accuracy score:\",accuracy_score(y_test, pred),\"\\n--------------\\n\")\n",
    "    return accur_scores, accuracy_score(y_test, pred)\n",
    "    \"\"\"\n",
    "    return accur_scores, accur_scores_train, accur_scores_unseen\n",
    "\n",
    "#k_fold_val_2(number_of_features=80,n_rep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9308 avg_train:0.9502238095238096 avg unseen:0.9294763513513514\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9310031746031745 avg_train:0.9505333333333333 avg unseen:0.9294031531531534\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9310349206349207 avg_train:0.9504587301587302 avg unseen:0.9290033783783782\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9305079365079366 avg_train:0.9505904761904763 avg unseen:0.9289414414414413\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9314857142857145 avg_train:0.9507761904761903 avg unseen:0.929222972972973\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.931015873015873 avg_train:0.9513126984126985 avg unseen:0.9293693693693695\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9323365079365079 avg_train:0.961347619047619 avg unseen:0.9306249999999999\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9324952380952382 avg_train:0.9616238095238097 avg unseen:0.930501126126126\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9323238095238094 avg_train:0.9620460317460319 avg unseen:0.9300619369369368\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9329079365079366 avg_train:0.9620555555555557 avg unseen:0.930213963963964\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9328317460317459 avg_train:0.9623523809523808 avg unseen:0.9302477477477478\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9323492063492064 avg_train:0.9619761904761907 avg unseen:0.9297072072072073\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.933568253968254 avg_train:0.9718841269841269 avg unseen:0.9307319819819818\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9336952380952382 avg_train:0.9724111111111111 avg unseen:0.9312331081081083\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9344571428571428 avg_train:0.9726619047619046 avg unseen:0.9308952702702702\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9338666666666668 avg_train:0.972904761904762 avg unseen:0.930641891891892\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9334730158730157 avg_train:0.9730015873015874 avg unseen:0.930563063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9338158730158731 avg_train:0.972452380952381 avg unseen:0.9301520270270269\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9320698412698413 avg_train:0.9556539682539681 avg unseen:0.9303322072072072\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9320380952380951 avg_train:0.9555095238095238 avg unseen:0.9300506756756756\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9322984126984127 avg_train:0.9563857142857143 avg unseen:0.9304391891891891\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9323555555555556 avg_train:0.9557650793650795 avg unseen:0.9302533783783783\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9327428571428572 avg_train:0.9565285714285716 avg unseen:0.9303997747747745\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9326349206349206 avg_train:0.9562904761904761 avg unseen:0.929774774774775\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9334031746031747 avg_train:0.9669285714285714 avg unseen:0.9309121621621621\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9336444444444445 avg_train:0.9671841269841269 avg unseen:0.930884009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9337777777777778 avg_train:0.9677666666666666 avg unseen:0.93106981981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.933384126984127 avg_train:0.9680857142857144 avg unseen:0.9313288288288287\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9335619047619047 avg_train:0.9681333333333333 avg unseen:0.9308445945945945\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.933384126984127 avg_train:0.9682015873015873 avg unseen:0.9304842342342342\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9343555555555555 avg_train:0.9778158730158731 avg unseen:0.9311430180180181\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346793650793651 avg_train:0.9781444444444443 avg unseen:0.9311148648648647\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342349206349206 avg_train:0.9784603174603176 avg unseen:0.930822072072072\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9345968253968254 avg_train:0.9787301587301589 avg unseen:0.930518018018018\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9347809523809523 avg_train:0.9789380952380954 avg unseen:0.9303096846846848\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349015873015873 avg_train:0.9791095238095237 avg unseen:0.9304504504504506\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9330920634920635 avg_train:0.963736507936508 avg unseen:0.9310810810810811\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.933638095238095 avg_train:0.9638666666666665 avg unseen:0.9309009009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9338857142857142 avg_train:0.9640857142857141 avg unseen:0.9306644144144144\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9337777777777778 avg_train:0.9641650793650792 avg unseen:0.9306024774774774\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9336317460317461 avg_train:0.9644412698412698 avg unseen:0.9308389639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9339936507936507 avg_train:0.9645396825396825 avg unseen:0.9300337837837839\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9346539682539681 avg_train:0.975831746031746 avg unseen:0.9314695945945947\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345333333333334 avg_train:0.9761174603174604 avg unseen:0.9306081081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350158730158729 avg_train:0.9764777777777778 avg unseen:0.9309346846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9347619047619049 avg_train:0.9769920634920634 avg unseen:0.9309515765765766\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9347809523809523 avg_train:0.9768873015873014 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.977268253968254 avg unseen:0.9309853603603603\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349460317460317 avg_train:0.9877650793650793 avg unseen:0.9309459459459459\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.934768253968254 avg_train:0.9881444444444445 avg unseen:0.9314583333333332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350603174603175 avg_train:0.9882730158730157 avg unseen:0.9306925675675676\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348634920634922 avg_train:0.9882904761904762 avg unseen:0.9306756756756757\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934615873015873 avg_train:0.9884174603174603 avg unseen:0.930259009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348825396825396 avg_train:0.9887396825396827 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9341523809523808 avg_train:0.9668412698412698 avg unseen:0.9311373873873876\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.933815873015873 avg_train:0.9673873015873015 avg unseen:0.9310641891891892\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9336761904761904 avg_train:0.9672920634920635 avg unseen:0.9306475225225225\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9342603174603176 avg_train:0.9676333333333332 avg unseen:0.9305855855855857\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9335936507936508 avg_train:0.967331746031746 avg unseen:0.929954954954955\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343365079365079 avg_train:0.9676190476190477 avg unseen:0.9302083333333333\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9346476190476191 avg_train:0.9800730158730158 avg unseen:0.9314977477477476\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348952380952381 avg_train:0.9801761904761904 avg unseen:0.9309346846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9346730158730159 avg_train:0.9804301587301586 avg unseen:0.9306081081081081\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.934736507936508 avg_train:0.9805587301587301 avg unseen:0.9307432432432431\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9352825396825396 avg_train:0.9806809523809524 avg unseen:0.930884009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347492063492063 avg_train:0.9808285714285714 avg unseen:0.9303378378378379\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348507936507938 avg_train:0.9917952380952381 avg unseen:0.9310022522522523\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348380952380951 avg_train:0.991795238095238 avg unseen:0.9313738738738738\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9351809523809523 avg_train:0.9921000000000001 avg unseen:0.9306362612612612\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9347809523809524 avg_train:0.9922476190476189 avg unseen:0.9307545045045046\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348952380952381 avg_train:0.9921539682539683 avg unseen:0.9305518018018016\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934736507936508 avg_train:0.9920777777777778 avg unseen:0.9299211711711711\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9339428571428572 avg_train:0.9695428571428572 avg unseen:0.9307939189189188\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9342857142857142 avg_train:0.9704317460317461 avg unseen:0.931463963963964\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9343746031746033 avg_train:0.9703111111111111 avg unseen:0.9306587837837837\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9342666666666668 avg_train:0.9709603174603175 avg unseen:0.9310585585585587\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934679365079365 avg_train:0.9707825396825395 avg unseen:0.9303772522522521\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9346031746031745 avg_train:0.9708619047619047 avg unseen:0.9306081081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9344444444444444 avg_train:0.9831920634920636 avg unseen:0.9308727477477476\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9352190476190478 avg_train:0.9840380952380953 avg unseen:0.9310585585585587\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349714285714286 avg_train:0.9841238095238095 avg unseen:0.9309459459459459\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9353587301587302 avg_train:0.9844317460317461 avg unseen:0.9312556306306309\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353968253968253 avg_train:0.9846015873015874 avg unseen:0.9308277027027027\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9342349206349206 avg_train:0.984690476190476 avg unseen:0.9302083333333332\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9353714285714286 avg_train:0.9943857142857143 avg unseen:0.9308389639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9352571428571429 avg_train:0.994773015873016 avg unseen:0.9308051801801803\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9348698412698413 avg_train:0.994379365079365 avg unseen:0.9300844594594594\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9355428571428573 avg_train:0.9947095238095237 avg unseen:0.930563063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9351301587301588 avg_train:0.9947523809523809 avg unseen:0.9302702702702702\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349206349206348 avg_train:0.9947174603174602 avg unseen:0.9300506756756755\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9314158730158731 avg_train:0.9528222222222221 avg unseen:0.9302927927927929\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9312571428571427 avg_train:0.9528825396825397 avg unseen:0.9300225225225226\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9312952380952382 avg_train:0.9530936507936509 avg unseen:0.9295326576576577\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9315873015873016 avg_train:0.9533253968253966 avg unseen:0.9297522522522522\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9313904761904762 avg_train:0.9534492063492062 avg unseen:0.9294481981981981\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.931511111111111 avg_train:0.953388888888889 avg unseen:0.929543918918919\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9325714285714284 avg_train:0.9640920634920636 avg unseen:0.93080518018018\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9327111111111109 avg_train:0.9645936507936508 avg unseen:0.9307713963963963\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9331111111111111 avg_train:0.9646936507936509 avg unseen:0.930793918918919\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9327174603174603 avg_train:0.9650714285714285 avg unseen:0.9305743243243242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9332952380952381 avg_train:0.9651238095238094 avg unseen:0.9302252252252251\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9332444444444445 avg_train:0.9651460317460318 avg unseen:0.9305461711711711\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9339428571428571 avg_train:0.9748714285714285 avg unseen:0.9312331081081081\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9337968253968253 avg_train:0.9750698412698412 avg unseen:0.9308277027027027\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9340000000000002 avg_train:0.9752238095238095 avg unseen:0.9312162162162164\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9333079365079365 avg_train:0.9751698412698412 avg unseen:0.9305855855855856\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9339365079365077 avg_train:0.975590476190476 avg unseen:0.9311148648648649\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9341079365079364 avg_train:0.9759111111111111 avg unseen:0.9306306306306306\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9326793650793651 avg_train:0.9583984126984126 avg unseen:0.9308502252252252\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.932368253968254 avg_train:0.9584380952380952 avg unseen:0.9307601351351352\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9329650793650793 avg_train:0.9584777777777777 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9334603174603174 avg_train:0.9587587301587303 avg unseen:0.9306137387387386\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9328825396825399 avg_train:0.9586317460317462 avg unseen:0.9303096846846848\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.932730158730159 avg_train:0.9588761904761905 avg unseen:0.9304222972972974\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9344380952380952 avg_train:0.9697015873015874 avg unseen:0.9310810810810811\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9339174603174603 avg_train:0.9704682539682542 avg unseen:0.9311373873873876\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9345714285714286 avg_train:0.9704380952380953 avg unseen:0.9311204954954954\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9340380952380952 avg_train:0.9707063492063494 avg unseen:0.9308445945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9337841269841269 avg_train:0.9704761904761906 avg unseen:0.930275900900901\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934279365079365 avg_train:0.9710349206349207 avg unseen:0.930867117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9344190476190476 avg_train:0.9809365079365079 avg unseen:0.9312049549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348000000000001 avg_train:0.981347619047619 avg unseen:0.9316216216216215\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350285714285714 avg_train:0.9818936507936509 avg unseen:0.9311486486486489\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9350412698412699 avg_train:0.9819539682539681 avg unseen:0.9307995495495497\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9349333333333334 avg_train:0.9820888888888889 avg unseen:0.9306644144144145\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.935231746031746 avg_train:0.9826 avg unseen:0.9305349099099098\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9339174603174603 avg_train:0.9663603174603175 avg unseen:0.9308558558558557\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.9670634920634922 avg unseen:0.9313682432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9340761904761905 avg_train:0.9670968253968254 avg unseen:0.9305799549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9337587301587301 avg_train:0.9670539682539682 avg unseen:0.9305799549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9342222222222222 avg_train:0.9671841269841269 avg unseen:0.9303716216216218\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9341650793650794 avg_train:0.9671539682539683 avg unseen:0.9303322072072072\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349269841269842 avg_train:0.9794746031746033 avg unseen:0.9310867117117115\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345142857142857 avg_train:0.9798253968253969 avg unseen:0.9311711711711711\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934488888888889 avg_train:0.9799285714285716 avg unseen:0.9307488738738741\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9349714285714286 avg_train:0.9801412698412698 avg unseen:0.9305349099099098\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9352190476190475 avg_train:0.9807920634920634 avg unseen:0.931126126126126\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347301587301585 avg_train:0.9805904761904761 avg unseen:0.9302477477477477\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9351428571428572 avg_train:0.9913841269841269 avg unseen:0.9311261261261262\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9355619047619047 avg_train:0.9911714285714287 avg unseen:0.9310698198198198\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349396825396825 avg_train:0.991331746031746 avg unseen:0.9306981981981983\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9354920634920635 avg_train:0.9913809523809523 avg unseen:0.9302984234234234\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348063492063492 avg_train:0.9919190476190476 avg unseen:0.9307038288288286\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9352 avg_train:0.9921714285714286 avg unseen:0.9302533783783784\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9343174603174603 avg_train:0.9697396825396827 avg unseen:0.9305518018018017\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349333333333334 avg_train:0.970257142857143 avg unseen:0.9311711711711713\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9347365079365079 avg_train:0.9706317460317461 avg unseen:0.9310472972972973\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.933968253968254 avg_train:0.9708396825396827 avg unseen:0.930625\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9346539682539682 avg_train:0.9711000000000002 avg unseen:0.9305743243243243\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9340317460317461 avg_train:0.9709253968253967 avg unseen:0.930748873873874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349142857142857 avg_train:0.9834349206349207 avg unseen:0.9305405405405405\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9351492063492062 avg_train:0.9836904761904762 avg unseen:0.9307319819819818\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350920634920635 avg_train:0.984115873015873 avg unseen:0.9302984234234234\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9350412698412699 avg_train:0.9842206349206349 avg unseen:0.9303772522522523\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353142857142857 avg_train:0.9849190476190478 avg unseen:0.9306531531531532\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934152380952381 avg_train:0.984952380952381 avg unseen:0.9303153153153154\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349841269841269 avg_train:0.9944349206349208 avg unseen:0.930625\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345904761904761 avg_train:0.9946809523809522 avg unseen:0.9303603603603604\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349142857142857 avg_train:0.9944666666666665 avg unseen:0.9307882882882882\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.935288888888889 avg_train:0.9948873015873017 avg unseen:0.9304617117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353206349206351 avg_train:0.9947539682539682 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9351365079365079 avg_train:0.9949555555555555 avg unseen:0.9305123873873874\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9347555555555555 avg_train:0.9730873015873017 avg unseen:0.9310585585585587\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9339809523809524 avg_train:0.9732444444444445 avg unseen:0.930242117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9345206349206349 avg_train:0.9737984126984127 avg unseen:0.9307432432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348126984126984 avg_train:0.9738301587301589 avg unseen:0.9302139639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348444444444445 avg_train:0.9744126984126984 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934368253968254 avg_train:0.9745460317460317 avg unseen:0.9307432432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9347873015873015 avg_train:0.9868809523809522 avg unseen:0.9305123873873874\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.934920634920635 avg_train:0.9874682539682539 avg unseen:0.9304954954954954\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9347936507936508 avg_train:0.9877333333333332 avg unseen:0.9306587837837837\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9353650793650793 avg_train:0.9881523809523808 avg unseen:0.9305292792792793\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348380952380952 avg_train:0.9882555555555556 avg unseen:0.9305349099099102\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934679365079365 avg_train:0.988422222222222 avg unseen:0.930394144144144\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9352444444444443 avg_train:0.99674126984127 avg unseen:0.9306813063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349968253968254 avg_train:0.9967761904761904 avg unseen:0.9304729729729729\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9353650793650795 avg_train:0.9967460317460318 avg unseen:0.9309290540540541\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9351746031746032 avg_train:0.9968380952380954 avg unseen:0.9303997747747749\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353904761904762 avg_train:0.9969873015873015 avg unseen:0.930365990990991\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9352888888888887 avg_train:0.9971190476190476 avg unseen:0.9297184684684684\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9315619047619047 avg_train:0.9546777777777778 avg unseen:0.9299493243243244\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9316888888888889 avg_train:0.9550698412698412 avg unseen:0.9301520270270269\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9322285714285713 avg_train:0.955347619047619 avg unseen:0.9299380630630631\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9316253968253968 avg_train:0.9551047619047619 avg unseen:0.9295213963963965\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9324634920634921 avg_train:0.9559873015873014 avg unseen:0.9302364864864865\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.932152380952381 avg_train:0.9560555555555555 avg unseen:0.9303265765765767\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9334285714285713 avg_train:0.9667412698412697 avg unseen:0.9310135135135135\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.933606349206349 avg_train:0.966968253968254 avg unseen:0.9308614864864865\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9336253968253967 avg_train:0.9669619047619046 avg unseen:0.930867117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9336698412698411 avg_train:0.9669206349206348 avg unseen:0.9305236486486487\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9334222222222223 avg_train:0.9673603174603175 avg unseen:0.9307770270270271\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9336126984126984 avg_train:0.9676634920634921 avg unseen:0.93053490990991\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9337396825396825 avg_train:0.9773714285714288 avg unseen:0.9309403153153153\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9340444444444445 avg_train:0.9773 avg unseen:0.9309346846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934095238095238 avg_train:0.9778190476190476 avg unseen:0.9307826576576576\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9340190476190476 avg_train:0.9777857142857144 avg unseen:0.9308445945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344444444444444 avg_train:0.9781698412698412 avg unseen:0.9306475225225225\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343365079365079 avg_train:0.9783301587301588 avg unseen:0.930242117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9331873015873015 avg_train:0.9607492063492064 avg unseen:0.9310191441441441\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9331428571428573 avg_train:0.9609777777777778 avg unseen:0.9305461711711712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9330666666666665 avg_train:0.9608222222222222 avg unseen:0.9305799549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9332444444444444 avg_train:0.9611936507936508 avg unseen:0.9305855855855856\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9334539682539682 avg_train:0.9612603174603175 avg unseen:0.9306193693693693\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9333650793650795 avg_train:0.9614126984126985 avg unseen:0.9303828828828827\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9337460317460318 avg_train:0.9723317460317461 avg unseen:0.9309515765765766\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9337142857142856 avg_train:0.9731285714285715 avg unseen:0.9307713963963965\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342158730158729 avg_train:0.9731015873015876 avg unseen:0.9308783783783784\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.9734365079365079 avg unseen:0.9304391891891893\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9342857142857142 avg_train:0.9733380952380952 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9339238095238094 avg_train:0.9734587301587303 avg unseen:0.9304448198198196\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342984126984124 avg_train:0.9842730158730157 avg unseen:0.9311430180180181\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9341269841269842 avg_train:0.9843746031746033 avg unseen:0.9309628378378378\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.93455873015873 avg_train:0.9846428571428572 avg unseen:0.9313738738738738\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9353396825396826 avg_train:0.985 avg unseen:0.9307488738738737\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9346539682539682 avg_train:0.9848238095238094 avg unseen:0.9306193693693695\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347301587301587 avg_train:0.9855428571428569 avg unseen:0.9303603603603604\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9341206349206349 avg_train:0.9691682539682539 avg unseen:0.931131756756757\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9344317460317461 avg_train:0.9695714285714286 avg unseen:0.9308389639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9341523809523808 avg_train:0.9697920634920636 avg unseen:0.930974099099099\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9339873015873016 avg_train:0.9701126984126985 avg unseen:0.9304786036036035\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9339111111111111 avg_train:0.9700555555555556 avg unseen:0.9302927927927929\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9350539682539681 avg_train:0.9706111111111113 avg unseen:0.9307826576576576\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9345269841269842 avg_train:0.9827492063492063 avg unseen:0.9306024774774775\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348380952380951 avg_train:0.9827904761904763 avg unseen:0.930670045045045\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934825396825397 avg_train:0.9834809523809525 avg unseen:0.9306813063063064\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9350920634920636 avg_train:0.9836238095238096 avg unseen:0.9304279279279278\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9346095238095239 avg_train:0.9834984126984126 avg unseen:0.9305461711711712\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343365079365081 avg_train:0.9840079365079365 avg unseen:0.9304786036036035\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9351492063492065 avg_train:0.993725396825397 avg unseen:0.9307770270270271\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345523809523811 avg_train:0.9938666666666668 avg unseen:0.9305630630630631\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9351492063492063 avg_train:0.9941285714285715 avg unseen:0.9306249999999999\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9354222222222223 avg_train:0.9941253968253968 avg unseen:0.9303997747747746\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344952380952379 avg_train:0.9941238095238095 avg unseen:0.9301858108108109\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347809523809523 avg_train:0.9943507936507938 avg unseen:0.9299268018018019\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342539682539682 avg_train:0.9727857142857145 avg unseen:0.9304842342342344\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345396825396826 avg_train:0.9732253968253968 avg unseen:0.9310022522522522\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9345904761904761 avg_train:0.9735380952380952 avg unseen:0.9307094594594593\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9343111111111111 avg_train:0.9737936507936508 avg unseen:0.9303716216216215\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9349015873015875 avg_train:0.9739873015873016 avg unseen:0.9302533783783783\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348190476190478 avg_train:0.974236507936508 avg unseen:0.9304842342342342\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9351555555555556 avg_train:0.9868825396825396 avg unseen:0.9307376126126125\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349269841269842 avg_train:0.9872285714285715 avg unseen:0.9309403153153154\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9346095238095238 avg_train:0.9872190476190476 avg unseen:0.9299662162162162\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9354285714285714 avg_train:0.9881349206349205 avg unseen:0.930962837837838\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.935377777777778 avg_train:0.9878301587301589 avg unseen:0.9302195945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343492063492064 avg_train:0.9878809523809524 avg unseen:0.9300844594594594\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349650793650793 avg_train:0.9964873015873016 avg unseen:0.9306193693693694\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9353460317460317 avg_train:0.9966777777777778 avg unseen:0.930731981981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9353206349206348 avg_train:0.996742857142857 avg unseen:0.9307432432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.935657142857143 avg_train:0.9967746031746031 avg unseen:0.9307376126126128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934647619047619 avg_train:0.9967158730158728 avg unseen:0.9301351351351351\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348063492063491 avg_train:0.9971412698412698 avg unseen:0.9300619369369368\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9340634920634922 avg_train:0.9761650793650795 avg unseen:0.9307770270270268\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346412698412698 avg_train:0.9764365079365079 avg unseen:0.9310472972972974\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.9770571428571428 avg unseen:0.9307150900900901\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9343809523809524 avg_train:0.9771444444444445 avg unseen:0.9306531531531532\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344317460317461 avg_train:0.9774111111111111 avg unseen:0.9301069819819819\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347174603174603 avg_train:0.9775444444444447 avg unseen:0.9302759009009008\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.934247619047619 avg_train:0.9906190476190476 avg unseen:0.9308952702702703\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9356253968253968 avg_train:0.9909142857142857 avg unseen:0.9309177927927929\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342857142857142 avg_train:0.9911222222222221 avg unseen:0.9303153153153151\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9346730158730159 avg_train:0.9911825396825394 avg unseen:0.9300788288288286\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348444444444445 avg_train:0.991309523809524 avg unseen:0.9302646396396396\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348126984126984 avg_train:0.9916396825396826 avg unseen:0.9301745495495496\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9354666666666667 avg_train:0.9983031746031746 avg unseen:0.9309346846846845\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9350730158730158 avg_train:0.9982206349206347 avg unseen:0.930579954954955\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349079365079365 avg_train:0.9986222222222222 avg unseen:0.930382882882883\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9352317460317461 avg_train:0.9985682539682541 avg unseen:0.9305405405405406\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9350603174603173 avg_train:0.9984333333333333 avg unseen:0.9300168918918917\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9359555555555555 avg_train:0.9985968253968255 avg unseen:0.9301126126126128\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9321968253968252 avg_train:0.9571476190476192 avg unseen:0.930563063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9321015873015872 avg_train:0.9568412698412699 avg unseen:0.9303603603603604\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9325904761904762 avg_train:0.9574095238095238 avg unseen:0.9300731981981981\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9328952380952383 avg_train:0.9577301587301587 avg unseen:0.9301632882882882\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9332444444444444 avg_train:0.9580555555555553 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9329650793650792 avg_train:0.957595238095238 avg unseen:0.9303772522522523\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9331111111111111 avg_train:0.9688507936507936 avg unseen:0.9313907657657657\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9336190476190476 avg_train:0.9690079365079365 avg unseen:0.9311148648648649\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9343555555555557 avg_train:0.9691142857142857 avg unseen:0.9311092342342343\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9331047619047618 avg_train:0.9690539682539684 avg unseen:0.9308502252252252\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9338285714285713 avg_train:0.9697873015873016 avg unseen:0.930670045045045\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9338603174603174 avg_train:0.9697047619047622 avg unseen:0.9303096846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342349206349208 avg_train:0.9798349206349206 avg unseen:0.9307545045045046\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346920634920636 avg_train:0.9806079365079364 avg unseen:0.9311148648648647\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9347492063492066 avg_train:0.9803476190476189 avg unseen:0.9308445945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9344253968253969 avg_train:0.9806507936507934 avg unseen:0.9307094594594595\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9343746031746032 avg_train:0.9807301587301587 avg unseen:0.9309459459459458\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9341015873015873 avg_train:0.9805047619047618 avg unseen:0.9301914414414415\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9330666666666666 avg_train:0.9626746031746033 avg unseen:0.930748873873874\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9335809523809524 avg_train:0.9632587301587301 avg unseen:0.9309572072072071\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9335809523809523 avg_train:0.9630793650793651 avg unseen:0.9304391891891891\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9340444444444445 avg_train:0.9635984126984128 avg unseen:0.9303885135135136\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934279365079365 avg_train:0.9636238095238094 avg unseen:0.9306418918918918\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9332507936507937 avg_train:0.9633650793650791 avg unseen:0.93\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9343174603174603 avg_train:0.9748412698412698 avg unseen:0.9306081081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346031746031745 avg_train:0.9754698412698413 avg unseen:0.9308952702702703\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9348126984126984 avg_train:0.975915873015873 avg unseen:0.9309009009009008\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9342730158730159 avg_train:0.9759015873015873 avg unseen:0.9305461711711712\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9345142857142856 avg_train:0.9762809523809522 avg unseen:0.9303603603603602\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9344063492063492 avg_train:0.9758968253968252 avg unseen:0.9304842342342342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9354031746031746 avg_train:0.9870555555555556 avg unseen:0.9313288288288288\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9343619047619048 avg_train:0.9871555555555555 avg unseen:0.9310529279279279\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9348253968253969 avg_train:0.9873920634920634 avg unseen:0.9310416666666664\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9346920634920634 avg_train:0.9877507936507937 avg unseen:0.9308277027027027\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9347365079365079 avg_train:0.9874666666666666 avg unseen:0.9299099099099098\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349841269841269 avg_train:0.988088888888889 avg unseen:0.9302927927927928\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348507936507936 avg_train:0.9717285714285714 avg unseen:0.9312049549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9342349206349206 avg_train:0.972031746031746 avg unseen:0.9303322072072073\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342158730158729 avg_train:0.9721603174603176 avg unseen:0.9308333333333333\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9344380952380953 avg_train:0.9724714285714285 avg unseen:0.9302533783783783\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.93455873015873 avg_train:0.9729650793650794 avg unseen:0.9306137387387391\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9339301587301586 avg_train:0.9731031746031745 avg unseen:0.9303603603603605\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342666666666667 avg_train:0.9853206349206348 avg unseen:0.9300506756756756\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346730158730159 avg_train:0.9860492063492065 avg unseen:0.9312049549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342285714285713 avg_train:0.9859206349206348 avg unseen:0.9302195945945945\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9346539682539684 avg_train:0.9860968253968254 avg unseen:0.9303490990990991\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9345015873015875 avg_train:0.9867222222222223 avg unseen:0.9304222972972974\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349142857142859 avg_train:0.9869015873015874 avg unseen:0.9299831081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348317460317461 avg_train:0.9955650793650794 avg unseen:0.9305855855855857\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.935263492063492 avg_train:0.9959412698412698 avg unseen:0.9307601351351352\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349206349206348 avg_train:0.9959539682539682 avg unseen:0.9307263513513512\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348126984126983 avg_train:0.9959698412698413 avg unseen:0.9299436936936937\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.935047619047619 avg_train:0.9962984126984127 avg unseen:0.930304054054054\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.935015873015873 avg_train:0.9963333333333333 avg unseen:0.9299493243243243\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9347238095238096 avg_train:0.975752380952381 avg unseen:0.9310135135135132\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9344063492063492 avg_train:0.9757761904761905 avg unseen:0.9304842342342341\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934584126984127 avg_train:0.976020634920635 avg unseen:0.9303603603603602\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9338857142857143 avg_train:0.9761809523809525 avg unseen:0.9302702702702702\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9349015873015873 avg_train:0.9763793650793651 avg unseen:0.9303716216216218\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9350412698412697 avg_train:0.977115873015873 avg unseen:0.9303209459459459\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349396825396826 avg_train:0.9897999999999999 avg unseen:0.9302308558558559\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349269841269842 avg_train:0.9903460317460316 avg unseen:0.931097972972973\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9352698412698411 avg_train:0.990311111111111 avg unseen:0.9306362612612613\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9349841269841269 avg_train:0.9903126984126983 avg unseen:0.9307545045045044\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344888888888888 avg_train:0.9909301587301588 avg unseen:0.9299887387387388\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9344571428571428 avg_train:0.9908904761904763 avg unseen:0.9299718468468469\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348825396825396 avg_train:0.9980079365079365 avg unseen:0.930304054054054\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9350539682539681 avg_train:0.9982507936507939 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9352507936507936 avg_train:0.9981444444444444 avg unseen:0.9304729729729729\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348761904761905 avg_train:0.9983253968253966 avg unseen:0.9306081081081081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-0382528d26c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mf_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                             k_accuracy, k_accuracy_train, k_accuracy_unseen = k_fold_val_2(number_of_features=45, \n\u001b[0m\u001b[1;32m     58\u001b[0m                                                                         \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                                                        \u001b[0mnum_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-3b1508eecbd1>\u001b[0m in \u001b[0;36mk_fold_val_2\u001b[0;34m(number_of_features, objective, num_class, learning_rate, n_estimators, max_depth, colsample_bytree, eval_metric)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m#select K features with use of random forest feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mX_train_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrforest_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m#X_train_f, ft = lasso(X_train, y_train, alpha=number_of_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dd238dfa3f68>\u001b[0m in \u001b[0;36mrforest_features\u001b[0;34m(data, labels, n_features)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0membeded_rf_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0membeded_rf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0membeded_rf_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeded_rf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \"Since 'prefit=True', call transform directly\")\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "#good one \n",
    "#------------------------------------------\n",
    "#LR: 0.1\n",
    "#nEst: 150\n",
    "#m_depth: 7\n",
    "#bytree: 0.7\n",
    "#ev_m: aucpr score avg:0.9394461205162186\n",
    "#-----------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "learning_rate =[0.1,0.2,0.3,0.4]\n",
    "n_estimators=[50,100, 150,200,250,300,350]\n",
    "max_depth=[1,2,3,4,5,6,7,8,9,10,15,20]\n",
    "colsample_bytree = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "eval_metric=[\"rmse\",\"auc\"]\n",
    "\"\"\"\n",
    "\n",
    "#print(df.shape)\n",
    "values = list(np.arange(0.00080, 0.002, 0.00001))\n",
    "values=list(range(40,50))\n",
    "#X,ft = lasso(df, target, alpha=0.02)\n",
    "#print(len(ft))\n",
    "\n",
    "#plt.figure(figsize=(8.0,8.0))\n",
    "k_scores = defaultdict(lambda: 0)\n",
    "\n",
    "objective=[\"multi:softprob\",\"multi:softprob\",\"reg:squarederror\",\"reg:squaredlogerror\", \n",
    "           \"binary:logistic\",\"binary:logitraw\",\"binary:hinge\"]\n",
    "learning_rate =[0.08, 0.1, 0.13]\n",
    "n_estimators=[150, 200, 250,300]\n",
    "max_depth=[5,6,7,8]\n",
    "colsample_bytree = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\n",
    "\n",
    "\n",
    "objective=[\"multi:softprob\"]#,\"multi:softmax\"]\n",
    "learning_rate =[0.07,0.08,0.09]\n",
    "n_estimators=[150,200,300,350]\n",
    "max_depth=[4,5,6]\n",
    "colsample_bytree = [0.45,0.5,0.55,0.6,0.65]\n",
    "\n",
    "eval_metric=[\"aucpr\"]\n",
    "for obj in objective:\n",
    "    for lr in learning_rate:\n",
    "        for n_est in n_estimators:\n",
    "            for m_depth in max_depth:\n",
    "                for bytree in colsample_bytree:\n",
    "                    \n",
    "                    for ev_m in eval_metric:\n",
    "                        k_scores = []\n",
    "                        k_scores_train = []\n",
    "                        k_scores_unseen = []\n",
    "                        f_scores = []\n",
    "                        for rnd in range(3):\n",
    "                            k_accuracy, k_accuracy_train, k_accuracy_unseen = k_fold_val_2(number_of_features=45, \n",
    "                                                                        objective=obj,\n",
    "                                                                       num_class = 2,\n",
    "                                                                       learning_rate =lr,\n",
    "                                                                       n_estimators=n_est,\n",
    "                                                                       max_depth=m_depth,\n",
    "                                                                       colsample_bytree = bytree,\n",
    "                                                                       eval_metric=ev_m)\n",
    "                            #k_accuracy, full_accuracy = k_fold_val_2(number_of_features=alpha)\n",
    "                            k_scores.append(k_accuracy)\n",
    "                            k_scores_train.append(k_accuracy_train)\n",
    "                            k_scores_unseen.append(k_accuracy_unseen)\n",
    "                        \n",
    "                        #f_scores.append(full_accuracy)\n",
    "                        f_out = open(\"param_ftune.txt\",\"a+\")\n",
    "                        f_out.write(\"Obj:{}\\nLR: {}\\nnEst: {}\\nm_depth: {}\\nbytree: {}\\nev_m: {} score avg:{} score train:{} avg unseen:{}\\n------------\\n\".format(\n",
    "                                            obj,\n",
    "                                            lr,\n",
    "                                            n_est, \n",
    "                                            m_depth,\n",
    "                                            bytree, \n",
    "                                            ev_m, \n",
    "                                            sum(k_scores)/len(k_scores),\n",
    "                                            sum(k_scores_train)/len(k_scores_train),\n",
    "                                            sum(k_scores_unseen)/len(k_scores_unseen)) )\n",
    "                        f_out.close()\n",
    "\n",
    "                        #if sum(k_scores)/len(k_scores) >= 0.938:\n",
    "                        print(\"Obj:{}\\nLR: {}\\nnEst: {}\\nm_depth: {}\\nbytree: {}\\nev_m: {} score avg_test:{} avg_train:{} avg unseen:{}\".format(\n",
    "                                            obj,\n",
    "                                            lr,\n",
    "                                            n_est, \n",
    "                                            m_depth,\n",
    "                                            bytree, \n",
    "                                            ev_m, \n",
    "                                            sum(k_scores)/len(k_scores),\n",
    "                                            sum(k_scores_train)/len(k_scores_train),\n",
    "                                            sum(k_scores_unseen)/len(k_scores_unseen)) )\n",
    "\n",
    "#res = []\n",
    "#for alpha in values:\n",
    "#    res.append(k_scores[alpha] / 10)\n",
    "#plt.plot(values, res, label=\"K-Fold\")\n",
    "#plt.plot(values,f_scores, label=\"Full Split 70/30\")\n",
    "#plt.legend()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.savefig(\"Lasso_feature.png\",dpi=600)\n",
    "\n",
    "\n",
    "#features = list(range(2,167))\n",
    "#accuracy_fold = []\n",
    "#accuracy_split = []\n",
    "#for f_number in features:\n",
    "#    acc_s, acc_s_s = k_fold_val_2(number_of_features=f_number,n_rep=10)\n",
    "#    accuracy_fold.append(acc_s)\n",
    "#    accuracy_split.append(acc_s_s)\n",
    "#plt.plot(features, accuracy_fold, label=\"K-Fold avg accuracy\")\n",
    "#plt.plot(features, accuracy_split, label=\"70/30 split accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/features_large_lexical.csv\"\n",
    "df, target, df_test,target_test = read_data(filename)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=0,test_size=0.2,stratify=target)\n",
    "\n",
    "\n",
    "objective=\"multi:softprob\"\n",
    "learning_rate =0.07\n",
    "n_estimators=300\n",
    "max_depth=5\n",
    "colsample_bytree = 0.45\n",
    "eval_metric=\"aucpr\"\n",
    "model = XGBClassifier(objective=objective,\n",
    "                                   num_class = 2,\n",
    "                                   learning_rate =learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   eval_metric=eval_metric,#\"rmse\",\n",
    "                                   use_label_encoder=False)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "predictions_unseen = model.predict(df_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_unseen = accuracy_score(target_test, predictions_unseen)\n",
    "print(\"Accuracy test: {} Accuracy unseen: {}\".format(accuracy, accuracy_unseen))\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    # train model\n",
    "    selection_model = XGBClassifier(objective=objective,\n",
    "                                   num_class = num_class,\n",
    "                                   learning_rate =learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   eval_metric=eval_metric,#\"rmse\",\n",
    "                                   use_label_encoder=False)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    # eval model\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    predictions = selection_model.predict(select_X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    select_X_unseen = selection.transform(df_test)\n",
    "    predictions_unseen = selection_model.predict(select_X_unseen)\n",
    "    accuracy_unseen = accuracy_score(target_test, predictions_unseen)\n",
    "    print(\"Thresh={}, n={}, Accuracy test: {} Accuracy unseen: {}\".format(thresh, select_X_train.shape[1], accuracy, accuracy_unseen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9269684290604793\n",
      "Train accuracy 0.9945781413488063\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=0,test_size=0.2)\n",
    "X_train_f, ft = rforest_features(X_train, y_train, 95)\n",
    "#X_train_f, ft = lasso(X_train, y_train, alpha=number_of_features)\n",
    "\n",
    "#for f in ft:\n",
    "#    all_features.append(f)\n",
    "\n",
    "#keep only selected features in train and test splits\n",
    "X_test_f = X_test[ft]\n",
    "\"\"\"\n",
    "#another classifiers for cross validation, that have pour performance till now\n",
    "reg_scores = cross_val_score(LogisticRegression(max_iter=10000,penalty='l1',\n",
    "                                                solver='liblinear',C=1.0), X, target,cv=5)\n",
    "\n",
    "svc_scores = cross_val_score(SVC(kernel='rbf', C=0.9), X, target,cv=5)\n",
    "\n",
    "for_scores = cross_val_score(RandomForestClassifier(n_estimators=35,\n",
    "                                                    criterion='gini',\n",
    "                                                    ccp_alpha=0.0003,\n",
    "                                                    min_samples_split=4), X, target,cv=5)\n",
    "\n",
    "\"\"\"\n",
    "xGb_cl = XGBClassifier(objective=\"multi:softprob\",\n",
    "                               random_state = 10,\n",
    "                               num_class = 2,\n",
    "                               learning_rate =0.02,\n",
    "                               n_estimators=700,\n",
    "                               max_depth=8,\n",
    "                               min_child_weight=1,\n",
    "                               reg_alpha = 0.4,\n",
    "                               #reg_lambda = 1,\n",
    "                               #colsample_bytree = 0.7,\n",
    "                               eval_metric=\"aucpr\",#\"rmse\",\n",
    "                       use_label_encoder=False)\n",
    "\n",
    "xGb_cl.fit(X_train_f, y_train)\n",
    "\n",
    "#predict test dataset and keep prediction labels\n",
    "pred = xGb_cl.predict(X_test_f)\n",
    "pred_train = xGb_cl.predict(X_train_f)\n",
    "#print(\"Fold accuracy score: \",accuracy_score(y_test, pred))\n",
    "print(\"Test accuracy\",accuracy_score(y_test, pred))\n",
    "print(\"Train accuracy\",accuracy_score(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimentionality reduction t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#X, _ = lasso_features(df, target, 4)\n",
    "#X,_ = lasso(df, target, alpha=0.00002)\n",
    "X = rforest_features(df, target, 56)\n",
    "tsne_2d = make_tsne(X, target, \"full dataset\",learning_rate=100, perplexity=20)\n",
    "#tsne_2d = make_tsne(X, target, \"full dataset\")\n",
    "#tsne_2d = make_tsne(X, target, \"full dataset\")\n",
    "#print(\"Plot t-SNE for train dataset with features selected from train portion\")\n",
    "#make_tsne(X_train_scaled, y_train, \"train dataset\")\n",
    "#print(\"Plot t-SNE for test dataset with features selected from train portion\")\n",
    "#make_tsne(X_test_scaled, y_test, \"test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimentionality reduction UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#X, _ = lasso_features(df, target, 4)\n",
    "umap_2d = perofrm_umap_clustering(X, target, \"all dataset\")\n",
    "#embedding = perofrm_umap_clustering(X, target, \"all dataset\")\n",
    "#embedding = perofrm_umap_clustering(X, target, \"all dataset\")\n",
    "#perofrm_umap_clustering(X_test_scaled, y_test,  \"test dataset\")\n",
    "#perofrm_umap_clustering(X_train_scaled, y_train, \"train dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#dbscan_clustering(tsne_2d, 0.00557, 25)\n",
    "#dbscan_clustering(tsne_2d, 0.0045, 25)\n",
    "#dbscan_clustering(tsne_2d, 0.0057, 30)\n",
    "#dbscan_clustering(umap_2d,0.007, 30)\n",
    "#dbscan_clustering(umap_2d,0.0025, 20)\n",
    "dbscan_clustering(umap_2d,0.0025, 20)\n",
    "#dbscan_clustering(X, 0.00557, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "mean_shift(tsne_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "res = cosine_similarity(X,Y= None, dense_output=True)\n",
    "pairs = set()\n",
    "f_out = open(\"cosine_graph.dot\",\"w+\")\n",
    "f_out.write(\"digraph {\\n\")\n",
    "for i in range(0, res.shape[0]):\n",
    "    f_out.write(\"\\t\\\"{}\\\"     [label=\\\"{}\\\"];\\n\".format(i,i))\n",
    "    #    \"1\" -- \"2\" [weight=100];\")\n",
    "for i in range(0,res.shape[0]):\n",
    "    max_sim = res[i,list(set(range(0,39123)) - set([i]))]\n",
    "    max_sim = [x for x in max_sim if x >= 0.9]\n",
    "    if len(max_sim) == 0:\n",
    "        continue\n",
    "    max_sim.sort(reverse=True)\n",
    "    if len(max_sim) > 5:\n",
    "        max_sim = max_sim[:5]\n",
    "    \n",
    "    \n",
    "    #print(max_sim)\n",
    "    #print(np.where(res[1,:] == max_sim))\n",
    "    for m_value in max_sim:\n",
    "        line = \"\\t\\\"{}\\\" -- \\\"{}\\\" [weight={}];\\n\".format(i, np.where(res[i,:] == m_value)[0][0], int(m_value*100.0))\n",
    "        line_rev = \"\\t\\\"{}\\\" -- \\\"{}\\\" [weight={}];\\n\".format(np.where(res[i,:] == m_value)[0][0], i, int(m_value*100.0))\n",
    "        if line in pairs or line_rev in pairs:\n",
    "            continue\n",
    "        pairs.add(line)\n",
    "        #print(line)\n",
    "        f_out.write(line)\n",
    "f_out.write(\"}\")\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(n_components=2,n_init=100,init_params = \"kmeans\",covariance_type = \"full\")\n",
    "#X, _ = lasso_features(df, target, 10)\n",
    "#X = kbest_features(df, target, 3)\n",
    "gmm.fit(X)\n",
    "\n",
    "#predictions from gmm\n",
    "labels = gmm.predict(X)\n",
    "TP = [True if labels[i]==1 and target[i] == 1 else False for i in range(len(target)) ].count(True)\n",
    "TN = [True if labels[i]==0 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FP = [True if labels[i]==1 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FN = [True if labels[i]==0 and target[i] == 1 else False for i in range(len(target)) ].count(True)\n",
    "P = len(target[target == 1])\n",
    "N = len(target[target == 0])\n",
    "print(\"True pos :{} tp rate:{}\".format(TP,TP/P))\n",
    "print(\"True neg :{} tn rate:{}\".format(TN, TN/N))\n",
    "print(\"False pos :{} fp rate:{}\".format(FP,FP/N))\n",
    "print(\"False neg :{} fn rate:{}\".format(FN,FN/P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#X = lasso_features(df, target, 4)\n",
    "#X = rforest_features(df, target, 4)\n",
    "TPR_L = []\n",
    "TNR_L = []\n",
    "FNR_L = []\n",
    "FPR_L = []\n",
    "purity = []\n",
    "#for n_features in range(1,10):\n",
    "for j in range(2,40):\n",
    "    #X = kbest_features(df, target, n_features)\n",
    "    X, _ = lasso_features(df, target, 4)\n",
    "    #X = rforest_features(df, target, 4)\n",
    "   \n",
    "    TPR = []\n",
    "    TNR = []\n",
    "    FPR = []\n",
    "    FNR = []\n",
    "    tmp_pur = []\n",
    "    for k in range(0,10):\n",
    "        gmm = GaussianMixture(n_components=j,init_params = \"kmeans\",covariance_type = \"full\")\n",
    "        gmm.fit(X)\n",
    "\n",
    "        #predictions from gmm\n",
    "        labels = gmm.predict(X)\n",
    "        tmp_pur.append(compute_pur(labels,target))\n",
    "        TP = [True if labels[i]==1 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "        TN = [True if labels[i]==0 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "        FP = [True if labels[i]==1 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "        FN = [True if labels[i]==0 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "        P = len(target[target == 2])\n",
    "        N = len(target[target == 0])\n",
    "        TPR.append(TP/P)\n",
    "        TNR.append(TN/N)\n",
    "        FPR.append(FP/N)\n",
    "        FNR.append(FN/P)\n",
    "    purity.append(sum(tmp_pur) / len(tmp_pur))\n",
    "    TPR = sum(TPR) / len(TPR)\n",
    "    TNR = sum(TNR) / len(TNR)\n",
    "    FPR = sum(FPR) / len(FPR)\n",
    "    FNR = sum(FNR) / len(FNR)\n",
    "    \n",
    "    #print(\"Number of features:{} TPR:{} TNR:{} FNR:{} FPR:{}\".format(n_features, TPR, TNR, FNR, FPR))\n",
    "    TPR_L.append(TPR)\n",
    "    TNR_L.append(TNR)\n",
    "    FNR_L.append(FNR)\n",
    "    FPR_L.append(FPR)\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "plt.plot(range(1,len(TPR_L)+1) ,TPR_L, label=\"True Positive\", color=\"Blue\" )\n",
    "plt.plot(range(1,len(TNR_L)+1) ,TNR_L, label=\"True Negative\", color=\"Green\" )\n",
    "plt.plot(range(1,len(FNR_L)+1) ,FNR_L, label=\"False Negative\", color=\"Red\")\n",
    "plt.plot(range(1,len(FPR_L)+1) ,FPR_L, label=\"False Positive\" , color =\"y\")\n",
    "plt.xticks(range(1,len(FPR_L)+1))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "plt.plot(range(2,40),purity, label=\"Purity avg\")\n",
    "plt.title(\"Avg cluster purity\")\n",
    "plt.show()\n",
    "print(1+TPR_L.index(max(TPR_L)))\n",
    "print(1+TNR_L.index(max(TNR_L)))\n",
    "print(1+FNR_L.index(min(FNR_L)))\n",
    "print(1+FPR_L.index(min(FPR_L)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "X, _ = lasso_features(df, target, 4)\n",
    "X_s = X[target==0,:]\n",
    "print(X_s.shape)\n",
    "cov = EllipticEnvelope().fit(X)\n",
    "res = cov.predict(X)\n",
    "print(len(target))\n",
    "labels = [target[i] for i in range(len(res)) if res[i] == -1]\n",
    "\n",
    "print(\"Wrong:\",labels.count(0))\n",
    "print(\"Right:\",labels.count(2))\n",
    "gm = GaussianMixture(n_components=2)\n",
    "gm.fit(X)\n",
    "densities = gm.score_samples(X)\n",
    "print(len(densities))\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = np.where(densities < density_threshold)[0]\n",
    "labels = [target[i] for i in anomalies]\n",
    "print(labels.count(0))\n",
    "print(labels.count(2))\n",
    "#print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "X, _ = lasso_features(df, target, 4)\n",
    "print(set(target))\n",
    "bgm = BayesianGaussianMixture(n_components=3, n_init=10, random_state=0,covariance_type='tied',init_params='kmeans')\n",
    "bgm.fit(X)\n",
    "labels = bgm.predict(X)\n",
    "\n",
    "labels[target == 2]\n",
    "\n",
    "TP = [True if labels[i]==2 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "TN = [True if labels[i]==0 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FP = [True if labels[i]==2 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FN = [True if labels[i]==0 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "P = len(target[target == 2])\n",
    "N = len(target[target == 0])\n",
    "print(\"True pos :{} tp rate:{}\".format(TP,TP/P))\n",
    "print(\"True neg :{} tn rate:{}\".format(TN, TN/N))\n",
    "print(\"False pos :{} fp rate:{}\".format(FP,FP/N))\n",
    "print(\"False neg :{} fn rate:{}\".format(FN,FN/P))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# calculate interquartile range\n",
    "def remove_outliers(data, labels):\n",
    "    all_outliers = set()\n",
    "    for i in range(data.shape[1]):\n",
    "        q25, q75 = np.percentile(data[:,i], 25), np.percentile(data[:,i], 75)\n",
    "        iqr = q75 - q25\n",
    "        # calculate the outlier cutoff\n",
    "        cut_off = iqr * 1.5\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off\n",
    "        # identify outliers\n",
    "        outliers = set()\n",
    "        for x in data[:,i]:\n",
    "            if x < lower or x > upper:\n",
    "                outliers = set(np.where(data[:,i] == x)[0]).union(outliers)\n",
    "        #outliers = [np.where(X[:,i] == x)[0] for x in X[:,i] if x < lower or x > upper]\n",
    "        #print(len(outliers))\n",
    "        #print(len(set(outliers)))\n",
    "        all_outliers = all_outliers.union(outliers)\n",
    "    #print(\"----\")\n",
    "    #print(len(all_outliers))\n",
    "    l = [labels[i] for i in all_outliers]\n",
    "    #print(l.count(0))\n",
    "    #print(l.count(1))\n",
    "    #print(l.count(2))\n",
    "    ind_clear = list(set(range(X.shape[0])) -all_outliers)\n",
    "    X_clear = data[ind_clear,:]\n",
    "    target_clear = labels[ind_clear]\n",
    "    #print(X_clear.shape)\n",
    "    return X_clear, target_clear\n",
    "#print(len(outliers))\n",
    "#print(target[outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "#X, _ = lasso_features(df, target, 4)\n",
    "print(set(target))\n",
    "#bgm = BayesianGaussianMixture(n_components=3, n_init=10, random_state=0,covariance_type='tied',init_params='kmeans')\n",
    "bgm = GaussianMixture(n_components=3, n_init=10,max_iter=1000, random_state=0,covariance_type = \"full\",init_params='kmeans')\n",
    "\n",
    "bgm.fit(X_clear)\n",
    "labels = bgm.predict(X_clear)\n",
    "\n",
    "print(labels[target_clear == 2])\n",
    "\n",
    "TP = [True if labels[i]==2 and target_clear[i] == 2 else False for i in range(len(target_clear)) ].count(True)\n",
    "TN = [True if labels[i]==0 and target_clear[i] == 0 else False for i in range(len(target_clear)) ].count(True)\n",
    "FP = [True if labels[i]==2 and target_clear[i] == 0 else False for i in range(len(target_clear)) ].count(True)\n",
    "FN = [True if labels[i]==0 and target_clear[i] == 2 else False for i in range(len(target_clear)) ].count(True)\n",
    "P = len(target_clear[target_clear == 2])\n",
    "N = len(target_clear[target_clear == 0])\n",
    "print(\"True pos :{} tp rate:{}\".format(TP,TP/P))\n",
    "print(\"True neg :{} tn rate:{}\".format(TN, TN/N))\n",
    "print(\"False pos :{} fp rate:{}\".format(FP,FP/N))\n",
    "print(\"False neg :{} fn rate:{}\".format(FN,FN/P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "r = labels[target_clear == 2]\n",
    "print(len(np.where(r == 0)[0]))\n",
    "print(len(np.where(r == 1)[0]))\n",
    "print(len(np.where(r == 2)[0]))\n",
    "print(\"------------\")\n",
    "r = labels[target_clear == 1]\n",
    "print(len(np.where(r == 0)[0]))\n",
    "print(len(np.where(r == 1)[0]))\n",
    "print(len(np.where(r == 2)[0]))\n",
    "print(\"------------\")\n",
    "r = labels[target_clear == 0]\n",
    "print(len(np.where(r == 0)[0]))\n",
    "print(len(np.where(r == 1)[0]))\n",
    "print(len(np.where(r == 2)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "#new_lb = { int(i.split(\" \")[0]) : int(i.split(\" \")[1]) for i in open('labels_sample.txt','r').read().split(\"\\n\") if i != ''}\n",
    "#new_target = np.array([new_lb[user_ids[u]] for u in range(len(user_ids))])\n",
    "#print(new_target.shape)\n",
    "print(target.shape)\n",
    "#X, _ = lasso_features(df, target, 5)\n",
    "#X = rforest_features(df, target, 5)\n",
    "#X = kbest_features(df, new_target, 5)\n",
    "#X_clear, target_clear = remove_outliers(X, target)\n",
    "#clusters = dbscan_clustering(X_clear, 0.03, 25)\n",
    "#print(set(clusters))\n",
    "#cc = defaultdict(lambda: set())\n",
    "#for i in range(len(clusters)):\n",
    "#    cc[clusters[i]].add(i)\n",
    "#g_1 = set(np.where(target_clear == 1)[0])\n",
    "#g_2 = set(np.where(target_clear == 2)[0])\n",
    "#g_0 = set(np.where(target_clear == 0)[0])\n",
    "#for c in cc:\n",
    "#    if c == -1:\n",
    "#        continue\n",
    "#    print(\" Inter g_0: {} g_1: {} g_2: {}\".format(len(g_0.intersection(cc[c])),  len(g_1.intersection(cc[c])), len(g_2.intersection(cc[c])) ))\n",
    "    \n",
    "\n",
    "for f in range(1,10):\n",
    "    bic = []\n",
    "    aic=[]\n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "    X, _ = lasso_features(df, target, f)\n",
    "    print(\"Features:{}\".format(f))\n",
    "    for n in range(1,50):\n",
    "        bgm = GaussianMixture(n_components=n, n_init=10,max_iter=10000, random_state=0,covariance_type = \"tied\",init_params='kmeans')\n",
    "\n",
    "        bgm.fit(X)\n",
    "        bic.append(bgm.bic(X))\n",
    "        aic.append(bgm.aic(X))\n",
    "    plt.plot(range(len(bic)), bic,label=\"Bic\")\n",
    "    plt.plot(range(len(aic)), aic,label=\"Aic\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"GM_f_{}_tied.png\".format(f))\n",
    "    plt.clf()\n",
    "#accuracy_score(target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "\n",
    "print(target.shape)\n",
    "#X, _ = lasso_features(df, target, 5)\n",
    "#X = rforest_features(df, target, 5)\n",
    "#X = kbest_features(df, new_target, 5)\n",
    "#X_clear, target_clear = remove_outliers(X, target)\n",
    "\n",
    "score = []\n",
    "inhert = []\n",
    "aic=[]\n",
    "purity = []\n",
    "avg_cl_size = []\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "X, _ = lasso_features(df, target, 50)\n",
    "clusters = range(2,30)\n",
    "for k in clusters:\n",
    "    # Declaring Model\n",
    "    model = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fitting Model\n",
    "    model.fit(X)\n",
    "    score.append(silhouette_score(X, model.labels_))\n",
    "    inhert.append(model.inertia_)\n",
    "    purity.append(compute_pur(model.labels_,target))\n",
    "    avg_cl_size.append(compute_size(model.labels_))\n",
    "plt.plot(  clusters, score)\n",
    "plt.plot(  clusters, inhert)\n",
    "plt.xticks(clusters,rotation=45)\n",
    "plt.xlabel(\"K number\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"K-means scores for mutliple number of clusters\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "plt.plot( clusters, purity)\n",
    "plt.title(\"Cluster purity\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "\n",
    "plt.plot( clusters, avg_cl_size)\n",
    "plt.title(\"Avg cluster size\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#print(len(model.labels_))\n",
    "#print(model.labels_[0])\n",
    "#print(type(model.labels_[0]))\n",
    "#print(type(target[1]))\n",
    "#print(target[1])\n",
    "def compute_pur(labels,target):\n",
    "    d = defaultdict(lambda: [])\n",
    "    for i in range(len(labels)):\n",
    "        d[int(labels[i])].append(int(target[i]))\n",
    "    cluster_purity = []\n",
    "    for c in d:\n",
    "        max_tar = max(d[c])\n",
    "        cluster_purity.append(max_tar/len(d[c]))\n",
    "    return sum(cluster_purity)/ len(cluster_purity)\n",
    "\n",
    "def compute_size(labels):\n",
    "    d = defaultdict(lambda: [])\n",
    "    for i in range(len(labels)):\n",
    "        d[int(labels[i])].append(int(target[i]))\n",
    "    clusters = [len(d[x]) for x in d]\n",
    "    \n",
    "    return sum(clusters)/ len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Reading the DataFrame\n",
    "#seeds_df = pd.read_csv(\n",
    "#    \"https://raw.githubusercontent.com/vihar/unsupervised-learning-with-python/master/seeds-less-rows.csv\")\n",
    "\n",
    "# Remove the grain species from the DataFrame, save for later\n",
    "#varieties = list(seeds_df.pop('grain_variety'))\n",
    "\n",
    "# Extract the measurements as a NumPy array\n",
    "#samples = seeds_df.values\n",
    "\n",
    "\"\"\"\n",
    "Perform hierarchical clustering on samples using the\n",
    "linkage() function with the method='complete' keyword argument.\n",
    "Assign the result to mergings.\n",
    "\"\"\"\n",
    "mergings = linkage(X, method='complete')\n",
    "\n",
    "\"\"\"\n",
    "Plot a dendrogram using the dendrogram() function on mergings,\n",
    "specifying the keyword arguments labels=varieties, leaf_rotation=90,\n",
    "and leaf_font_size=6.\n",
    "\"\"\"\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    "           )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "X, _ = lasso_features(df, target, 4)\n",
    "for i in range(10,250,10):\n",
    "    print(\"i:{}\".format(i))\n",
    "    tsne_2d = make_tsne(X, target, \"full dataset\", learning_rate=350, perplexity=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "X, _ = lasso_features(df, target, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "perofrm_umap_clustering(X, target,  \"Full\", n_neighbors=25, n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "scores = []\n",
    "for j in range(100):\n",
    "    df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "    X = rforest_features(df, target, 40)\n",
    "    for i in range(1000):\n",
    "        clf = XGBClassifier(objective=\"multi:softprob\",\n",
    "                            num_class = 2,\n",
    "                            learning_rate =0.2,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=5,\n",
    "                            colsample_bytree = 0.7,\n",
    "                            eval_metric=\"rmse\",\n",
    "                            use_label_encoder=False)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3,)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_test,y_test))\n",
    "    \n",
    "print(\"Avg score:{}\".format(sum(scores)/len(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
